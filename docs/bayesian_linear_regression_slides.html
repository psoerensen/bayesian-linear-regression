<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Peter Sørensen">
  <title>Bayesian Linear Regression – Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-2c1b5f745a11cfad616ebade4a4a7d24.css">
  <link rel="stylesheet" href="slides.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Peter Sørensen 
</div>
        <p class="quarto-title-affiliation">
            Center for Quantitative Genetics and Genomics
          </p>
        <p class="quarto-title-affiliation">
            Aarhus University
          </p>
    </div>
</div>

</section>
<section id="overview" class="slide level2">
<h2>Overview</h2>
<ul>
<li><strong>Classical Linear Regression</strong>
<ul>
<li>Model, inference, and limitations</li>
</ul></li>
<li><strong>Bayesian Linear Regression</strong>
<ul>
<li>Motivation, priors, and posteriors<br>
</li>
<li>Conditional posteriors and inference</li>
</ul></li>
<li><strong>Computation and Applications</strong>
<ul>
<li>MCMC and Gibbs sampling<br>
</li>
<li>Diagnostics and R implementation</li>
</ul></li>
</ul>
</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<p><strong>Bayesian Linear Regression Models</strong> provide a flexible statistical framework for modeling complex biological and healthcare data. They support key applications such as:</p>
<ul>
<li><strong>Genome-wide association studies (GWAS)</strong> and <strong>fine-mapping</strong> of causal variants<br>
</li>
<li><strong>Polygenic risk scoring (PRS)</strong> for predicting complex traits and disease risk<br>
</li>
<li><strong>Gene and pathway enrichment analyses</strong> to test biological hypotheses<br>
</li>
<li><strong>Integrative multi-omics modeling</strong> across the genome, transcriptome, epigenome, and proteome<br>
</li>
<li>Applications to <strong>registry-based healthcare data</strong>, enabling population-level <strong>risk prediction</strong> and <strong>disease modeling</strong></li>
</ul>
</section>
<section id="introduction-1" class="slide level2">
<h2>Introduction</h2>
<ul>
<li><strong>Bayesian Linear Regression (BLR)</strong> extends classical regression by incorporating <strong>prior information</strong> and producing <strong>posterior distributions</strong> over model parameters.<br>
</li>
<li><strong>Advantages:</strong>
<ul>
<li>Handles <strong>high-dimensional</strong> and <strong>small-sample</strong> problems.<br>
</li>
<li>Provides <strong>full uncertainty quantification</strong>.<br>
</li>
<li>Enables <strong>regularization</strong> and integration of <strong>prior biological knowledge</strong>.</li>
</ul></li>
</ul>
</section>
<section id="classical-linear-regression" class="slide level2">
<h2>Classical Linear Regression</h2>
<p>The standard linear regression model, which assumes that the observed outcomes can be expressed as a linear combination of predictors plus random noise:</p>
<p><span class="math display">\[
y = X\beta + e, \quad e \sim \mathcal{N}(0, \sigma^2 I_n)
\]</span></p>
<ul>
<li><span class="math inline">\(y\)</span>: <span class="math inline">\(n \times 1\)</span> vector of observed outcomes<br>
</li>
<li><span class="math inline">\(X\)</span>: <span class="math inline">\(n \times p\)</span> design matrix of predictors<br>
</li>
<li><span class="math inline">\(\beta\)</span>: <span class="math inline">\(p \times 1\)</span> vector of unknown coefficients<br>
</li>
<li><span class="math inline">\(e\)</span>: Gaussian noise with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^2\)</span></li>
</ul>
</section>
<section id="estimation" class="slide level2">
<h2>Estimation</h2>
<p>Given the linear model, we can estimate the regression coefficients and residual variance using the method of ordinary least squares (OLS), which minimizes the sum of squared residuals:</p>
<p>Regression effects: <span class="math display">\[
\hat{\beta} = (X^\top X)^{-1} X^\top y
\]</span></p>
<p>Residual variance: <span class="math display">\[
\hat{\sigma}^2 = \frac{1}{n-p}\sum_i (y_i - x_i^\top \hat{\beta})^2
\]</span></p>
<p>Inference via standard errors and <span class="math inline">\(t\)</span>-tests, confidence intervals, and prediction intervals.</p>
</section>
<section id="limitations" class="slide level2">
<h2>Limitations</h2>
<p>While ordinary least squares estimation is simple and widely used, it has several important limitations that motivate the use of regularized or Bayesian approaches:</p>
<ul>
<li>No explicit control over <strong>effect size distribution</strong></li>
<li>Sensitive when <strong>collinearity</strong> is high</li>
<li><strong>Not identifiable</strong> when <strong><span class="math inline">\(p&gt;n\)</span></strong></li>
<li>Uncertainty largely <strong>asymptotic</strong> unless normality assumptions hold</li>
</ul>
</section>
<section id="why-bayesian-linear-regression" class="slide level2">
<h2>Why Bayesian Linear Regression?</h2>
<p>The Bayesian framework extends linear regression by incorporating prior beliefs about the model parameters and updating them with observed data through Bayes’ theorem:</p>
<ul>
<li>Combines <strong>likelihood</strong> and <strong>prior</strong> to form the <strong>posterior</strong>.<br>
</li>
<li>Priors express beliefs about <strong>effect sizes</strong>:
<ul>
<li>Normal → many small effects<br>
</li>
<li>Spike-and-slab → sparse effects<br>
</li>
</ul></li>
<li>Acts as a <strong>regularizer</strong>:
<ul>
<li>Shrinks small/noisy effects toward <span class="math inline">\(0\)</span><br>
</li>
<li>Preserves large, important effects<br>
</li>
</ul></li>
<li><strong>Stable when <span class="math inline">\(p &gt; n\)</span></strong> due to prior information.<br>
</li>
<li>Provides <strong>full posterior distributions</strong> for <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
<!---

---


## Overview: Bayesian Linear Regression


- Combines data and prior knowledge using **Bayes’ rule**.  
- Uses **conjugate priors** to yield closed-form full conditionals.  
- Employs **Gibbs sampling** to approximate the posterior distribution.  
- Estimates **parameters, uncertainty, and predictions** from posterior draws.

--->
</section>
<section id="bayesian-linear-regression-with-gaussian-priors" class="slide level2">
<h2>Bayesian Linear Regression with Gaussian Priors</h2>
<p>Bayesian linear regression starts with the same model structure as classical linear regression.</p>
<p><span class="math display">\[
y = X\beta + e, \quad e \sim \mathcal{N}(0, \sigma^2 I_n)
\]</span></p>
<ul>
<li><span class="math inline">\(y\)</span>: <span class="math inline">\(n \times 1\)</span> vector of observed outcomes<br>
</li>
<li><span class="math inline">\(X\)</span>: <span class="math inline">\(n \times p\)</span> design matrix of predictors<br>
</li>
<li><span class="math inline">\(\beta\)</span>: <span class="math inline">\(p \times 1\)</span> vector of unknown coefficients<br>
</li>
<li><span class="math inline">\(e\)</span>: Gaussian noise with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^2\)</span></li>
</ul>
</section>
<section id="likelihood-in-bayesian-linear-regression" class="slide level2">
<h2>Likelihood in Bayesian Linear Regression</h2>
<p>Because the residuals are assumed to be Gaussian,</p>
<p><span class="math display">\[
e \sim \mathcal{N}(0, \sigma^2 I_n)
\]</span></p>
<p>it follows that the <strong>response vector</strong> <span class="math inline">\(y\)</span> follows a multivariate normal distribution:</p>
<p><span class="math display">\[
y = X\beta + e \quad \Rightarrow \quad y \sim \mathcal{N}(X\beta, \sigma^2 I_n)
\]</span></p>
<p>This defines the <strong>likelihood</strong>, i.e.&nbsp;the probability of the observed data given the model parameters <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
p(y \mid X, \beta, \sigma^2) = \mathcal{N}(y \mid X\beta, \sigma^2 I_n)
\]</span></p>
</section>
<section id="introducing-priors" class="slide level2">
<h2>Introducing Priors</h2>
<p>In Bayesian linear regression, we specify <strong>prior distributions</strong> to express our beliefs about the model parameters before observing the data.</p>
<p>A common <strong>conjugate prior</strong> for the regression coefficients is</p>
<p><span class="math display">\[
\beta \mid \sigma_b^2 \sim \mathcal{N}(0, \sigma_b^2 I_p)
\]</span></p>
<p>This expresses the belief that most effect sizes are small and centered around zero — consistent with the <strong>polygenic assumption</strong> often used in genetics.</p>
<p>Using <strong>conjugate priors</strong> ensures that the <strong>posterior distributions</strong> remain in the same family as the priors (e.g., scaled inverse-chi-squared for variance parameters), enabling <strong>closed-form Gibbs sampling updates</strong>.</p>
</section>
<section id="role-of-the-prior-variance-sigma_b2" class="slide level2">
<h2>Role of the Prior Variance <span class="math inline">\(\sigma_b^2\)</span></h2>
<p>The parameter <span class="math inline">\(\sigma_b^2\)</span> acts as a <strong>shrinkage</strong> (or <strong>regularization</strong>) parameter:</p>
<ul>
<li>Small <span class="math inline">\(\sigma_b^2\)</span> → stronger shrinkage toward zero<br>
</li>
<li>Large <span class="math inline">\(\sigma_b^2\)</span> → weaker shrinkage, allowing larger effects</li>
</ul>
<p>It determines the <strong>strength of regularization</strong> and is often treated as an <strong>unknown hyperparameter</strong> to be estimated from the data.</p>
</section>
<section id="priors-on-variance-components" class="slide level2">
<h2>Priors on Variance Components</h2>
<p>We also place priors on the variance components to complete the hierarchical model:</p>
<p><span class="math display">\[
\sigma_b^2 \mid S_b, v_b \sim S_b \, \chi^{-2}(v_b), \quad
\sigma^2 \mid S, v \sim S \, \chi^{-2}(v)
\]</span></p>
<ul>
<li><span class="math inline">\(S_b\)</span> and <span class="math inline">\(v_b\)</span> are hyperparameters that control the prior distribution for the <strong>effect size variance</strong> <span class="math inline">\(\sigma_b^2\)</span>.<br>
</li>
<li><span class="math inline">\(S\)</span> and <span class="math inline">\(v\)</span> are hyperparameters for the <strong>residual variance</strong> <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
<p>These scaled inverse-chi-squared priors ensure conjugacy, enabling <strong>closed-form updates</strong> for the variance parameters in Gibbs sampling.</p>
<p><strong>Typical choices:</strong><br>
- Small degrees of freedom (e.g., <span class="math inline">\(v_b = v = 4\)</span>) give weakly informative, heavy-tailed priors.<br>
- Scale parameters <span class="math inline">\(S_b\)</span> and <span class="math inline">\(S\)</span> are often set based on expected variance magnitudes (e.g., empirical estimates).</p>
</section>
<section id="posterior-distribution" class="slide level2">
<h2>Posterior Distribution</h2>
<p>In Bayesian linear regression, we combine the <strong>likelihood</strong> and <strong>prior distributions</strong> using <strong>Bayes’ rule</strong> to obtain the <strong>joint posterior</strong>:</p>
<p><span class="math display">\[
p(\beta, \sigma_b^2, \sigma^2 \mid y) \propto
p(y \mid \beta, \sigma^2)\;
p(\beta \mid \sigma_b^2)\;
p(\sigma_b^2)\;
p(\sigma^2)
\]</span></p>
<p>The posterior distribution represents all <strong>updated knowledge</strong> about the unknown parameters after observing the data.<br>
It serves as the foundation for computing <strong>posterior means</strong>, <strong>credible intervals</strong>, and <strong>predictions</strong>.</p>
<p>In practice, the posterior is often too complex to evaluate directly, so we use <strong>sampling-based methods</strong> such as Gibbs sampling to approximate it.</p>
</section>
<section id="conjugacy-and-gibbs-sampling" class="slide level2">
<h2>Conjugacy and Gibbs Sampling</h2>
<p>With <strong>conjugate priors</strong>, each parameter’s <strong>full conditional distribution</strong> has a closed-form solution.<br>
This makes <strong>Gibbs sampling</strong> a natural and efficient inference method.</p>
<ul>
<li>Parameters are updated one at a time, each from its conditional posterior.<br>
</li>
<li>The resulting Markov chain explores the <strong>joint posterior</strong> of<br>
<span class="math inline">\((\beta, \sigma_b^2, \sigma^2)\)</span>.</li>
</ul>
<p>Gibbs sampling thus provides an easy way to approximate the full posterior in Bayesian linear regression.</p>
</section>
<section id="full-conditional-for-beta" class="slide level2">
<h2>Full Conditional for <span class="math inline">\(\beta\)</span></h2>
<p>Given <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\sigma_b^2\)</span>, and the data <span class="math inline">\(y\)</span>, the regression coefficients have a <strong>multivariate normal</strong> conditional posterior:</p>
<p><span class="math display">\[
\beta \mid \sigma^2, \sigma_b^2, y \sim
\mathcal{N}(\mu_\beta, \Sigma_\beta)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\Sigma_\beta = \left( \frac{X^\top X}{\sigma^2} + \frac{I}{\sigma_b^2} \right)^{-1},
\quad
\mu_\beta = \Sigma_\beta \frac{X^\top y}{\sigma^2}
\]</span></p>
<p>This distribution represents our <strong>updated belief</strong> about <span class="math inline">\(\beta\)</span> after observing the data, while holding <span class="math inline">\(\sigma_b^2\)</span> and <span class="math inline">\(\sigma^2\)</span> fixed.</p>
</section>
<section id="comparison-to-classical-ols" class="slide level2">
<h2>Comparison to Classical OLS</h2>
<p>In classical regression, the OLS estimator is</p>
<p><span class="math display">\[
\hat\beta_{\text{OLS}} = (X^\top X)^{-1} X^\top y
\]</span></p>
<p>The estimate of <span class="math inline">\(\beta\)</span> is <strong>independent of <span class="math inline">\(\sigma^2\)</span></strong>, since <span class="math inline">\(\sigma^2\)</span> only scales the likelihood, not its maximum.</p>
<p>In Bayesian regression, <span class="math inline">\(\sigma^2\)</span> appears explicitly in the posterior:</p>
<p><span class="math display">\[
\Sigma_\beta = \left( \frac{X^\top X}{\sigma^2} + \frac{I}{\sigma_b^2} \right)^{-1},
\quad
\mu_\beta = \Sigma_\beta \frac{X^\top y}{\sigma^2}
\]</span></p>
<p>The term <span class="math inline">\(\frac{I}{\sigma_b^2}\)</span> introduces <strong>shrinkage</strong>, regularizing estimates and stabilizing inference especially when <span class="math inline">\(p &gt; n\)</span> or predictors are highly correlated. Thus, the Bayesian posterior mean is a <strong>regularized, uncertainty-aware generalization</strong> of OLS.</p>
</section>
<section id="full-conditional-for-beta_j" class="slide level2">
<h2>Full Conditional for <span class="math inline">\(\beta_j\)</span></h2>
<p>Instead of sampling <span class="math inline">\(\beta\)</span> jointly, we can update each coefficient <span class="math inline">\(\beta_j\)</span> <strong>one at a time</strong>, holding all others fixed efficient.</p>
<p>Let <span class="math inline">\(X_j\)</span> be the <span class="math inline">\(j\)</span>th column of <span class="math inline">\(X\)</span> and define the <strong>partial residual</strong>:</p>
<p><span class="math display">\[
r_j = y - X_{-j} \beta_{-j}
\]</span></p>
<p>Then the conditional posterior for <span class="math inline">\(\beta_j\)</span> is univariate normal:</p>
<p><span class="math display">\[
\beta_j \mid D \sim \mathcal{N} \!\left(
\frac{X_j^\top r_j}{X_j^\top X_j + \sigma^2 / \sigma_b^2},\;
\frac{\sigma^2}{X_j^\top X_j + \sigma^2 / \sigma_b^2}
\right)
\]</span></p>
<p>This corresponds to a <strong>regularized least-squares update</strong>. Residual updates <strong>avoid matrix inversion</strong>, scale to high dimensions, and extend naturally to <strong>sparse (spike-and-slab)</strong> models.</p>
</section>
<section id="full-conditional-for-sigma_b2" class="slide level2">
<h2>Full Conditional for <span class="math inline">\(\sigma_b^2\)</span></h2>
<p>The conditional distribution of the <strong>prior variance</strong> <span class="math inline">\(\sigma_b^2\)</span>, given <span class="math inline">\(\beta\)</span> and the hyperparameters, is a <strong>scaled inverse-chi-squared</strong>:</p>
<p><span class="math display">\[
\sigma_b^2 \mid \beta \sim \tilde{S}_b \, \chi^{-2}(\tilde{v}_b)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\tilde{v}_b = v_b + p, \quad
\tilde{S}_b = \frac{\beta^\top \beta + v_b S_b}{\tilde{v}_b}
\]</span></p>
<p>At each Gibbs iteration, <span class="math inline">\(\sigma_b^2\)</span> is sampled directly given <span class="math inline">\(\beta\)</span>. This update reflects our revised belief about the <strong>variability of effect sizes</strong> after observing the current posterior draw of <span class="math inline">\(\beta\)</span>.</p>
</section>
<section id="full-conditional-for-sigma2" class="slide level2">
<h2>Full Conditional for <span class="math inline">\(\sigma^2\)</span></h2>
<p>The conditional distribution of the <strong>residual variance</strong> <span class="math inline">\(\sigma^2\)</span>,<br>
given <span class="math inline">\(\beta\)</span> and the data, is also <strong>scaled inverse-chi-squared</strong>:</p>
<p><span class="math display">\[
\sigma^2 \mid \beta, y \sim \tilde{S} \, \chi^{-2}(\tilde{v})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\tilde{v} = v + n, \quad
\tilde{S} = \frac{(y - X\beta)^\top (y - X\beta) + v S}{\tilde{v}}
\]</span></p>
<p>At each Gibbs iteration, <span class="math inline">\(\sigma^2\)</span> is sampled directly given <span class="math inline">\(\beta\)</span>.<br>
This captures our updated belief about the <strong>residual variability</strong><br>
after accounting for the current linear predictor <span class="math inline">\(X\beta\)</span>.</p>
</section>
<section id="gibbs-sampling-motivation" class="slide level2">
<h2>Gibbs Sampling: Motivation</h2>
<p>Bayesian inference often involves <strong>complex posteriors</strong> that lack closed-form solutions. To approximate these, we use <strong>Markov Chain Monte Carlo (MCMC)</strong> methods.</p>
<p>MCMC builds a <strong>Markov chain</strong> whose stationary distribution is the target posterior. Once the chain has <strong>converged</strong>, its samples can be used to estimate:</p>
<ul>
<li>Posterior means, variances, and credible intervals<br>
</li>
<li>Predictive distributions<br>
</li>
<li>Other functions of interest</li>
</ul>
<p>Among MCMC algorithms, the <strong>Gibbs sampler</strong> is especially useful when all <strong>full conditional distributions</strong> are available in <strong>closed form</strong>.</p>
</section>
<section id="gibbs-sampling-the-algorithm" class="slide level2">
<h2>Gibbs Sampling: The Algorithm</h2>
<p>For Bayesian linear regression with conjugate priors, the joint posterior is:</p>
<p><span class="math display">\[
p(\beta, \sigma_b^2 , \sigma^2 \mid y) \propto
p(y \mid \beta, \sigma^2)\; p(\beta \mid \sigma_b^2)\; p(\sigma_b^2)\; p(\sigma^2)
\]</span></p>
<p>We iteratively draw from the following <strong>full conditionals</strong>:</p>
<ol type="1">
<li>Sample <span class="math inline">\(\beta \mid \sigma_b^2, \sigma^2, y\)</span><br>
</li>
<li>Sample <span class="math inline">\(\sigma_b^2 \mid \beta\)</span><br>
</li>
<li>Sample <span class="math inline">\(\sigma^2 \mid \beta, y\)</span></li>
</ol>
<p>Each step updates one parameter given the latest values of the others. Repeating this sequence yields samples from the <strong>joint posterior</strong> <span class="math inline">\(p(\beta, \sigma_b^2, \sigma^2 \mid y)\)</span>.</p>
<p>Because each conditional is <strong>standard</strong> (Normal or scaled inverse-<span class="math inline">\(\chi^2\)</span>), Gibbs sampling is both <strong>efficient</strong> and <strong>easy to implement</strong>.</p>
</section>
<section id="posterior-summaries" class="slide level2">
<h2>Posterior Summaries</h2>
<p>After running the Gibbs sampler, we obtain a sequence of posterior draws <span class="math inline">\(\{\theta^{(t)}\}_{t=1}^T\)</span> for parameters such as <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(\sigma^2\)</span>, or <span class="math inline">\(\sigma_b^2\)</span>, where <span class="math inline">\(T\)</span> denotes the total number of MCMC iterations (after burn-in).<br>
We can summarize the posterior distribution using:</p>
<ul>
<li><p><strong>Posterior mean</strong><br>
<span class="math display">\[
\mathbb{E}[\theta \mid y] \approx \frac{1}{T} \sum_{t=1}^{T} \theta^{(t)}
\]</span></p></li>
<li><p><strong>Posterior median</strong>: the median of <span class="math inline">\(\{\theta^{(t)}\}\)</span></p></li>
<li><p><strong>95% credible interval</strong>: the interval between the 2.5th and 97.5th percentiles of <span class="math inline">\(\{\theta^{(t)}\}\)</span></p></li>
</ul>
<p>These summaries describe the most probable values of <span class="math inline">\(\theta\)</span> and their associated uncertainty after combining data with prior beliefs.</p>
</section>
<section id="estimating-uncertainty" class="slide level2">
<h2>Estimating Uncertainty</h2>
<p>Bayesian inference provides <strong>full posterior distributions</strong>, not just point estimates. Uncertainty is quantified directly from the posterior samples:</p>
<ul>
<li><strong>Posterior standard deviation</strong><br>
<span class="math display">\[
\mathrm{SD}(\theta \mid y) \approx
\sqrt{\frac{1}{T-1} \sum_{t=1}^{T} (\theta^{(t)} - \bar{\theta})^2}
\]</span></li>
</ul>
<p>The <strong>width</strong> of the credible interval reflects this uncertainty. Parameters with broader posteriors are estimated with less precision, and the degree of uncertainty depends on both the data and the prior.</p>
</section>
<section id="posterior-prediction" class="slide level2">
<h2>Posterior Prediction</h2>
<p>Given a new observation <span class="math inline">\(x_{\text{new}}\)</span>, we can predict using posterior draws:</p>
<ol type="1">
<li>Compute predicted means for each sample: <span class="math display">\[
\hat{y}_{\text{new}}^{(t)} = x_{\text{new}}^\top \beta^{(t)}
\]</span></li>
<li>Add residual uncertainty: <span class="math display">\[
y_{\text{new}}^{(t)} \sim
\mathcal{N}\!\left(x_{\text{new}}^\top \beta^{(t)},\; \sigma^{2(t)}\right)
\]</span></li>
</ol>
<p>The resulting samples <span class="math inline">\(\{y_{\text{new}}^{(t)}\}\)</span> form a <strong>posterior predictive distribution</strong>, from which we can derive <strong>predictive intervals</strong> and evaluate <strong>predictive accuracy</strong>.</p>
</section>
<section id="model-checking-and-hypothesis-testing" class="slide level2">
<h2>Model Checking and Hypothesis Testing</h2>
<p>Posterior samples enable rich <strong>model diagnostics</strong> and <strong>hypothesis testing</strong>:</p>
<ul>
<li><p><strong>Posterior probability of an event</strong><br>
<span class="math display">\[
\Pr(\beta_j \ne 0 \mid y)
\approx \frac{1}{T} \sum_{t=1}^{T} \mathbf{1}\!\left(\beta_j^{(t)} \ne 0\right)
\]</span></p></li>
<li><p><strong>Posterior predictive checks</strong><br>
Simulate new datasets using posterior draws and compare them to the observed data to assess model fit.</p></li>
<li><p><strong>Model comparison</strong><br>
Bayes factors and marginal likelihoods can be approximated to formally test or compare competing models.</p></li>
</ul>
<p>These tools extend Bayesian inference beyond estimation to <strong>model validation</strong>, <strong>uncertainty quantification</strong>, and <strong>decision-making</strong>.</p>
</section>
<section id="convergence-diagnostics" class="slide level2">
<h2>Convergence Diagnostics</h2>
<p>Before interpreting MCMC results, we must check that the Gibbs sampler has <strong>converged</strong> to the target posterior distribution.</p>
<p>Convergence diagnostics assess whether the Markov chain has reached its <strong>stationary distribution</strong> and is producing valid samples.</p>
<p>Two basic strategies are:</p>
<ul>
<li><strong>Burn-in</strong> – Discard early iterations (e.g., first 1000) to remove dependence on starting values.<br>
</li>
<li><strong>Thinning</strong> – Keep every <span class="math inline">\(k\)</span>-th sample to reduce autocorrelation.</li>
</ul>
<p>These steps improve sample quality and ensure reliable posterior summaries.</p>
</section>
<section id="trace-plots" class="slide level2">
<h2>Trace Plots</h2>
<p>A simple yet powerful diagnostic is the <strong>trace plot</strong>,<br>
showing sampled parameter values <span class="math inline">\(\theta^{(t)}\)</span> over iterations <span class="math inline">\(t\)</span>.</p>
<ul>
<li>A <strong>converged chain</strong> fluctuates around a stable mean — no trend or drift.<br>
</li>
<li>Multiple chains from different starting points should <strong>overlap</strong> and <strong>mix well</strong>.</li>
</ul>
<p>Trace plots help detect: - Lack of stationarity (upward/downward trends) - Poor mixing or multimodality - Burn-in issues</p>
<p>Visual inspection is often the <strong>first step</strong> in assessing convergence.</p>
</section>
<section id="autocorrelation" class="slide level2">
<h2>Autocorrelation</h2>
<p>Samples from a Gibbs sampler are <strong>correlated</strong>, especially for tightly coupled parameters.<br>
The <strong>autocorrelation function (ACF)</strong> quantifies dependence across lags <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
\hat{\rho}_k =
\frac{\sum_{t=1}^{T-k} (\theta^{(t)} - \bar{\theta})(\theta^{(t+k)} - \bar{\theta})}
     {\sum_{t=1}^{T} (\theta^{(t)} - \bar{\theta})^2}
\]</span></p>
<ul>
<li>High <span class="math inline">\(\hat{\rho}_k\)</span> → slow mixing and fewer effective samples<br>
</li>
<li>Low <span class="math inline">\(\hat{\rho}_k\)</span> → better mixing and faster convergence</li>
</ul>
<p>Reducing autocorrelation may require <strong>more iterations</strong>, <strong>reparameterization</strong>, or <strong>thinning</strong> the chain.</p>
</section>
<section id="effective-sample-size-ess" class="slide level2">
<h2>Effective Sample Size (ESS)</h2>
<p>Autocorrelation reduces the number of <em>independent</em> samples obtained.</p>
<p>The <strong>effective sample size (ESS)</strong> adjusts for this:</p>
<p><span class="math display">\[
\text{ESS}(\theta) =
\frac{T}{1 + 2 \sum_{k=1}^{K} \hat{\rho}_k}
\]</span></p>
<ul>
<li>Small ESS → chain is highly correlated, less informative<br>
</li>
<li>Rule of thumb: <span class="math inline">\(\text{ESS} &gt; 100\)</span> per parameter for stable inference</li>
</ul>
<p>ESS provides a quantitative measure of <strong>sampling efficiency</strong> and helps determine whether more iterations are needed.</p>
</section>
<section id="gelmanrubin-diagnostic-hatr" class="slide level2">
<h2>Gelman–Rubin Diagnostic (<span class="math inline">\(\hat{R}\)</span>)</h2>
<p>When running multiple chains, the <strong>Gelman–Rubin statistic</strong> compares <strong>between-chain</strong> and <strong>within-chain</strong> variability.</p>
<p>For <span class="math inline">\(m\)</span> chains with <span class="math inline">\(T\)</span> iterations each:</p>
<p><span class="math display">\[
W = \frac{1}{m} \sum_{i=1}^{m} s_i^2, \quad
B = \frac{T}{m-1} \sum_{i=1}^{m} (\bar{\theta}_i - \bar{\theta})^2
\]</span></p>
<p>The potential scale reduction factor:</p>
<p><span class="math display">\[
\hat{R} = \sqrt{ \frac{\hat{V}}{W} }, \quad
\hat{V} = \frac{T-1}{T} W + \frac{1}{T} B
\]</span></p>
<p>Values of <span class="math inline">\(\hat{R}\)</span> close to 1 indicate convergence, whereas <span class="math inline">\(\hat{R} &gt; 1.1\)</span> suggests that the chains have not yet converged.</p>
</section>
<section id="geweke-diagnostic" class="slide level2">
<h2>Geweke Diagnostic</h2>
<p>The <strong>Geweke test</strong> checks whether early and late portions of a single chain have the same mean, indicating <strong>stationarity</strong>.</p>
<p><span class="math display">\[
Z =
\frac{\bar{\theta}_A - \bar{\theta}_B}
     {\sqrt{\text{Var}(\bar{\theta}_A) + \text{Var}(\bar{\theta}_B)}}
\]</span></p>
<p>Typically:</p>
<ul>
<li>Segment A = first 10% of the chain<br>
</li>
<li>Segment B = last 50% of the chain</li>
</ul>
<p>Under convergence, <span class="math inline">\(Z \sim \mathcal{N}(0,1)\)</span>.</p>
<ul>
<li><span class="math inline">\(|Z| \le 2\)</span> → chain likely stationary<br>
</li>
<li><span class="math inline">\(|Z| &gt; 2\)</span> → potential non-convergence</li>
</ul>
<p>These diagnostics ensure that posterior summaries reflect the <strong>true target distribution</strong>.</p>
</section>
<section id="motivation-for-the-spike-and-slab-prior" class="slide level2">
<h2>Motivation for the Spike-and-Slab Prior</h2>
<p>In standard Bayesian linear regression,</p>
<p><span class="math display">\[
\beta_j \sim \mathcal{N}(0, \sigma_b^2)
\]</span></p>
<p>This <strong>Gaussian (shrinkage) prior</strong> assumes that all predictors have small effects but does <strong>not allow exact zeros</strong>, limiting its ability to perform variable selection.</p>
<p>The <strong>spike-and-slab prior</strong> addresses this by mixing two components:</p>
<ul>
<li>A <strong>spike</strong> at zero → represents excluded predictors<br>
</li>
<li>A <strong>slab</strong> (wide normal) → represents active predictors</li>
</ul>
<p>This formulation produces <strong>sparse</strong>, interpretable models that automatically select the most relevant variables.</p>
</section>
<section id="spike-and-slab-bayesian-linear-regression" class="slide level2">
<h2>Spike-and-Slab Bayesian Linear Regression</h2>
<p>As in classical Bayesian linear regression, the outcome is modeled as</p>
<p><span class="math display">\[
y = Xb + e, \quad e \sim \mathcal{N}(0, \sigma^2 I_n)
\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the <span class="math inline">\(n \times 1\)</span> response vector, <span class="math inline">\(X\)</span> is the <span class="math inline">\(n \times p\)</span> design matrix of predictors, <span class="math inline">\(b\)</span> is the <span class="math inline">\(p \times 1\)</span> vector of regression coefficients, and <span class="math inline">\(\sigma^2\)</span> is the residual variance.</p>
<p>This defines the <strong>likelihood</strong>:</p>
<p><span class="math display">\[
y \mid b, \sigma^2 \sim \mathcal{N}(Xb, \sigma^2 I_n)
\]</span></p>
<p>The goal is to estimate <span class="math inline">\(b\)</span> and determine which predictors truly contribute to explaining variation in <span class="math inline">\(y\)</span>.</p>
</section>
<section id="the-spike-and-slab-mixture-prior" class="slide level2">
<h2>The Spike-and-Slab Mixture Prior</h2>
<p>Each regression coefficient <span class="math inline">\(b_i\)</span> is drawn from a <strong>two-component mixture prior</strong>:</p>
<p><span class="math display">\[
p(b_i \mid \sigma_b^2, \pi)
= \pi\, \mathcal{N}(0, \sigma_b^2) + (1 - \pi)\, \delta_0
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\pi\)</span> is the <strong>prior probability</strong> that <span class="math inline">\(b_i\)</span> is nonzero (active predictor)<br>
</li>
<li><span class="math inline">\(\delta_0\)</span> is a <strong>point mass at zero</strong> (excluded predictor)</li>
</ul>
<p>Thus, with probability <span class="math inline">\(\pi\)</span> a predictor belongs to the <strong>slab</strong> (included), and with probability <span class="math inline">\(1 - \pi\)</span> it belongs to the <strong>spike</strong> (excluded).</p>
<p>This prior induces <strong>sparsity</strong>, allowing the model to automatically select relevant predictors while shrinking others exactly to zero.</p>
</section>
<section id="hierarchical-indicator-representation" class="slide level2">
<h2>Hierarchical (Indicator) Representation</h2>
<p>The spike-and-slab prior can be expressed hierarchically by introducing a <strong>binary inclusion indicator</strong> <span class="math inline">\(\delta_i\)</span>:</p>
<p><span class="math display">\[
b_i = \alpha_i \, \delta_i
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\alpha_i \mid \sigma_b^2 \sim \mathcal{N}(0, \sigma_b^2),
\quad
\delta_i \mid \pi \sim \text{Bernoulli}(\pi)
\]</span></p>
<ul>
<li><span class="math inline">\(\alpha_i\)</span>: effect size when the predictor is <strong>included</strong><br>
</li>
<li><span class="math inline">\(\delta_i\)</span>: binary variable indicating <strong>inclusion (1)</strong> or <strong>exclusion (0)</strong></li>
</ul>
<p>This representation separates <strong>effect size</strong> (<span class="math inline">\(\alpha_i\)</span>) from <strong>inclusion</strong> (<span class="math inline">\(\delta_i\)</span>), making inference straightforward via Gibbs sampling. Marginalizing over <span class="math inline">\(\delta_i\)</span> recovers the <strong>spike-and-slab mixture prior</strong> defined earlier.</p>
</section>
<section id="prior-for-the-inclusion-probability-pi" class="slide level2">
<h2>Prior for the Inclusion Probability <span class="math inline">\(\pi\)</span></h2>
<p>The overall <strong>sparsity level</strong> of the model is controlled by <span class="math inline">\(\pi\)</span>, which represents the prior probability that a predictor is included.<br>
We assign <span class="math inline">\(\pi\)</span> a <strong>Beta prior</strong>:</p>
<p><span class="math display">\[
\pi \sim \text{Beta}(\alpha, \beta)
\]</span></p>
<ul>
<li>Small <span class="math inline">\(\alpha\)</span> and large <span class="math inline">\(\beta\)</span> → favor <strong>sparser models</strong><br>
</li>
<li><span class="math inline">\(\alpha = \beta = 1\)</span> → <strong>uniform prior</strong> (no preference)<br>
</li>
<li>Larger <span class="math inline">\(\alpha\)</span> → favor <strong>denser models</strong></li>
</ul>
<p>This prior allows the <strong>data to inform the degree of sparsity</strong> through posterior updating of <span class="math inline">\(\pi\)</span>.</p>
</section>
<section id="priors-for-variance-components" class="slide level2">
<h2>Priors for Variance Components</h2>
<p>Variance parameters are typically assigned <strong>scaled inverse-chi-squared</strong> priors:</p>
<p><span class="math display">\[
\sigma_b^2 \sim S_b \chi^{-2}(v_b), \quad
\sigma^2 \sim S \chi^{-2}(v)
\]</span></p>
<p>These priors are <strong>conjugate</strong>, yielding <strong>closed-form conditional updates</strong> for both variance components.<br>
The hyperparameters <span class="math inline">\((S_b, v_b)\)</span> and <span class="math inline">\((S, v)\)</span> encode prior beliefs about the variability of <strong>effect sizes</strong> and <strong>residual noise</strong>, respectively.</p>
<p>In the spike-and-slab model, the <strong>sum of squares</strong> for updating <span class="math inline">\(\sigma_b^2\)</span> is computed only over the <strong>included effects</strong>, i.e.,</p>
<p><span class="math display">\[
\sum_{i:\delta_i=1} \alpha_i^2,
\]</span></p>
<p>ensuring that the variance of inactive predictors (where <span class="math inline">\(\delta_i = 0\)</span>) does not influence the estimate of <span class="math inline">\(\sigma_b^2\)</span>.</p>
</section>
<section id="joint-posterior-structure" class="slide level2">
<h2>Joint Posterior Structure</h2>
<p>As in Bayesian linear regression with normal priors, we combine the <strong>likelihood</strong> and <strong>priors</strong> to obtain the <strong>joint posterior</strong> over all model parameters:</p>
<p><span class="math display">\[
p(\mu, \alpha, \delta, \pi, \sigma_b^2, \sigma^2 \mid y)
\propto
p(y \mid \mu, \alpha, \delta, \sigma^2)\,
p(\alpha \mid \sigma_b^2)\,
p(\delta \mid \pi)\,
p(\pi)\,
p(\sigma_b^2)\,
p(\sigma^2)
\]</span></p>
<p>This captures our <strong>updated beliefs</strong> about effect sizes, inclusion indicators, and variance components after observing the data.</p>
<p>The inference procedure follows the same principle as for standard BLR — we use <strong>Gibbs sampling</strong> to draw from each parameter’s full conditional distribution.<br>
Next, we derive these <strong>full conditional distributions</strong> from the joint posterior.</p>
</section>
<section id="gibbs-sampling-the-algorithm-spike-and-slab-blr" class="slide level2">
<h2>Gibbs Sampling: The Algorithm (Spike-and-Slab BLR)</h2>
<p>We iteratively draw from the following <strong>full conditionals</strong>:</p>
<div style="font-size:1em; line-height:1em">
<ol type="1">
<li>Sample <span class="math inline">\(\alpha \mid \delta, \sigma_b^2, \sigma^2, y\)</span><br>
</li>
<li>Sample <span class="math inline">\(\delta \mid \alpha, \pi, y\)</span><br>
</li>
<li>Sample <span class="math inline">\(\pi \mid \delta\)</span><br>
</li>
<li>Sample <span class="math inline">\(\sigma_b^2 \mid \alpha, \delta\)</span><br>
</li>
<li>Sample <span class="math inline">\(\sigma^2 \mid \alpha, \delta, y\)</span></li>
</ol>
</div>
<p>Each step updates one parameter block given the others, and iterating the sequence yields samples from the joint posterior. Since all conditionals have <strong>standard forms</strong> (Normal, Bernoulli, Beta, scaled inverse-<span class="math inline">\(\chi^2\)</span>), Gibbs sampling is <strong>straightforward and efficient</strong>.</p>
</section>
<section id="posterior-inclusion-probabilities" class="slide level2">
<h2>Posterior Inclusion Probabilities</h2>
<p>The <strong>posterior inclusion probability (PIP)</strong> quantifies how likely each predictor is to be <strong>included in the model</strong> (i.e., truly associated with <span class="math inline">\(y\)</span>):</p>
<p><span class="math display">\[
\widehat{\Pr}(\delta_i = 1 \mid y)
= \frac{1}{T} \sum_{t=1}^{T} \delta_i^{(t)}
\]</span></p>
<ul>
<li><strong>High PIP</strong> → predictor is likely important<br>
</li>
<li><strong>Low PIP</strong> → predictor is likely irrelevant</li>
</ul>
<p>PIPs provide a direct measure of <strong>variable relevance</strong> and form the basis for <strong>Bayesian feature selection</strong>.</p>
</section>
<section id="advantages-of-spike-and-slab-priors" class="slide level2">
<h2>Advantages of Spike-and-Slab Priors</h2>
<p>This hierarchical mixture prior offers several key benefits:</p>
<ul>
<li><strong>Sparsity</strong> — allows exact zeros for irrelevant predictors<br>
</li>
<li><strong>Interpretability</strong> — binary indicators yield posterior inclusion probabilities (PIPs)<br>
</li>
<li><strong>Adaptivity</strong> — the inclusion probability <span class="math inline">\(\pi\)</span> is inferred from the data<br>
</li>
<li><strong>Balance</strong> — captures both strong signals (for detection) and small effects (for prediction)</li>
</ul>
<p>Thus, spike-and-slab models naturally combine <strong>variable selection</strong> with <strong>Bayesian uncertainty quantification</strong>.</p>
</section>
<section id="summary-of-bayesian-linear-regression" class="slide level2">
<h2>Summary of Bayesian Linear Regression</h2>
<p><strong>Bayesian Linear Regression</strong> combines the <strong>likelihood</strong> and <strong>prior</strong> to form the <strong>posterior</strong>, enabling principled modeling, regularization, and uncertainty quantification.</p>
<ul>
<li>Inference is performed via <strong>MCMC</strong>, typically <strong>Gibbs sampling</strong>, producing posterior draws for <strong>means</strong>, <strong>credible intervals</strong>, and <strong>predictions</strong>.<br>
</li>
<li><strong>Spike-and-slab priors</strong> introduce <strong>sparsity</strong> and support <strong>variable selection</strong>, assigning <strong>exact zeros</strong> to irrelevant predictors and identifying key variables through <strong>posterior inclusion probabilities (PIPs)</strong>.<br>
</li>
<li><strong>Conjugate</strong> and <strong>mixture</strong> priors allow for <strong>efficient</strong> and <strong>robust inference</strong>, even when <strong><span class="math inline">\(p &gt; n\)</span></strong>.<br>
</li>
<li>With appropriate <strong>convergence diagnostics</strong>, Bayesian models yield <strong>stable and reliable inference</strong> across diverse data settings.</li>
</ul>
</section>
<section id="applications-in-genomics" class="slide level2">
<h2>Applications in Genomics</h2>
<p>We have now covered the basic framework of <strong>Bayesian Linear Regression (BLR)</strong> and will illustrate how it provides a <strong>unified approach</strong> for analyzing genetic and genomic data.</p>
<ul>
<li><strong>Genome-Wide Association Studies (GWAS)</strong> and <strong>fine-mapping</strong> of causal variants<br>
</li>
<li><strong>Genetic prediction</strong> and <strong>heritability estimation</strong><br>
</li>
<li><strong>Pathway</strong> and <strong>gene-set enrichment</strong> analyses</li>
</ul>
<p>These applications demonstrate how BLR connects <strong>statistical modeling</strong> with <strong>biological interpretation</strong> in quantitative genetics.</p>
<script>
document.addEventListener("DOMContentLoaded", function() {
  const backBtn = document.createElement("a");
  backBtn.href = "index.html";
  backBtn.target = "_self";
  backBtn.textContent = "← Back to Main Page";
  backBtn.style.cssText = `
    position: fixed; top: 10px; right: 20px;
    background: #fff; color: #000;
    padding: 8px 14px; border-radius: 6px;
    border: 1px solid #000;
    font-weight: 600; font-family: sans-serif;
    text-decoration: none; font-size: 14px;
    box-shadow: 0 0 6px rgba(0,0,0,0.15);
    transition: all 0.2s ease-in-out;
    z-index: 9999;
  `;
  backBtn.onmouseover = () => {
    backBtn.style.background = '#000';
    backBtn.style.color = '#fff';
  };
  backBtn.onmouseout = () => {
    backBtn.style.background = '#fff';
    backBtn.style.color = '#000';
  };
  document.body.appendChild(backBtn);
});
</script>



</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    

    <script>

      // htmlwidgets need to know to resize themselves when slides are shown/hidden.

      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current

      // slide changes (different for each slide format).

      (function () {

        // dispatch for htmlwidgets

        function fireSlideEnter() {

          const event = window.document.createEvent("Event");

          event.initEvent("slideenter", true, true);

          window.document.dispatchEvent(event);

        }

    

        function fireSlideChanged(previousSlide, currentSlide) {

          fireSlideEnter();

    

          // dispatch for shiny

          if (window.jQuery) {

            if (previousSlide) {

              window.jQuery(previousSlide).trigger("hidden");

            }

            if (currentSlide) {

              window.jQuery(currentSlide).trigger("shown");

            }

          }

        }

    

        // hookup for slidy

        if (window.w3c_slidy) {

          window.w3c_slidy.add_observer(function (slide_num) {

            // slide_num starts at position 1

            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);

          });

        }

    

      })();

    </script>

    

    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>