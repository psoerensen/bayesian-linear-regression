<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Peter Sørensen">

<title>Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC – Bayesian Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3f1ceb0eb11a066e0501c9e2d1de2658.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Bayesian Linear Regression</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./notes.html" aria-current="page"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./bayesian_linear_regression_slides.html"> 
<span class="menu-text">Slides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./bayesian_linear_regression_slides_with_narration.html"> 
<span class="menu-text">Narrated Slides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./classical_linear_regression_simulation.html"> 
<span class="menu-text">Classical Regression</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./bayesian_linear_regression_conjugate.html"> 
<span class="menu-text">Bayesian (Gaussian Prior)</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./bayesian_spike_and_slab.html"> 
<span class="menu-text">Bayesian (Spike &amp; Slab)</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#classical-linear-regression" id="toc-classical-linear-regression" class="nav-link" data-scroll-target="#classical-linear-regression">Classical Linear Regression</a>
  <ul class="collapse">
  <li><a href="#model-specification" id="toc-model-specification" class="nav-link" data-scroll-target="#model-specification">Model Specification</a></li>
  <li><a href="#parameter-estimation-via-ols-and-mle" id="toc-parameter-estimation-via-ols-and-mle" class="nav-link" data-scroll-target="#parameter-estimation-via-ols-and-mle">Parameter Estimation via OLS and MLE</a></li>
  <li><a href="#inference-on-regression-coefficients" id="toc-inference-on-regression-coefficients" class="nav-link" data-scroll-target="#inference-on-regression-coefficients">Inference on Regression Coefficients</a></li>
  <li><a href="#prediction-and-uncertainty-in-predictions" id="toc-prediction-and-uncertainty-in-predictions" class="nav-link" data-scroll-target="#prediction-and-uncertainty-in-predictions">Prediction and Uncertainty in Predictions</a></li>
  <li><a href="#limitations-of-classical-linear-regression" id="toc-limitations-of-classical-linear-regression" class="nav-link" data-scroll-target="#limitations-of-classical-linear-regression">Limitations of Classical Linear Regression</a></li>
  </ul></li>
  <li><a href="#bayesian-linear-regression-with-gaussian-priors" id="toc-bayesian-linear-regression-with-gaussian-priors" class="nav-link" data-scroll-target="#bayesian-linear-regression-with-gaussian-priors">Bayesian Linear Regression with Gaussian priors</a>
  <ul class="collapse">
  <li><a href="#prior-distributions" id="toc-prior-distributions" class="nav-link" data-scroll-target="#prior-distributions">Prior Distributions</a></li>
  <li><a href="#posterior-distribution" id="toc-posterior-distribution" class="nav-link" data-scroll-target="#posterior-distribution">Posterior Distribution</a></li>
  </ul></li>
  <li><a href="#gibbs-sampling" id="toc-gibbs-sampling" class="nav-link" data-scroll-target="#gibbs-sampling">Gibbs Sampling</a>
  <ul class="collapse">
  <li><a href="#posterior-summaries-and-inference-from-gibbs-samples" id="toc-posterior-summaries-and-inference-from-gibbs-samples" class="nav-link" data-scroll-target="#posterior-summaries-and-inference-from-gibbs-samples">Posterior Summaries and Inference from Gibbs Samples</a></li>
  </ul></li>
  <li><a href="#convergence-diagnostics-for-gibbs-sampling" id="toc-convergence-diagnostics-for-gibbs-sampling" class="nav-link" data-scroll-target="#convergence-diagnostics-for-gibbs-sampling">Convergence Diagnostics for Gibbs Sampling</a>
  <ul class="collapse">
  <li><a href="#geweke-diagnostic" id="toc-geweke-diagnostic" class="nav-link" data-scroll-target="#geweke-diagnostic">Geweke Diagnostic</a></li>
  </ul></li>
  <li><a href="#bayesian-linear-regression-with-spike-and-slab-priors" id="toc-bayesian-linear-regression-with-spike-and-slab-priors" class="nav-link" data-scroll-target="#bayesian-linear-regression-with-spike-and-slab-priors">Bayesian Linear Regression with Spike-and-Slab Priors</a>
  <ul class="collapse">
  <li><a href="#prior-distributions-1" id="toc-prior-distributions-1" class="nav-link" data-scroll-target="#prior-distributions-1">Prior Distributions</a></li>
  <li><a href="#posterior-distribution-1" id="toc-posterior-distribution-1" class="nav-link" data-scroll-target="#posterior-distribution-1">Posterior Distribution</a></li>
  <li><a href="#gibbs-sampling-1" id="toc-gibbs-sampling-1" class="nav-link" data-scroll-target="#gibbs-sampling-1">Gibbs Sampling</a></li>
  <li><a href="#posterior-inclusion-probability" id="toc-posterior-inclusion-probability" class="nav-link" data-scroll-target="#posterior-inclusion-probability">Posterior Inclusion Probability</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="notes.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Peter Sørensen </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Center for Quantitative Genetics and Genomics
          </p>
        <p class="affiliation">
            Aarhus University
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Bayesian linear regression (BLR) extends the classical linear regression framework by incorporating prior information into the model and producing full posterior distributions over parameters, rather than single-point estimates. This approach offers several key advantages, particularly in the context of modern data analysis challenges such as high dimensionality, small sample sizes, and the need for uncertainty quantification.</p>
<p>In genomics and other biological applications, BLR is widely used for tasks such as mapping genetic variants, predicting genetic predisposition (e.g., polygenic risk scores), estimating genetic parameters like heritability, and performing gene set enrichment or pathway analyses. These applications benefit from BLR’s ability to unify inference and prediction within a probabilistic framework.</p>
<p>The BLR model builds on the familiar linear regression formulation, where the observed outcome is modeled as a linear function of predictors plus Gaussian noise. However, unlike classical inference—which relies on least squares or maximum likelihood estimation and provides only point estimates and asymptotic intervals—Bayesian inference yields full posterior distributions over the unknown coefficients and variance. This allows for richer uncertainty quantification and more robust inference.</p>
<p>Several motivations drive the use of Bayesian methods in linear regression. First, BLR naturally quantifies uncertainty through posterior distributions, allowing the analyst to compute credible intervals, posterior probabilities, and predictive distributions. Second, prior distributions act as regularizers, helping to stabilize estimation in noisy or underdetermined settings, such as when the number of predictors <span class="math inline">\(p\)</span> exceeds the number of observations <span class="math inline">\(n\)</span>. Gaussian priors encourage shrinkage toward zero, while more structured priors (such as spike-and-slab) enable sparse or grouped solutions. Third, BLR makes it straightforward to incorporate external knowledge—such as biological relevance or prior experimental results—into the modeling process.</p>
<p>These notes begin by reviewing the classical linear regression model and its limitations. We then introduce the Bayesian linear regression model, outline the inference workflow, and show how to derive the full conditional posterior distributions for the model parameters using conjugate priors. Finally, we describe how posterior inference is performed using Gibbs sampling and conclude with practical considerations for implementation, diagnostics, and applications in R.</p>
<div style="page-break-after: always;"></div>
</section>
<section id="classical-linear-regression" class="level1">
<h1>Classical Linear Regression</h1>
<p>Classical linear regression is one of the most widely used statistical modeling tools. It provides a simple yet powerful framework for modeling the relationship between a response variable and a set of predictor variables. The goal is to estimate how changes in the predictors <span class="math inline">\(X\)</span> affect the outcome <span class="math inline">\(y\)</span>, assuming a linear relationship and normally distributed errors.</p>
<section id="model-specification" class="level3">
<h3 class="anchored" data-anchor-id="model-specification">Model Specification</h3>
<p>We start by specifying the standard linear model:</p>
<p><span class="math display">\[
y = X\beta + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I_n)
\]</span></p>
<p>Here:</p>
<ul>
<li><span class="math inline">\(y\)</span> is the <span class="math inline">\(n \times 1\)</span> vector of observed outcomes,</li>
<li><span class="math inline">\(X\)</span> is the <span class="math inline">\(n \times p\)</span> design matrix of predictors (which may include an intercept),</li>
<li><span class="math inline">\(\beta\)</span> is the <span class="math inline">\(p \times 1\)</span> vector of unknown regression coefficients,</li>
<li><span class="math inline">\(\sigma^2\)</span> is the residual (error) variance,</li>
<li>and <span class="math inline">\(\epsilon\)</span> is the vector of i.i.d. normal errors with mean zero and variance <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
<p>Because the errors are Gaussian, the distribution of <span class="math inline">\(y\)</span> is:</p>
<p><span class="math display">\[
y \sim \mathcal{N}(X\beta, \sigma^2 I_n)
\]</span></p>
<p>This defines the <strong>likelihood</strong>—the probability model for the observed data given the parameters.</p>
</section>
<section id="parameter-estimation-via-ols-and-mle" class="level3">
<h3 class="anchored" data-anchor-id="parameter-estimation-via-ols-and-mle">Parameter Estimation via OLS and MLE</h3>
<p>In classical regression, parameters are estimated using <strong>Ordinary Least Squares (OLS)</strong>, which minimizes the residual sum of squares. Under the assumption of normally distributed errors, these estimators also correspond to the <strong>Maximum Likelihood Estimators (MLE)</strong>.</p>
<p>The OLS estimate for the regression coefficients is:</p>
<p><span class="math display">\[
\hat\beta = (X^\top X)^{-1} X^\top y
\]</span></p>
<p>This estimator is only valid when <span class="math inline">\(X^\top X\)</span> is invertible, which requires that the predictors are linearly independent and that <span class="math inline">\(n \ge p\)</span>.</p>
<p>The residual variance is estimated using:</p>
<p><span class="math display">\[
\hat\sigma^2 = \frac{1}{n - p} \sum_{i=1}^{n} (y_i - x_i^\top \hat\beta)^2 = \frac{1}{n - p} \sum_{i=1}^{n} (e_i)^2
\]</span></p>
<p>which provides a measure of the average squared distance between the observed and fitted values.</p>
</section>
<section id="inference-on-regression-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="inference-on-regression-coefficients">Inference on Regression Coefficients</h3>
<p>Once the model parameters are estimated, we can assess uncertainty and perform hypothesis tests.</p>
<p>The estimated <strong>variance-covariance matrix</strong> of <span class="math inline">\(\hat\beta\)</span> is:</p>
<p><span class="math display">\[
\widehat{\mathrm{Var}}(\hat\beta) = \hat\sigma^2 (X^\top X)^{-1}
\]</span></p>
<p>From this, the <strong>standard error</strong> for each estimated coefficient <span class="math inline">\(\hat\beta_j\)</span> is:</p>
<p><span class="math display">\[
\mathrm{SE}(\hat\beta_j) = \sqrt{ \hat\sigma^2 \left[ (X^\top X)^{-1} \right]_{jj} }
\]</span></p>
<p>To test whether a coefficient is significantly different from zero, we use the <strong><span class="math inline">\(t\)</span>-statistic</strong>:</p>
<p><span class="math display">\[
t_j = \frac{\hat\beta_j}{\mathrm{SE}(\hat\beta_j)}
\]</span></p>
<p>Under the null hypothesis <span class="math inline">\(H_0: \beta_j = 0\)</span>, and assuming Gaussian errors, this statistic follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n - p\)</span> degrees of freedom.</p>
<p>A <strong><span class="math inline">\(100(1 - \alpha)\%\)</span> confidence interval</strong> for <span class="math inline">\(\beta_j\)</span> is given by:</p>
<p><span class="math display">\[
\hat\beta_j \pm t_{n - p,\; \alpha/2} \cdot \mathrm{SE}(\hat\beta_j)
\]</span></p>
<p>These tools allow us to quantify uncertainty in our parameter estimates and test hypotheses about individual effects.</p>
</section>
<section id="prediction-and-uncertainty-in-predictions" class="level3">
<h3 class="anchored" data-anchor-id="prediction-and-uncertainty-in-predictions">Prediction and Uncertainty in Predictions</h3>
<p>For a new observation <span class="math inline">\(x_{\text{new}}\)</span> (a <span class="math inline">\(p \times 1\)</span> vector), the predicted mean response is:</p>
<p><span class="math display">\[
\hat{y}_{\text{new}} = x_{\text{new}}^\top \hat\beta
\]</span></p>
<p>The variance of this predicted response reflects both the uncertainty in the coefficient estimates and the inherent noise in the data:</p>
<p><span class="math display">\[
\widehat{\mathrm{Var}}(\hat{y}_{\text{new}}) =
\hat\sigma^2 \left(1 + x_{\text{new}}^\top (X^\top X)^{-1} x_{\text{new}}\right)
\]</span></p>
<p>This leads to <strong>prediction intervals</strong> that are wider than confidence intervals for <span class="math inline">\(\beta\)</span> because they account for additional variability in future observations.</p>
</section>
<section id="limitations-of-classical-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-classical-linear-regression">Limitations of Classical Linear Regression</h3>
<p>While classical linear regression is simple and interpretable, it has several well-known limitations:</p>
<ul>
<li>It does not allow for the incorporation of <strong>prior knowledge</strong> about parameters.</li>
<li>There is <strong>no explicit control</strong> over the distribution of effect sizes.</li>
<li>The model becomes <strong>non-identifiable</strong> when the number of predictors exceeds the number of observations (<span class="math inline">\(p &gt; n\)</span>), since <span class="math inline">\(X^\top X\)</span> is not invertible.</li>
<li>Estimates can be unstable or highly variable in the presence of <strong>multicollinearity</strong> (highly correlated predictors).</li>
<li>Uncertainty quantification relies on <strong>asymptotic results</strong> or the assumption of Gaussian errors, which may not always hold in practice.</li>
</ul>
<p>These limitations motivate the development and application of <strong>Bayesian linear regression</strong>, which extends the classical framework by incorporating prior distributions and producing full posterior distributions for all unknown parameters.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="bayesian-linear-regression-with-gaussian-priors" class="level1">
<h1>Bayesian Linear Regression with Gaussian priors</h1>
<p>Bayesian linear regression starts with the same model structure as classical linear regression. We assume the outcome vector <span class="math inline">\(y\)</span> is generated from a linear function of predictors <span class="math inline">\(X\)</span> with additive Gaussian noise. Specifically, the model is written as:</p>
<p><span class="math display">\[
y = X\beta + e, \quad e \sim \mathcal{N}(0, \sigma^2 I_n)
\]</span></p>
<p>Here, <span class="math inline">\(y\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of observed outcomes, <span class="math inline">\(X\)</span> is the <span class="math inline">\(n \times p\)</span> design matrix of predictors, <span class="math inline">\(\beta\)</span> is the <span class="math inline">\(p \times 1\)</span> vector of unknown regression coefficients, and <span class="math inline">\(e\)</span> is a vector of random errors assumed to be independent and identically distributed (i.i.d.) Gaussian noise with mean zero and constant variance <span class="math inline">\(\sigma^2\)</span>. Because the residuals are Gaussian, it follows that the marginal distribution of <span class="math inline">\(y\)</span> is:</p>
<p><span class="math display">\[
y \sim \mathcal{N}(X\beta, \sigma^2 I_n)
\]</span></p>
<p>This defines the <strong>likelihood</strong>—the probability model for the observed data, conditional on the unknown parameters <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<section id="prior-distributions" class="level3">
<h3 class="anchored" data-anchor-id="prior-distributions">Prior Distributions</h3>
<p>To perform Bayesian inference, we must specify <strong>prior distributions</strong> that encode our beliefs about the parameters before observing the data.</p>
<p>A commonly used <strong>conjugate prior</strong> for the regression coefficients <span class="math inline">\(\beta\)</span> is a multivariate normal distribution centered at zero:</p>
<p><span class="math display">\[
\beta \mid \sigma_b^2 \sim \mathcal{N}(0, \sigma_b^2 I_p)
\]</span></p>
<p>This prior reflects a belief that most effect sizes are small and centered near zero, consistent with the <strong>polygenic assumption</strong> in genetics. The parameter <span class="math inline">\(\sigma_b^2\)</span> is the <strong>prior variance</strong> and acts as a <strong>shrinkage parameter</strong>:</p>
<ul>
<li>When <span class="math inline">\(\sigma_b^2\)</span> is small, the prior strongly favors values of <span class="math inline">\(\beta\)</span> near zero, resulting in more shrinkage of estimates.<br>
</li>
<li>When <span class="math inline">\(\sigma_b^2\)</span> is large, the prior becomes more diffuse, allowing for larger effect sizes and less shrinkage.</li>
</ul>
<p>Thus, <span class="math inline">\(\sigma_b^2\)</span> controls the <strong>prior belief about the magnitude of effect sizes</strong>, and is often treated as an unknown hyperparameter to be estimated from the data (e.g., via hierarchical modeling or Gibbs sampling).</p>
<p>In addition, it is common to place <strong>scaled inverse-chi-squared distributions</strong> on the two variance parameters, <span class="math inline">\(\sigma_b^2\)</span> and <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
\sigma_b^2 \mid S_b, v_b \sim S_b \, \chi^{-2}(v_b)
\]</span></p>
<p><span class="math display">\[
\sigma^2 \mid S, v \sim S \, \chi^{-2}(v)
\]</span></p>
<p>Here:</p>
<ul>
<li><span class="math inline">\(S_b\)</span> and <span class="math inline">\(v_b\)</span> are user-defined hyperparameters that control the prior distribution on the <strong>variance of the regression coefficients</strong>.</li>
<li><span class="math inline">\(S\)</span> and <span class="math inline">\(v\)</span> are hyperparameters for the <strong>residual variance</strong> <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
<p>These priors are <strong>conjugate</strong> to the Gaussian likelihood and the normal prior on <span class="math inline">\(\beta\)</span>, which means they lead to posterior distributions in the same family (i.e., scaled inverse-chi-squared or Inverse-Gamma). This conjugacy simplifies derivations and enables closed-form <strong>Gibbs sampling</strong> steps.</p>
<p>These priors not only express prior knowledge or assumptions but also act as <strong>regularizers</strong>. In particular:</p>
<ul>
<li>The prior on <span class="math inline">\(\beta\)</span> shrinks small/noisy effect estimates toward zero.</li>
<li>The priors on variance parameters prevent overfitting and stabilize inference, especially in <strong>high-dimensional</strong> scenarios where <span class="math inline">\(p &gt; n\)</span>.</li>
</ul>
<p>This makes conjugate priors a practical and computationally efficient choice in Bayesian linear regression models.</p>
</section>
<section id="posterior-distribution" class="level3">
<h3 class="anchored" data-anchor-id="posterior-distribution">Posterior Distribution</h3>
<p>The core of Bayesian analysis is to combine the likelihood with the prior distributions using <strong>Bayes’ rule</strong>, which yields the joint posterior:</p>
<p><span class="math display">\[
p(\beta, \sigma_b^2 , \sigma^2 \mid y) \propto
p(y \mid \beta, \sigma^2)\; p(\beta \mid \sigma_b^2)\; p(\sigma_b^2)\; p(\sigma^2)
\]</span></p>
<p>This posterior encapsulates all updated knowledge about the unknown parameters after observing the data. It is the key quantity of interest in Bayesian inference and serves as the basis for computing summaries such as posterior means, credible intervals, or predictions.</p>
<p>Because we are using <strong>conjugate priors</strong>, the full conditional distributions of the parameters have <strong>closed-form solutions</strong>, which makes <strong>Gibbs sampling</strong> a natural and efficient inference strategy. In Gibbs sampling, we alternately sample from the conditional distributions of each parameter given the others.</p>
<section id="full-conditional-for-beta" class="level4">
<h4 class="anchored" data-anchor-id="full-conditional-for-beta">Full Conditional for <span class="math inline">\(\beta\)</span></h4>
<p>The full conditional distribution of the regression coefficients <span class="math inline">\(\beta\)</span>, given <span class="math inline">\(\sigma^2\)</span> and the observed data <span class="math inline">\(y\)</span>, is a multivariate normal distribution. We can write this in a compact and interpretable form using the <strong>conditional posterior mean</strong> <span class="math inline">\(\mu_\beta\)</span> and <strong>conditional posterior covariance matrix</strong> <span class="math inline">\(\Sigma_\beta\)</span>:</p>
<p><span class="math display">\[
\beta \mid \sigma^2, \sigma_b^2, y \sim \mathcal{N}(\mu_\beta,\; \Sigma_\beta)
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\Sigma_\beta = \left( \frac{X^\top X}{\sigma^2} + \frac{I}{\sigma_b^2} \right)^{-1}, \quad
\mu_\beta = \Sigma_\beta \cdot \frac{X^\top y}{\sigma^2}
\]</span></p>
<p>This conditional distribution reflects our updated belief about the regression coefficients after observing <span class="math inline">\(y\)</span>, while conditioning on fixed values of <span class="math inline">\(\sigma_b^2\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
</section>
<section id="full-conditional-for-beta_j" class="level4">
<h4 class="anchored" data-anchor-id="full-conditional-for-beta_j">Full Conditional for <span class="math inline">\(\beta_j\)</span></h4>
<p>In practice, rather than sampling the entire vector <span class="math inline">\(\beta\)</span> jointly, we can update each coefficient <span class="math inline">\(\beta_j\)</span> <strong>one at a time</strong>, holding all others fixed. This is often more efficient for large <span class="math inline">\(p\)</span>, and is particularly useful in Gibbs sampling frameworks like spike-and-slab models.</p>
<p>Let <span class="math inline">\(X_j\)</span> be the <span class="math inline">\(j\)</span>th column of the design matrix, and define the <strong>partial residual</strong>:</p>
<p><span class="math display">\[
r_j = y - X_{-j} \beta_{-j}
\]</span></p>
<p>where <span class="math inline">\(X_{-j}\)</span> is the matrix with the <span class="math inline">\(j\)</span>th column removed, and <span class="math inline">\(\beta_{-j}\)</span> is the vector of all coefficients except <span class="math inline">\(\beta_j\)</span>.</p>
<p>This coordinate-wise update strategy is justified because the full conditional distribution of <span class="math inline">\(\beta\)</span> is multivariate normal. When all other coefficients are held fixed, the conditional distribution of a single coefficient <span class="math inline">\(\beta_j\)</span> given the data and remaining parameters is univariate normal. This is a standard result from the theory of the multivariate normal distribution, which implies that any subset of variables also follows a (conditional) normal distribution.</p>
<p>The full conditional for <span class="math inline">\(\beta_j\)</span> is:</p>
<p><span class="math display">\[
\beta_j \mid D \sim \mathcal{N} \left(
\frac{X_j^\top r_j}{X_j^\top X_j + \sigma^2 / \sigma_b^2},\;
\frac{\sigma^2}{X_j^\top X_j + \sigma^2 / \sigma_b^2}
\right)
\]</span></p>
<p>This update can be derived directly from the Gaussian likelihood and Gaussian prior on <span class="math inline">\(\beta_j\)</span>, and corresponds to a regularized least-squares update. By cycling through all <span class="math inline">\(j = 1, \dots, p\)</span> using the current residuals, we efficiently obtain samples from the full conditional distribution of <span class="math inline">\(\beta\)</span>.</p>
<p>Residual updates are also attractive because they <strong>avoid matrix inversion</strong>, scale well to high dimensions, and naturally extend to models with sparsity indicators (e.g., spike-and-slab).</p>
</section>
<section id="comparison-to-classical-ols" class="level4">
<h4 class="anchored" data-anchor-id="comparison-to-classical-ols">Comparison to Classical OLS</h4>
<p>Recall that in classical linear regression, the <strong>ordinary least squares (OLS)</strong> estimator of <span class="math inline">\(\beta\)</span> is:</p>
<p><span class="math display">\[
\hat\beta_{\text{OLS}} = (X^\top X)^{-1} X^\top y
\]</span></p>
<p>This estimator is obtained by maximizing the likelihood under the assumption of Gaussian errors: <span class="math display">\[
y \sim \mathcal{N}(X\beta, \sigma^2 I)
\]</span></p>
<p>While the likelihood depends on <span class="math inline">\(\sigma^2\)</span>, it <strong>cancels out</strong> when estimating <span class="math inline">\(\beta\)</span> via OLS or MLE, because it only affects the <strong>scale</strong> of the likelihood, not the location of the maximum. As a result, the estimate of <span class="math inline">\(\beta\)</span> is <strong>independent of <span class="math inline">\(\sigma^2\)</span></strong>.</p>
<p>In contrast, the Bayesian formulation includes prior information, and <span class="math inline">\(\sigma^2\)</span> appears explicitly in the posterior:</p>
<p><span class="math display">\[
\Sigma_\beta = \left( \frac{X^\top X}{\sigma^2} + \frac{I}{\sigma_b^2} \right)^{-1}
\]</span></p>
<p>This introduces <strong>dependence on <span class="math inline">\(\sigma^2\)</span></strong>, meaning that uncertainty about the data also affects the precision of our belief about <span class="math inline">\(\beta\)</span>.</p>
<p>Moreover, the additional term <span class="math inline">\(\frac{I}{\sigma_b^2}\)</span> in the posterior precision matrix encodes prior information about effect sizes. This shrinks the estimates toward zero and helps <strong>regularize</strong> the inference, particularly when <span class="math inline">\(p &gt; n\)</span> or when predictors are highly correlated.</p>
<p>Thus, the Bayesian posterior mean:</p>
<p><span class="math display">\[
\mu_\beta = \Sigma_\beta \cdot \frac{X^\top y}{\sigma^2}
\]</span></p>
<p>can be seen as a <strong>regularized, uncertainty-aware generalization</strong> of the OLS estimate.</p>
</section>
<section id="full-conditional-for-sigma_b2" class="level4">
<h4 class="anchored" data-anchor-id="full-conditional-for-sigma_b2">Full Conditional for <span class="math inline">\(\sigma_b^2\)</span></h4>
<p>The full conditional distribution of the prior variance <span class="math inline">\(\sigma_b^2\)</span>, given the current values of <span class="math inline">\(\beta\)</span> and the hyperparameters, is a <strong>scaled inverse-chi-squared distribution</strong>:</p>
<p><span class="math display">\[
\sigma_b^2 \mid \beta \sim \tilde{S}_b \, \chi^{-2}(\tilde{v}_b)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\tilde{v}_b = v_b + p\)</span> is the updated degrees of freedom, with <span class="math inline">\(p\)</span> the number of regression coefficients,</li>
<li><span class="math inline">\(\tilde{S}_b = \dfrac{\beta^\top \beta + v_b S_b}{\tilde{v}_b}\)</span> is the updated scale parameter.</li>
</ul>
<p>This form is convenient for <strong>Gibbs sampling</strong>: at each iteration, a new value of <span class="math inline">\(\sigma_b^2\)</span> can be sampled directly, given the current value of <span class="math inline">\(\beta\)</span>. It reflects our updated belief about the variability of the regression coefficients after observing the current posterior draw of <span class="math inline">\(\beta\)</span>.</p>
</section>
<section id="full-conditional-for-sigma2" class="level4">
<h4 class="anchored" data-anchor-id="full-conditional-for-sigma2">Full Conditional for <span class="math inline">\(\sigma^2\)</span></h4>
<p>The full conditional distribution of the residual variance <span class="math inline">\(\sigma^2\)</span>, given the current values of <span class="math inline">\(\beta\)</span> and the data, is a <strong>scaled inverse-chi-squared distribution</strong>:</p>
<p><span class="math display">\[
\sigma^2 \mid \beta, y \sim \tilde{S} \, \chi^{-2}(\tilde{v})
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\tilde{v} = v + n\)</span> is the updated degrees of freedom, with <span class="math inline">\(n\)</span> the number of observations,</li>
<li><span class="math inline">\(\tilde{S} = \dfrac{(y - X\beta)^\top (y - X\beta) + v S}{\tilde{v}}\)</span> is the updated scale parameter.</li>
</ul>
<p>This form is convenient for <strong>Gibbs sampling</strong>: at each iteration, a new value of <span class="math inline">\(\sigma^2\)</span> can be sampled directly, given the current values of <span class="math inline">\(\beta\)</span>. It reflects our updated belief about the residual variability in the data after accounting for the current linear predictor <span class="math inline">\(X\beta\)</span>.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
</section>
<section id="gibbs-sampling" class="level1">
<h1>Gibbs Sampling</h1>
<p>Bayesian inference often requires sampling from complex <strong>posterior distributions</strong> that cannot be computed analytically. In such cases, we rely on <strong>Markov Chain Monte Carlo (MCMC)</strong> methods to approximate the posterior using a sequence of dependent samples.</p>
<p>MCMC algorithms construct a <strong>Markov chain</strong> whose stationary distribution is the target posterior. Once the chain has <strong>converged</strong>, the sampled values can be used to estimate posterior expectations, make predictions, and conduct inference.</p>
<p>One of the simplest and most widely used MCMC algorithms is the <strong>Gibbs sampler</strong>. Gibbs sampling is especially convenient when all <strong>full conditional distributions</strong> of the model parameters are available in closed form.</p>
<p>In the Bayesian linear regression model with conjugate priors, the joint posterior distribution is:</p>
<p><span class="math display">\[
p(\beta, \sigma_b^2 , \sigma^2 \mid y) \propto
p(y \mid \beta, \sigma^2)\; p(\beta \mid \sigma_b^2)\; p(\sigma_b^2)\; p(\sigma^2)
\]</span></p>
<p>We can implement a Gibbs sampler by iteratively drawing from the following full conditionals:</p>
<ol type="1">
<li>Sample <span class="math inline">\(\beta \mid \sigma_b^2, \sigma^2, y\)</span></li>
<li>Sample <span class="math inline">\(\sigma_b^2 \mid \beta\)</span></li>
<li>Sample <span class="math inline">\(\sigma^2 \mid \beta, y\)</span></li>
</ol>
<p>Each step updates one parameter conditional on the latest values of the others. Repeating this sequence over many iterations yields samples from the <strong>joint posterior</strong> <span class="math inline">\(p(\beta, \sigma_b^2, \sigma^2 \mid y)\)</span>.</p>
<p>Because each conditional distribution is standard (normal or scaled inverse-chi-squared), sampling is straightforward and efficient. Once the Gibbs sampler has <strong>converged</strong>, these posterior draws form the basis for inference.</p>
<section id="posterior-summaries-and-inference-from-gibbs-samples" class="level2">
<h2 class="anchored" data-anchor-id="posterior-summaries-and-inference-from-gibbs-samples">Posterior Summaries and Inference from Gibbs Samples</h2>
<p>After running the Gibbs sampler and obtaining <span class="math inline">\(T\)</span> posterior draws of all parameters, we can use these samples to compute a wide range of quantities relevant to Bayesian inference. These include:</p>
<ul>
<li><strong>Posterior means and medians</strong> as point estimates of parameters</li>
<li><strong>Credible intervals</strong> to quantify uncertainty</li>
<li><strong>Posterior standard deviations</strong> as measures of variability</li>
<li><strong>Posterior probabilities</strong> of hypotheses, such as <span class="math inline">\(\Pr(\beta_j &gt; 0 \mid y)\)</span></li>
<li><strong>Posterior predictive distributions</strong> for new observations</li>
<li><strong>Model diagnostics</strong> such as convergence checks or residual analysis</li>
</ul>
<p>These quantities allow us to summarize uncertainty, generate predictions, and make probabilistic statements about model parameters and data.</p>
<section id="posterior-summaries" class="level4">
<h4 class="anchored" data-anchor-id="posterior-summaries">Posterior Summaries</h4>
<p>Once we have a collection of posterior draws for a parameter <span class="math inline">\(\theta\)</span> (e.g., <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(\sigma^2\)</span>, or <span class="math inline">\(\sigma_b^2\)</span>), we can summarize the posterior distribution using:</p>
<ul>
<li><p><strong>Posterior mean</strong>: <span class="math display">\[
\mathbb{E}[\theta \mid y] \approx \frac{1}{T} \sum_{t=1}^{T} \theta^{(t)}
\]</span></p></li>
<li><p><strong>Posterior median</strong>: The median value of the sampled <span class="math inline">\(\theta^{(t)}\)</span>.</p></li>
<li><p><strong>Credible intervals</strong>: For example, a 95% credible interval for <span class="math inline">\(\theta\)</span> can be obtained as the 2.5% and 97.5% quantiles of the posterior samples: <span class="math display">\[
[\theta]_{0.025}, [\theta]_{0.975}
\]</span></p></li>
</ul>
<p>These summaries provide insight into the likely values of the parameter after accounting for uncertainty in both the data and prior beliefs.</p>
</section>
<section id="estimating-uncertainty" class="level4">
<h4 class="anchored" data-anchor-id="estimating-uncertainty">Estimating Uncertainty</h4>
<p>Bayesian inference provides <strong>full posterior distributions</strong>, not just point estimates. This allows us to directly quantify the uncertainty of parameters:</p>
<ul>
<li><p><strong>Posterior standard deviation</strong>: <span class="math display">\[
\mathrm{SD}(\theta \mid y) \approx \sqrt{\frac{1}{T-1} \sum_{t=1}^{T} \left( \theta^{(t)} - \bar{\theta} \right)^2}
\]</span></p></li>
<li><p>This uncertainty is reflected in the <strong>width</strong> of the credible intervals and can vary across different parameters or under different priors.</p></li>
</ul>
</section>
<section id="prediction" class="level4">
<h4 class="anchored" data-anchor-id="prediction">Prediction</h4>
<p>Given a new observation <span class="math inline">\(x_{\text{new}}\)</span>, we can generate <strong>posterior predictive distributions</strong> using the sampled parameter values:</p>
<ol type="1">
<li><p>For each draw <span class="math inline">\(t\)</span>, compute: <span class="math display">\[
\hat{y}_{\text{new}}^{(t)} = x_{\text{new}}^\top \beta^{(t)}
\]</span></p></li>
<li><p>Optionally, add residual noise from the corresponding draw of <span class="math inline">\(\sigma^{2(t)}\)</span>: <span class="math display">\[
y_{\text{new}}^{(t)} \sim \mathcal{N}\left(x_{\text{new}}^\top \beta^{(t)},\; \sigma^{2(t)} \right)
\]</span></p></li>
<li><p>Use these <span class="math inline">\(y_{\text{new}}^{(t)}\)</span> samples to construct predictive intervals or evaluate predictive performance.</p></li>
</ol>
</section>
<section id="model-checking-and-hypothesis-testing" class="level4">
<h4 class="anchored" data-anchor-id="model-checking-and-hypothesis-testing">Model Checking and Hypothesis Testing</h4>
<p>The posterior draws can also be used for <strong>model diagnostics</strong> or <strong>hypothesis testing</strong>:</p>
<ul>
<li><p><strong>Posterior probability of an event</strong>, such as a non-zero effect: <span class="math display">\[
\Pr(\beta_j \ne 0 \mid y) \approx \frac{1}{T} \sum_{t=1}^{T} \mathbf{1}\left( \beta_j^{(t)} \ne 0 \right)
\]</span></p></li>
<li><p><strong>Posterior predictive checks</strong>: Simulate new datasets from the model using posterior draws and compare them to the observed data. Discrepancies may indicate model misfit.</p></li>
<li><p><strong>Bayes factors</strong> and <strong>marginal likelihoods</strong> can be computed or approximated for formal hypothesis testing or model comparison, though these often require specialized methods beyond standard Gibbs output.</p></li>
</ul>
<p>These procedures allow us to move beyond point estimates and engage in a full Bayesian analysis that accounts for uncertainty in parameter estimation, prediction, and decision-making.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
</section>
<section id="convergence-diagnostics-for-gibbs-sampling" class="level1">
<h1>Convergence Diagnostics for Gibbs Sampling</h1>
<p>Before interpreting results from a Gibbs sampler, it is crucial to assess whether the sampler has <strong>converged</strong> to the target posterior distribution. Convergence diagnostics help determine if the Markov Chain has reached its stationary distribution and is providing valid samples.</p>
<section id="burn-in-and-thinning" class="level4">
<h4 class="anchored" data-anchor-id="burn-in-and-thinning">Burn-in and Thinning</h4>
<ul>
<li><strong>Burn-in</strong>: Discard initial samples (e.g., first 1000 iterations) to allow the chain to reach stationarity.</li>
<li><strong>Thinning</strong>: Keep every <span class="math inline">\(k\)</span>-th sample to reduce autocorrelation. This helps with storage but does not improve convergence.</li>
</ul>
</section>
<section id="trace-plots" class="level4">
<h4 class="anchored" data-anchor-id="trace-plots">Trace Plots</h4>
<p>A simple but effective tool is the <strong>trace plot</strong>: plotting sampled values of a parameter (e.g., <span class="math inline">\(\beta_j^{(t)}\)</span>) against iteration number <span class="math inline">\(t\)</span>:</p>
<ul>
<li>A converged chain should resemble a <strong>stationary process</strong> with no apparent trend.</li>
<li>Multiple chains started from different initial values should <strong>mix well</strong> and overlap.</li>
</ul>
</section>
<section id="autocorrelation" class="level4">
<h4 class="anchored" data-anchor-id="autocorrelation">Autocorrelation</h4>
<p>Gibbs samples are often correlated. We assess this using the <strong>autocorrelation function (ACF)</strong>:</p>
<ul>
<li><p>For lag <span class="math inline">\(k\)</span>, the sample autocorrelation of parameter <span class="math inline">\(\theta\)</span> is:</p>
<p><span class="math display">\[
\hat{\rho}_k = \frac{\sum_{t=1}^{T-k} \left( \theta^{(t)} - \bar{\theta} \right)\left( \theta^{(t+k)} - \bar{\theta} \right)}{\sum_{t=1}^{T} \left( \theta^{(t)} - \bar{\theta} \right)^2}
\]</span></p></li>
<li><p>High autocorrelation suggests <strong>slow mixing</strong>, requiring longer chains or thinning.</p></li>
</ul>
</section>
<section id="effective-sample-size" class="level4">
<h4 class="anchored" data-anchor-id="effective-sample-size">Effective Sample Size</h4>
<p>The <strong>effective sample size</strong> (ESS) adjusts for autocorrelation and reflects the number of independent samples:</p>
<p><span class="math display">\[
\text{ESS}(\theta) = \frac{T}{1 + 2 \sum_{k=1}^{K} \hat{\rho}_k}
\]</span></p>
<ul>
<li>A small ESS means the chain is highly autocorrelated and less informative.</li>
<li>As a rule of thumb, aim for <span class="math inline">\(\text{ESS} &gt; 100\)</span> per parameter.</li>
</ul>
</section>
<section id="gelmanrubin-diagnostic-r" class="level4">
<h4 class="anchored" data-anchor-id="gelmanrubin-diagnostic-r">Gelman–Rubin Diagnostic (R̂)</h4>
<p>When running <strong>multiple chains</strong>, the Gelman–Rubin statistic <span class="math inline">\(\hat{R}\)</span> compares between-chain and within-chain variance:</p>
<ol type="1">
<li><p>Let <span class="math inline">\(m\)</span> be the number of chains and <span class="math inline">\(T\)</span> the number of iterations per chain.</p></li>
<li><p>For each parameter <span class="math inline">\(\theta\)</span>, compute:</p>
<ul>
<li><p>The within-chain variance:</p>
<p><span class="math display">\[
W = \frac{1}{m} \sum_{i=1}^{m} s_i^2
\]</span></p></li>
<li><p>The between-chain variance:</p>
<p><span class="math display">\[
B = \frac{T}{m-1} \sum_{i=1}^{m} (\bar{\theta}_i - \bar{\theta})^2
\]</span></p></li>
</ul></li>
<li><p>The <strong>potential scale reduction factor</strong> is:</p>
<p><span class="math display">\[
\hat{R} = \sqrt{ \frac{\hat{V}}{W} }, \quad \text{where } \hat{V} = \frac{T - 1}{T} W + \frac{1}{T} B
\]</span></p></li>
</ol>
<ul>
<li>A value <span class="math inline">\(\hat{R} \approx 1\)</span> indicates convergence.</li>
<li>Values <span class="math inline">\(\hat{R} &gt; 1.1\)</span> suggest that the chain has <strong>not converged</strong>.</li>
</ul>
</section>
<section id="geweke-diagnostic" class="level3">
<h3 class="anchored" data-anchor-id="geweke-diagnostic">Geweke Diagnostic</h3>
<p>The <strong>Geweke diagnostic</strong> tests for stationarity by comparing the means of two segments of a single chain:</p>
<ul>
<li><p>Typically, the <strong>first 10%</strong> and the <strong>last 50%</strong> of the chain are used.</p></li>
<li><p>For a parameter <span class="math inline">\(\theta\)</span>, the test statistic is:</p>
<p><span class="math display">\[
Z = \frac{\bar{\theta}_A - \bar{\theta}_B}{\sqrt{ \text{Var}(\bar{\theta}_A) + \text{Var}(\bar{\theta}_B) }}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\bar{\theta}_A\)</span> is the mean of the early window,</li>
<li><span class="math inline">\(\bar{\theta}_B\)</span> is the mean of the late window.</li>
</ul></li>
<li><p>Under the null hypothesis of stationarity, <span class="math inline">\(Z\)</span> approximately follows a standard normal distribution.</p></li>
</ul>
<p>Values of <span class="math inline">\(Z\)</span> far from zero (e.g., <span class="math inline">\(|Z| &gt; 2\)</span>) suggest that the chain has <strong>not converged</strong>, as early and late samples differ systematically.</p>
<p>Monitoring these diagnostics ensures that posterior summaries and predictions are based on reliable samples from the true posterior distribution.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="bayesian-linear-regression-with-spike-and-slab-priors" class="level1">
<h1>Bayesian Linear Regression with Spike-and-Slab Priors</h1>
<p>Similar to the Bayesian linear regression model with Gaussian priors, we begin by specifying the <strong>likelihood</strong> for the observed data. The response vector <span class="math inline">\(y\)</span> is assumed to follow a Gaussian distribution, conditional on the regression parameters:</p>
<p><span class="math display">\[
y \mid \mathbf{b}, \sigma^2 \sim \mathcal{N}\left(X \mathbf{b},\; \sigma^2 I_n \right)
\]</span></p>
<p>where <span class="math inline">\(y\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of observed outcomes, <span class="math inline">\(X\)</span> is an <span class="math inline">\(n \times m\)</span> design matrix of predictor variables, <span class="math inline">\(\mathbf{b}\)</span> is an <span class="math inline">\(m \times 1\)</span> vector of regression coefficients, and <span class="math inline">\(\sigma^2\)</span> is the residual variance. This defines the data-generating process: given the regression coefficients and residual variance, the outcomes are normally distributed around the linear predictor <span class="math inline">\(X\mathbf{b}\)</span>.</p>
<p>In standard Bayesian linear regression (BLR), each regression coefficient <span class="math inline">\(\beta_j\)</span> is typically assigned a Gaussian prior:</p>
<p><span class="math display">\[
\beta_j \sim \mathcal{N}(0, \sigma_b^2)
\]</span></p>
<p>This reflects the belief that all predictors may contribute to the outcome, with effect sizes centered around zero and uncertainty governed by the prior variance <span class="math inline">\(\sigma_b^2\)</span>. Such <strong>shrinkage priors</strong> perform well in settings where many small effects are expected. However, they do <strong>not permit exact zeros</strong>, limiting their utility for <strong>variable selection</strong> or enforcing <strong>sparsity</strong>.</p>
<p>To address this, we adopt a hierarchical model structure using <strong>spike-and-slab priors</strong>, a type of <strong>two-component mixture model</strong>. Conditional on the regression coefficients, the outcomes <span class="math inline">\(y\)</span> follow a Gaussian distribution as above. At the second level of the hierarchy, however, each regression effect is assumed to arise from one of two components:</p>
<ul>
<li>A <strong>slab</strong>: a diffuse Gaussian distribution representing non-zero effects.<br>
</li>
<li>A <strong>spike</strong>: a point mass at zero representing exactly zero effects.</li>
</ul>
<p>This formulation allows us to directly model sparsity. Specifically, each coefficient <span class="math inline">\(b_i\)</span> is assumed to follow the mixture prior:</p>
<p><span class="math display">\[
p(b_i \mid \sigma_b^2, \pi) =
\pi \, \mathcal{N}(0, \sigma_b^2) + (1 - \pi) \, \delta_0,
\]</span></p>
<p>where <span class="math inline">\(\delta_0\)</span> denotes a point mass at zero, and <span class="math inline">\(\pi\)</span> is the prior probability that <span class="math inline">\(b_i\)</span> is non-zero.</p>
<p>Compared to standard Gaussian priors, <strong>spike-and-slab priors</strong> allow for exact zeros in regression coefficients. This enables <strong>automatic variable selection</strong> within a fully Bayesian framework, combining interpretability with uncertainty quantification.</p>
<p>The resulting <strong>two-component mixture prior</strong> offers several key advantages:</p>
<ul>
<li><strong>Sparsity</strong>: Supports exact zeros in the coefficient vector, allowing the model to exclude irrelevant predictors.</li>
<li><strong>Interpretability</strong>: Posterior samples of the binary inclusion variables <span class="math inline">\(\delta_i\)</span> yield <strong>posterior inclusion probabilities (PIPs)</strong>, which help identify important predictors.</li>
<li><strong>Adaptivity</strong>: By placing a <strong>Beta prior</strong> on the sparsity parameter <span class="math inline">\(\pi\)</span>, the model can learn the degree of sparsity directly from the data.</li>
<li><strong>Prediction–detection trade-off</strong>: The mixture structure balances the inclusion of small, potentially weak effects (for prediction) with the identification of stronger signals (for detection).</li>
</ul>
<p>In summary, the spike-and-slab prior extends Bayesian linear regression to high-dimensional settings by enabling principled variable selection and adaptive regularization. The following sections derive the full conditional distributions used for inference via Gibbs sampling.</p>
<section id="prior-distributions-1" class="level3">
<h3 class="anchored" data-anchor-id="prior-distributions-1">Prior Distributions</h3>
<section id="spike-and-slab-prior-for-regression-effects" class="level4">
<h4 class="anchored" data-anchor-id="spike-and-slab-prior-for-regression-effects">Spike-and-Slab Prior for Regression Effects</h4>
<p>To explicitly model sparsity, we use a <strong>spike-and-slab prior</strong>, which introduces a hierarchical structure. Each regression coefficient <span class="math inline">\(b_i\)</span> is expressed as:</p>
<p><span class="math display">\[
b_i = \alpha_i \cdot \delta_i, \quad i = 1, \dots, m
\]</span></p>
<p>Here, <span class="math inline">\(\delta_i\)</span> is a binary <strong>inclusion indicator</strong>, and <span class="math inline">\(\alpha_i\)</span> is the effect size when the predictor is active. We place the following priors:</p>
<p><span class="math display">\[
\alpha_i \mid \sigma_b^2 \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, \sigma_b^2), \quad
\delta_i \mid \pi \overset{\text{i.i.d.}}{\sim} \text{Bernoulli}(\pi)
\]</span></p>
<p>That is, each <span class="math inline">\(\delta_i\)</span> is independently drawn from a Bernoulli distribution with success probability <span class="math inline">\(\pi\)</span>, which represents the <strong>a priori probability</strong> that predictor <span class="math inline">\(i\)</span> is relevant.</p>
<p>Marginalizing over <span class="math inline">\(\delta_i\)</span>, the prior for <span class="math inline">\(b_i\)</span> becomes a two-component mixture:</p>
<p><span class="math display">\[
p(b_i \mid \sigma_b^2, \pi) = \pi \cdot \mathcal{N}(0, \sigma_b^2) + (1 - \pi) \cdot \delta_0
\]</span></p>
<p>where <span class="math inline">\(\delta_0\)</span> denotes a point mass at zero. This expresses that with probability <span class="math inline">\(\pi\)</span>, <span class="math inline">\(b_i\)</span> is drawn from a Gaussian (“slab”), and with probability <span class="math inline">\(1 - \pi\)</span>, it is exactly zero (“spike”).</p>
<p>The parameter <span class="math inline">\(\pi\)</span> controls the overall sparsity of the model. Importantly, the prior inclusion probability <span class="math inline">\(\pi\)</span> is distinct from the <strong>posterior inclusion probability</strong> <span class="math inline">\(\Pr(\delta_i = 1 \mid y)\)</span>, which is inferred from the data. A simple Monte Carlo estimate of this posterior probability is the average value of <span class="math inline">\(\delta_i\)</span> across samples from a Gibbs sampler.</p>
</section>
<section id="prior-for-the-inclusion-probability-pi" class="level4">
<h4 class="anchored" data-anchor-id="prior-for-the-inclusion-probability-pi">Prior for the Inclusion Probability <span class="math inline">\(\pi\)</span></h4>
<p>Rather than fixing <span class="math inline">\(\pi\)</span> in advance, we often treat it as a random variable and assign it a <strong>Beta prior</strong>:</p>
<p><span class="math display">\[
\pi \sim \text{Beta}(\alpha, \beta)
\]</span></p>
<p>This prior is defined on the interval <span class="math inline">\([0, 1]\)</span> and allows the data to inform the level of sparsity. The choice of <span class="math inline">\((\alpha, \beta)\)</span> reflects prior beliefs:</p>
<ul>
<li>Small <span class="math inline">\(\alpha\)</span> and large <span class="math inline">\(\beta\)</span> favor sparse models (most effects are zero).</li>
<li><span class="math inline">\(\alpha = \beta = 1\)</span> gives a uniform prior.</li>
<li>Larger <span class="math inline">\(\alpha\)</span> relative to <span class="math inline">\(\beta\)</span> favors denser models.</li>
</ul>
<p>Because of conjugacy with the Bernoulli prior on <span class="math inline">\(\delta_i\)</span>, the posterior update of <span class="math inline">\(\pi\)</span> is straightforward in Gibbs sampling.</p>
</section>
<section id="priors-for-variance-parameters" class="level4">
<h4 class="anchored" data-anchor-id="priors-for-variance-parameters">Priors for Variance Parameters</h4>
<p>As in the Gaussian BLR model, we assign <strong>scaled inverse-chi-squared priors</strong> to the variance components:</p>
<ul>
<li><p>For the prior variance of the effect sizes:</p>
<p><span class="math display">\[
\sigma_b^2 \sim S_b \cdot \chi^{-2}(v_b)
\]</span></p></li>
<li><p>For the residual variance:</p>
<p><span class="math display">\[
\sigma^2 \sim S \cdot \chi^{-2}(v)
\]</span></p></li>
</ul>
<p>These conjugate priors allow for closed-form updates in Gibbs sampling. The hyperparameters <span class="math inline">\((S_b, v_b)\)</span> and <span class="math inline">\((S, v)\)</span> encode prior beliefs about the variability of the coefficients and the residuals, and can be tuned to reflect prior knowledge or set to weakly informative values when such knowledge is limited.</p>
</section>
</section>
<section id="posterior-distribution-1" class="level3">
<h3 class="anchored" data-anchor-id="posterior-distribution-1">Posterior Distribution</h3>
<p>In the spike-and-slab Bayesian linear regression model, we introduce hierarchical priors for sparsity and variance components. The <strong>joint prior distribution</strong> over all model parameters factorizes as:</p>
<p><span class="math display">\[
p(\mu, \alpha, \delta, \pi, \sigma_b^2, \sigma^2)
\propto
p(\alpha \mid \sigma_b^2) \cdot
p(\delta \mid \pi) \cdot
p(\pi) \cdot
p(\sigma_b^2) \cdot
p(\sigma^2)
\]</span></p>
<p>with the components defined as follows:</p>
<ul>
<li><span class="math inline">\(p(\alpha \mid \sigma_b^2)\)</span>: Normal priors on the latent effect sizes,</li>
<li><span class="math inline">\(p(\delta \mid \pi)\)</span>: Bernoulli priors on binary inclusion indicators,</li>
<li><span class="math inline">\(p(\pi)\)</span>: Beta prior on the inclusion probability,</li>
<li><span class="math inline">\(p(\sigma_b^2)\)</span> and <span class="math inline">\(p(\sigma^2)\)</span>: Scaled inverse-chi-squared priors for the variance components.</li>
</ul>
<p>These priors encode our initial beliefs about sparsity and effect magnitudes before seeing the data.</p>
<p>Combining the prior structure with the likelihood using <strong>Bayes’ rule</strong>, we obtain the <strong>joint posterior distribution</strong> of all unknown parameters given the data <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
p(\mu, \alpha, \delta, \pi, \sigma_b^2, \sigma^2 \mid y)
\propto
p(y \mid \mu, \alpha, \delta, \sigma^2) \cdot
p(\alpha \mid \sigma_b^2) \cdot
p(\delta \mid \pi) \cdot
p(\pi) \cdot
p(\sigma_b^2) \cdot
p(\sigma^2)
\]</span></p>
<p>This expression defines the complete probabilistic model and captures our <strong>updated beliefs</strong> about the intercept <span class="math inline">\(\mu\)</span>, the regression effects <span class="math inline">\(\alpha\)</span>, the sparsity indicators <span class="math inline">\(\delta\)</span>, the prior inclusion probability <span class="math inline">\(\pi\)</span>, and both variance parameters after observing the data.</p>
<p>Since this posterior is analytically intractable, inference proceeds via <strong>Gibbs sampling</strong>, where each parameter block is updated iteratively from its full conditional distribution.</p>
</section>
<section id="gibbs-sampling-1" class="level3">
<h3 class="anchored" data-anchor-id="gibbs-sampling-1">Gibbs Sampling</h3>
<p>In this hierarchical Bayesian model with spike-and-slab priors, all <strong>full conditional posterior distributions</strong> are of known standard form. This allows us to use <strong>Gibbs sampling</strong>, where each parameter is sampled from its conditional distribution given the data and all other current parameter values.</p>
<p>At each iteration of the Gibbs sampler, we cycle through the following updates:</p>
<p><span class="math display">\[
[\mu \mid D],\quad
[\alpha \mid D],\quad
[\delta \mid D],\quad
[\pi \mid D],\quad
[\sigma_b^2 \mid D],\quad
[\sigma^2 \mid D]
\]</span></p>
<p>where <span class="math inline">\(D\)</span> denotes the observed data and all other current parameter values. The remainder of this section outlines the key updates for each block.</p>
<section id="updating-effect-sizes-alpha_i" class="level4">
<h4 class="anchored" data-anchor-id="updating-effect-sizes-alpha_i">Updating Effect Sizes <span class="math inline">\(\alpha_i\)</span></h4>
<p>Each latent effect <span class="math inline">\(\alpha_i\)</span> has a conditional posterior distribution that depends on whether the corresponding inclusion indicator <span class="math inline">\(\delta_i\)</span> is 0 or 1.</p>
<p><strong>If</strong> <span class="math inline">\(\delta_i = 0\)</span>, the effect <span class="math inline">\(b_i = \alpha_i \cdot \delta_i = 0\)</span> is excluded from the model, and the likelihood does <strong>not</strong> depend on <span class="math inline">\(\alpha_i\)</span>. In this case, <span class="math inline">\(\alpha_i\)</span> is not identifiable from the data, and its posterior is proportional to its prior:</p>
<p><span class="math display">\[
p(\alpha_i \mid D, \delta_i = 0) \propto \mathcal{N}(0, \sigma_b^2)
\]</span></p>
<p>Because <span class="math inline">\(\alpha_i\)</span> has no effect on the likelihood when <span class="math inline">\(\delta_i = 0\)</span>, practical implementations typically set <span class="math inline">\(b_i = 0\)</span>.</p>
<p><strong>If</strong> <span class="math inline">\(\delta_i = 1\)</span>, the effect contributes to the likelihood. Define the <strong>partial residual</strong> that excludes the contribution from predictor <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[
r_i = y - 1\mu - X_{-i} b_{-i}
\]</span></p>
<p>Then, the full conditional for <span class="math inline">\(\alpha_i\)</span> is Gaussian with mean and variance:</p>
<p><span class="math display">\[
\alpha_i \mid D \sim \mathcal{N} \left(
  \frac{X_i^\top r_i}{X_i^\top X_i + \sigma^2 / \sigma_b^2},\;
  \frac{\sigma^2}{X_i^\top X_i + \sigma^2 / \sigma_b^2}
  \right)
\]</span></p>
<p>This update corresponds to a <strong>shrinkage estimator</strong> of <span class="math inline">\(\alpha_i\)</span> that balances fit to the data (via <span class="math inline">\(X_i^\top r_i\)</span>) with regularization (via <span class="math inline">\(\sigma_b^2\)</span>).</p>
</section>
<section id="updating-inclusion-indicators-delta_i" class="level4">
<h4 class="anchored" data-anchor-id="updating-inclusion-indicators-delta_i">Updating Inclusion Indicators <span class="math inline">\(\delta_i\)</span></h4>
<p>Each indicator <span class="math inline">\(\delta_i \in \{0, 1\}\)</span> determines whether the <span class="math inline">\(i\)</span>th predictor is included in the model. Its full conditional is a <strong>Bernoulli distribution</strong> with success probability based on comparing the model fit with and without the predictor.</p>
<p>Let:</p>
<ul>
<li><span class="math inline">\(\text{RSS}_0\)</span>: residual sum of squares with <span class="math inline">\(\delta_i = 0\)</span>,</li>
<li><span class="math inline">\(\text{RSS}_1\)</span>: residual sum of squares with <span class="math inline">\(\delta_i = 1\)</span>.</li>
</ul>
<p>Then:</p>
<p><span class="math display">\[
\Pr(\delta_i = 1 \mid D) = \frac{
\exp\left( -\frac{1}{2\sigma^2} \text{RSS}_1 \right) \pi
}{
\exp\left( -\frac{1}{2\sigma^2} \text{RSS}_0 \right)(1 - \pi) +
\exp\left( -\frac{1}{2\sigma^2} \text{RSS}_1 \right)\pi
}
\]</span></p>
<p>To sample <span class="math inline">\(\delta_i\)</span>, compute this probability and draw from the Bernoulli distribution.</p>
<section id="numerically-stable-version-using-log-odds" class="level5">
<h5 class="anchored" data-anchor-id="numerically-stable-version-using-log-odds">Numerically Stable Version (Using Log-Odds)</h5>
<p>To avoid numerical underflow when RSS values are large, compute the <strong>log-odds</strong>:</p>
<p><span class="math display">\[
\log \left( \frac{\theta_i}{1 - \theta_i} \right) =
\frac{1}{2\sigma^2} (\text{RSS}_0 - \text{RSS}_1) - \log \left( \frac{1 - \pi}{\pi} \right)
\]</span></p>
<p>Then recover the probability <span class="math inline">\(\theta_i\)</span> using the <strong>inverse-logit</strong> (logistic) function:</p>
<p><span class="math display">\[
\theta_i = \frac{\exp(K_i)}{1 + \exp(K_i)}
\]</span></p>
<p>This provides a stable way to compute the probability of inclusion, especially when likelihood differences are large.</p>
</section>
</section>
<section id="updating-pi" class="level4">
<h4 class="anchored" data-anchor-id="updating-pi">Updating <span class="math inline">\(\pi\)</span></h4>
<p>With prior <span class="math inline">\(\pi \sim \text{Beta}(\eta, \beta)\)</span> and <span class="math inline">\(\delta_i \sim \text{Bernoulli}(\pi)\)</span>, the conditional posterior is:</p>
<p><span class="math display">\[
\pi \mid D \sim \text{Beta} \left(
\sum_{i=1}^m \delta_i + \eta,\;
m - \sum_{i=1}^m \delta_i + \beta
\right)
\]</span></p>
</section>
<section id="updating-sigma_b2" class="level4">
<h4 class="anchored" data-anchor-id="updating-sigma_b2">Updating <span class="math inline">\(\sigma_b^2\)</span></h4>
<p>The full conditional distribution of the prior variance <span class="math inline">\(\sigma_b^2\)</span>, given the current values of the effect sizes <span class="math inline">\(\alpha_i\)</span> and inclusion indicators <span class="math inline">\(\delta_i\)</span>, is a <strong>scaled inverse-chi-squared distribution</strong>:</p>
<p><span class="math display">\[
\sigma_b^2 \mid \alpha, \delta \sim \tilde{S}_b \cdot \chi^{-2}(\tilde{v}_b)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(p = \sum_{i=1}^m \delta_i\)</span> is the number of included (non-zero) effects,<br>
</li>
<li><span class="math inline">\(\tilde{v}_b = v_b + p\)</span> is the updated degrees of freedom,<br>
</li>
<li><span class="math inline">\(\tilde{S}_b = \dfrac{\sum_{i=1}^m \delta_i \alpha_i^2 + v_b S_b}{\tilde{v}_b}\)</span> is the updated scale parameter.</li>
</ul>
<p>This update accounts only for those coefficients currently included in the model (<span class="math inline">\(\delta_i = 1\)</span>), reflecting the prior belief that excluded effects are exactly zero and thus do not contribute to the variance estimate.</p>
<p>This form allows direct sampling of <span class="math inline">\(\sigma_b^2\)</span> at each Gibbs iteration and reflects the updated uncertainty about the size of the non-zero regression effects.</p>
</section>
<section id="updating-sigma2" class="level4">
<h4 class="anchored" data-anchor-id="updating-sigma2">Updating <span class="math inline">\(\sigma^2\)</span></h4>
<p>The full conditional distribution of the residual variance <span class="math inline">\(\sigma^2\)</span>, given the current values of <span class="math inline">\(\beta\)</span> and the data, is a <strong>scaled inverse-chi-squared distribution</strong>:</p>
<p><span class="math display">\[
\sigma^2 \mid \beta, y \sim \tilde{S} \, \chi^{-2}(\tilde{v})
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\tilde{v} = v + n\)</span> is the updated degrees of freedom, with <span class="math inline">\(n\)</span> the number of observations,</li>
<li><span class="math inline">\(\tilde{S} = \dfrac{(y - X\beta)^\top (y - X\beta) + v S}{\tilde{v}}\)</span> is the updated scale parameter.</li>
</ul>
<p>This form is convenient for <strong>Gibbs sampling</strong>: at each iteration, a new value of <span class="math inline">\(\sigma^2\)</span> can be sampled directly, given the current values of <span class="math inline">\(\beta\)</span>. It reflects our updated belief about the residual variability in the data after accounting for the current linear predictor <span class="math inline">\(X\beta\)</span>.</p>
<p>This completes one iteration of the Gibbs sampler. Each step updates parameters from their full conditional distributions, enabling efficient posterior inference under the spike-and-slab prior.</p>
</section>
<section id="posterior-inference" class="level4">
<h4 class="anchored" data-anchor-id="posterior-inference">Posterior inference</h4>
<p>Each step in the Gibbs sampler involves only standard distributions (Gaussian, Bernoulli, Beta, scaled-inverse-chi-squared), allowing efficient and scalable posterior inference. Iterating these updates produces samples from the joint posterior, which can be used to estimate marginal posterior summaries such as:</p>
<ul>
<li>Posterior means or medians of effects,</li>
<li>Posterior inclusion probabilities,</li>
<li>Credible intervals for regression coefficients,</li>
<li>Model sparsity levels.</li>
</ul>
</section>
</section>
<section id="posterior-inclusion-probability" class="level3">
<h3 class="anchored" data-anchor-id="posterior-inclusion-probability">Posterior Inclusion Probability</h3>
<p>While <span class="math inline">\(\pi\)</span> defines a global prior probability of inclusion, the <strong>posterior inclusion probability</strong> <span class="math inline">\(\Pr(\delta_i = 1 \mid y)\)</span> is computed <strong>separately for each marker</strong> after observing the data.</p>
<p>A Monte Carlo estimator of this probability is:</p>
<p><span class="math display">\[
\widehat{\Pr}(\delta_i = 1 \mid y) = \frac{1}{T} \sum_{t = 1}^{T} \delta_i^{(t)}
\]</span></p>
<p>where <span class="math inline">\(\delta_i^{(t)}\)</span> is the sampled value of <span class="math inline">\(\delta_i\)</span> in iteration <span class="math inline">\(t\)</span> of the Gibbs sampler.</p>
<p>This posterior quantity reflects our <strong>updated belief</strong> about whether each marker is truly associated with the trait, and is a key quantity used in Bayesian fine-mapping.</p>
<!-- Explain this section more later 


### Posterior Distribution

Combining the likelihood and priors using **Bayes’ rule**, the full **joint posterior distribution** becomes:

$$
p(\mu, \alpha, \delta, \pi, \sigma_b^2, \sigma^2 \mid y) 
\propto 
p(y \mid \mu, \alpha, \delta, \sigma^2) \cdot 
p(\alpha \mid \sigma_b^2) \cdot 
p(\delta \mid \pi) \cdot 
p(\pi) \cdot 
p(\sigma_b^2) \cdot 
p(\sigma^2)
$$

This expression captures all updated beliefs about the parameters after observing the data. Inference typically proceeds via **Gibbs sampling**, updating each block of parameters from its full conditional distribution.


## MCMC Implementation: Gibbs Sampling for the Spike-and-Slab Model

In the Bayesian spike-and-slab model, all **full conditional posterior distributions (fcpd)** are of known standard form. This allows us to implement inference using **Gibbs sampling**, where we iteratively sample from each of the following conditionals:

$$
[\mu \mid D],\quad
[\alpha \mid D],\quad
[\delta \mid D],\quad
[\pi \mid D],\quad
[\sigma_b^2 \mid D],\quad
[\sigma^2 \mid D]
$$

Here, $D$ denotes the **data** and all **other parameters** in the model, excluding the one currently being updated.

---

### Updating the Intercept $\mu$

We begin with the update for the scalar intercept $\mu$. From the model:

$$
y \mid \mu, \alpha, \delta, \sigma^2 
\sim \mathcal{N}\left(1 \mu + Xb,\; \sigma^2 I_n \right)
$$

where $b = (\alpha_1 \delta_1, \ldots, \alpha_m \delta_m)^\top$ is the vector of marker effects.

The **full conditional posterior** for $\mu$ is proportional to the likelihood:

$$
p(\mu \mid D) \propto p(y \mid \mu, \alpha, \delta, \sigma^2) 
\propto \exp\left\{ -\frac{1}{2\sigma^2}(y - 1\mu - Xb)^\top (y - 1\mu - Xb) \right\}
$$

We expand the quadratic form in the exponent:

$$
(y - 1\mu - Xb)^\top (y - 1\mu - Xb)
= \mu^\top 1^\top 1 \mu - 2\mu 1^\top (y - Xb) + \text{const}
$$

Let:

$$
\hat\mu = \frac{1^\top (y - Xb)}{1^\top 1} = \frac{1^\top (y - Xb)}{n}
$$

We can now complete the square in $\mu$:

$$
(y - 1\mu - Xb)^\top (y - 1\mu - Xb)
\propto (\mu - \hat\mu)^2 \cdot (1^\top 1)
$$

So the full conditional posterior becomes:

$$
p(\mu \mid D) \propto \exp\left\{ -\frac{1}{2\sigma^2} (\mu - \hat\mu)^2 \cdot n \right\}
$$

This is the kernel of a normal distribution. Thus, the **Gibbs update** is:

$$
\mu \mid D \sim \mathcal{N}\left( \hat\mu,\; \frac{\sigma^2}{n} \right)
$$

where:
- $\hat\mu = \frac{1^\top (y - Xb)}{n}$ is the mean of the residuals,
- $\frac{\sigma^2}{n}$ is the variance due to averaging over $n$ samples.

This step is simple and computationally efficient, and it updates the global intercept $\mu$ based on the current values of the marker effects $b$ and the observed data $y$.


### Updating the Marker Effects $\alpha_i$

We now consider the Gibbs sampling step for updating the individual regression coefficients $\alpha_i$ in the spike-and-slab model.

The full conditional posterior distribution (fcpd) for $\alpha_i$ is proportional to the product of the likelihood and the prior:

$$
p(\alpha_i \mid D) \propto p(y \mid \mu, \alpha, \delta, \sigma^2)\; p(\alpha_i \mid \sigma_b^2)
$$

There are two cases to consider, depending on the value of the indicator variable $\delta_i$.

---

#### Case 1: $\delta_i = 0$

When $\delta_i = 0$, the marker effect is inactive, so the product $\alpha_i \delta_i = 0$. In this case, the likelihood does not depend on $\alpha_i$, and the conditional posterior reduces to the prior:

$$
p(\alpha_i \mid D) \propto p(\alpha_i \mid \sigma_b^2)
$$

Since the prior is Gaussian:

$$
\alpha_i \mid D \sim \mathcal{N}(0, \sigma_b^2)
$$

This means $\alpha_i$ is sampled from its prior distribution and does not affect the data likelihood.

---

#### Case 2: $\delta_i = 1$

When $\delta_i = 1$, the effect $\alpha_i$ is included in the model, and the full conditional posterior becomes:

$$
p(\alpha_i \mid D) \propto p(y \mid \mu, \alpha, \delta, \sigma^2)\; p(\alpha_i \mid \sigma_b^2)
$$

Both the likelihood and the prior are Gaussian, so their product is also Gaussian. Let $X_i$ denote the $i$th column of the design matrix $X$. Define $X_{-i}$ as the matrix $X$ with the $i$th column removed, and let $b_{-i}$ be the vector of effects excluding the $i$th entry.

Define the residual (i.e., the part of $y$ not explained by all other predictors):

$$
r_i = y - 1\mu - X_{-i} b_{-i}
$$

Then the full conditional posterior of $\alpha_i$ is:

$$
\alpha_i \mid D \sim \mathcal{N}(\hat\alpha_i,\; \sigma^2 / (X_i^\top X_i + \sigma^2 / \sigma_b^2))
$$

where the **posterior mean** $\hat\alpha_i$ solves:

$$
(X_i^\top X_i + \sigma^2 / \sigma_b^2)\, \hat\alpha_i = X_i^\top r_i
$$

Equivalently:

$$
\hat\alpha_i = \frac{X_i^\top r_i}{X_i^\top X_i + \sigma^2 / \sigma_b^2}
$$

and the posterior variance is:

$$
\mathrm{Var}(\alpha_i \mid D) = \frac{\sigma^2}{X_i^\top X_i + \sigma^2 / \sigma_b^2}
$$

Thus, this Gibbs update is a standard normal draw centered at the adjusted least squares estimate of $\alpha_i$, shrunk by the prior.


### Updating the Inclusion Indicator $\delta_i$

We now derive the full conditional posterior distribution for the binary inclusion variable $\delta_i$ in the spike-and-slab model. This variable determines whether the $i$th marker effect is "on" ($\delta_i = 1$) or "off" ($\delta_i = 0$).

The full conditional is proportional to the product of the likelihood and the prior:

$$
p(\delta_i \mid D) \propto p(y \mid \mu, \alpha, \delta, \sigma^2)\; p(\delta_i \mid \pi)
$$

#### Case 1: $\delta_i = 0$

In this case, the $i$th marker is excluded from the model. The corresponding effect is zero, so $\alpha_i \delta_i = 0$.

Let $X_{-i}$ be the matrix $X$ with the $i$th column removed, and $b_{-i}$ the vector of all effects except the $i$th. The likelihood becomes:

$$
p(y \mid \delta_i = 0, D_{-i}) \propto \exp\left( -\frac{1}{2\sigma^2} (y - 1\mu - X_{-i} b_{-i})^\top (y - 1\mu - X_{-i} b_{-i}) \right)
$$

The prior for $\delta_i = 0$ is:

$$
p(\delta_i = 0 \mid \pi) = 1 - \pi
$$

Putting it together:

$$
\Pr(\delta_i = 0 \mid D) \propto \exp\left( -\frac{1}{2\sigma^2} \text{RSS}_0 \right) (1 - \pi)
$$

where $\text{RSS}_0$ is the **residual sum of squares** of the model without the $i$th marker.

---

#### Case 2: $\delta_i = 1$

In this case, the $i$th marker is included, and the effect is $\alpha_i$.

The likelihood becomes:

$$
p(y \mid \delta_i = 1, D) \propto \exp\left( -\frac{1}{2\sigma^2} (y - 1\mu - Xb)^\top (y - 1\mu - Xb) \right)
$$

The prior for $\delta_i = 1$ is:

$$
p(\delta_i = 1 \mid \pi) = \pi
$$

Putting it together:

$$
\Pr(\delta_i = 1 \mid D) \propto \exp\left( -\frac{1}{2\sigma^2} \text{RSS}_1 \right) \pi
$$

where $\text{RSS}_1$ is the residual sum of squares when all $m$ SNPs are included.

---

#### Normalized Posterior Probabilities

To draw a sample of $\delta_i$ from its posterior, we normalize the two unnormalized probabilities:

$$
\Pr(\delta_i = 0 \mid D) = \frac{
\exp\left( -\frac{1}{2\sigma^2} \text{RSS}_0 \right) (1 - \pi)
}{
\exp\left( -\frac{1}{2\sigma^2} \text{RSS}_0 \right) (1 - \pi) +
\exp\left( -\frac{1}{2\sigma^2} \text{RSS}_1 \right) \pi
}
$$

and

$$
\Pr(\delta_i = 1 \mid D) = \frac{
\exp\left( -\frac{1}{2\sigma^2} \text{RSS}_1 \right) \pi
}{
\exp\left( -\frac{1}{2\sigma^2} \text{RSS}_0 \right) (1 - \pi) +
\exp\left( -\frac{1}{2\sigma^2} \text{RSS}_1 \right) \pi
}
$$

These probabilities define a **Bernoulli distribution** from which we draw the next value of $\delta_i$ in the Gibbs sampler.

---

#### Estimating Posterior Inclusion Probabilities

A Monte Carlo estimator of the marginal posterior probability that SNP $i$ is included in the model is:

$$
\hat\phi_i = \Pr(\delta_i = 1 \mid y) \approx \frac{1}{L} \sum_{j = 1}^L \delta_i^{(j)}
$$

where $\delta_i^{(j)}$ is the sampled value of $\delta_i$ at iteration $j$ of the Gibbs sampler, and $L$ is the total number of samples.

This posterior inclusion probability $\hat\phi_i$ is a key summary used in **Bayesian variable selection**.


### Remarks on Sampling $\delta_i$ and Computing Inclusion Probabilities

1. **Efficient Updating of Residuals:**

   The residual sum of squares (RSS) values, $\text{RSS}_0$ and $\text{RSS}_1$, which are used in computing the conditional posterior probabilities of $\delta_i$, can be updated efficiently rather than recomputing from scratch. Efficient computational strategies for RSS updates are discussed in more detail in earlier sections (see page 316 of the source text).

2. **Computing Posterior Probabilities:**

   In practice, the posterior inclusion probability $\theta_i = \Pr(\delta_i = 1 \mid D)$ can be computed more numerically stably using the **log-odds transformation**. Define the log odds of inclusion as:

   $$
   \log\left( \frac{\theta_i}{1 - \theta_i} \right) = \frac{1}{2\sigma^2} (\text{RSS}_0 - \text{RSS}_1) - \left[ \log(1 - \pi) - \log(\pi) \right] = K_i
   $$

   From this, the posterior inclusion probability $\theta_i$ can be recovered using the logistic (inverse logit) function:

   $$
   \theta_i = \frac{ \exp(K_i) }{ 1 + \exp(K_i) }
   $$

   This formulation avoids directly computing exponentials of large negative numbers, which may underflow in floating point arithmetic.

3. **Two Ways to Characterize Posterior Inclusion:**

   Once the Markov chain has converged, the sampled values $\theta_i^{[j]}$ at iteration $j$ approximate the marginal posterior distribution $\Pr(\delta_i = 1 \mid y)$. This gives rise to **two characterizations** of the posterior inclusion probability:

   - **Monte Carlo average of samples of $\delta_i$ (binary indicator):**

     $$
     \hat\phi_i = \frac{1}{L} \sum_{j = 1}^L \delta_i^{[j]}
     $$

     This yields a point estimate of the posterior inclusion probability for marker $i$.

   - **Monte Carlo description via $\theta_i^{[j]}$ samples:**

     Each $\theta_i^{[j]}$ computed via the log-odds equation provides a **continuous posterior estimate**. These samples can be used to estimate the **entire distribution** of $\Pr(\delta_i = 1 \mid y)$ rather than just its mean.

   This second characterization can also be used to estimate quantities such as the **false discovery rate (FDR)**, which is discussed in the next chapter.


### Updating $\left[\pi \mid D \right]$

The full conditional posterior distribution (fcpd) of the prior inclusion probability $\pi$ can be derived by combining the prior and the likelihood of the binary indicator variables $\delta_i$.

From Bayes’ rule, the density of the conditional distribution is proportional to:

$$
p(\pi \mid D) \propto p(\delta \mid \pi)\, p(\pi)
$$

Assuming that each $\delta_i$ is conditionally independent and follows a Bernoulli distribution given $\pi$, and that the prior on $\pi$ is a Beta distribution with hyperparameters $\eta$ and $\beta$, we have:

- Likelihood of $\delta$ given $\pi$:
  $$
  p(\delta \mid \pi) = \prod_{i=1}^m \pi^{\delta_i}(1 - \pi)^{1 - \delta_i}
  = \pi^{\sum_{i=1}^m \delta_i}(1 - \pi)^{m - \sum_{i=1}^m \delta_i}
  $$

- Prior on $\pi$:
  $$
  \pi \sim \text{Beta}(\eta, \beta), \quad 
  p(\pi) \propto \pi^{\eta - 1} (1 - \pi)^{\beta - 1}
  $$

Putting the two together:

$$
p(\pi \mid D) \propto 
\pi^{\sum_i \delta_i + \eta - 1} 
(1 - \pi)^{m - \sum_i \delta_i + \beta - 1}
$$

This is the **kernel of a Beta distribution** with updated shape parameters:

$$
\pi \mid D \sim \text{Beta} \left(
\sum_{i=1}^m \delta_i + \eta,\;
m - \sum_{i=1}^m \delta_i + \beta
\right)
$$

This update is computationally straightforward and fits naturally into the Gibbs sampling framework.






## Deriving the Full Conditional Posterior for the Regression Coefficients ($\beta \mid \sigma^2, y$)

In Bayesian linear regression, one key step is to derive the **full conditional distribution** of the regression coefficients $\beta$, given the residual variance $\sigma^2$ and the observed data $y$. This is required for implementing **Gibbs sampling**, where we iteratively sample from each conditional distribution.

We begin by applying **Bayes' rule**, focusing only on the terms involving $\beta$:

$$
p(\beta \mid \sigma^2, y) \propto p(y \mid \beta, \sigma^2) \cdot p(\beta)
$$

This means the conditional posterior distribution of $\beta$ is proportional to the product of the **likelihood** of the data given $\beta$ and the **prior** distribution of $\beta$.

### Likelihood Term

From the model:

$$
y = X\beta + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I_n),
$$

we know that the likelihood (i.e., the probability of the data given the parameters) follows a multivariate normal distribution:

$$
p(y \mid \beta, \sigma^2) \propto \exp\left( -\frac{1}{2\sigma^2}(y - X\beta)^\top(y - X\beta) \right).
$$

This term measures how well the linear model with given parameters explains the observed data.

### Prior Term

We place a **normal prior** on the coefficients $\beta$, centered at zero, with prior variance $\sigma_b^2$:

$$
\beta \sim \mathcal{N}(0, \sigma_b^2 I),
$$

which leads to the prior density:

$$
p(\beta) \propto \exp\left( -\frac{1}{2\sigma_b^2} \beta^\top \beta \right).
$$

This expresses our prior belief that the regression coefficients are likely to be small (centered at zero), reflecting shrinkage or regularization.

### Combining Likelihood and Prior

To derive the conditional posterior, we combine the exponents of the likelihood and the prior. Focusing only on the terms that depend on $\beta$:

$$
- \frac{1}{2\sigma^2}(y - X\beta)^\top(y - X\beta)
- \frac{1}{2\sigma_b^2} \beta^\top \beta.
$$

We expand the first quadratic form:

$$
(y - X\beta)^\top(y - X\beta) 
= y^\top y - 2\beta^\top X^\top y + \beta^\top X^\top X \beta,
$$

so the expression becomes:

$$
- \frac{1}{2\sigma^2} \left( y^\top y - 2\beta^\top X^\top y + \beta^\top X^\top X \beta \right)
- \frac{1}{2\sigma_b^2} \beta^\top \beta.
$$

Since $y^\top y$ does not involve $\beta$, it is constant with respect to the distribution we are trying to derive and can be omitted from the expression.

Now we collect terms involving $\beta$:

$$
- \frac{1}{2} \left(
\beta^\top \left( \frac{X^\top X}{\sigma^2} + \frac{I}{\sigma_b^2} \right) \beta
- 2 \beta^\top \frac{X^\top y}{\sigma^2}
\right).
$$

This is a quadratic form in $\beta$ and can be rewritten using the **completing the square** technique.

## Completing the Square

We now complete the square to express the exponent in the form of a multivariate normal distribution. The identity we use is:

$$
\beta^\top A \beta - 2 \beta^\top b =
(\beta - A^{-1}b)^\top A (\beta - A^{-1}b) - b^\top A^{-1} b,
$$

which allows us to re-center the quadratic form.

We define:

- Matrix:  
  $$
  A = \frac{X^\top X}{\sigma^2} + \frac{I}{\sigma_b^2}
  $$

- Vector:  
  $$
  b = \frac{X^\top y}{\sigma^2}
  $$

Substituting into the identity, the exponent becomes:

$$
- \frac{1}{2} \left( \beta - A^{-1}b \right)^\top A \left( \beta - A^{-1}b \right) + \text{constant}.
$$

Since the constant term does not involve $\beta$, it is absorbed into the normalizing constant of the distribution.

Thus, we recognize this as the kernel of a multivariate normal distribution.

## Final Posterior Form

We conclude that the conditional posterior distribution of $\beta$ given $\sigma^2$ and $y$ is:

$$
\beta \mid \sigma^2, y \sim \mathcal{N}(A^{-1}b,\; A^{-1}).
$$

Alternatively, we can define:

$$
V = \left( \frac{X^\top X}{\sigma^2} + \frac{I}{\sigma_b^2} \right)^{-1},
$$

and write:

$$
\beta \mid \sigma^2, y \sim \mathcal{N}\left( V X^\top y / \sigma^2,\; V \right).
$$

This expression is easy to sample from in Gibbs sampling and provides a full probabilistic description of the posterior uncertainty in the regression coefficients.

### Why Does the Term $b^\top A^{-1} b$ Disappear?

The expression:

$$
\beta^\top A \beta - 2 \beta^\top b
$$

can be rewritten (by completing the square) as:

$$
(\beta - A^{-1} b)^\top A (\beta - A^{-1} b) - b^\top A^{-1} b.
$$

The last term, $b^\top A^{-1} b$, is constant with respect to $\beta$, and therefore it is absorbed into the proportionality constant when writing the posterior as:

$$
p(\beta \mid \cdots) \propto \exp\left(-\frac{1}{2}(\beta - A^{-1}b)^\top A (\beta - A^{-1}b)\right).
$$

This simplification is very helpful—it allows us to identify the posterior distribution as a multivariate normal distribution without needing to calculate the full normalizing constant.


### Prior Distributions

We use **conjugate priors** for analytical tractability:

- **Coefficients**:
  $$
  \beta \sim \mathcal{N}(0, \sigma_b^2 I_p)
  $$

- **Residual variance**:
  $$
  \sigma^2 \sim \text{Inverse-Gamma}(a, b)
  $$

These priors encode our assumptions or prior beliefs:

- The normal prior reflects the belief that most effects are small (centered at 0).
- The inverse-gamma prior is commonly used for variances in Gaussian models due to conjugacy.

---

### Objective

Our goal is to derive the full conditional posterior distributions:

$$
\beta \mid \sigma^2, y
\quad \text{and} \quad
\sigma^2 \mid \beta, y
$$

These conditionals are central to **Gibbs sampling**, a popular **Markov Chain Monte Carlo (MCMC)** method for Bayesian inference.

Understanding these derivations helps you:

- Apply Bayes' rule to multivariate models  
- See how priors influence posterior uncertainty  
- Appreciate the power of conjugacy  
- Build efficient Gibbs samplers

This material is especially suited for students with basic knowledge of multivariate normal distributions and linear models who are learning Bayesian methods for the first time.


## Deriving the Full Conditional Posterior for the Regression Coefficients ($\beta \mid \sigma^2, y$)

We begin with Bayes’ rule:
$$
p(\beta \mid \sigma^2, y) \propto p(y \mid \beta, \sigma^2) \cdot p(\beta)
$$

### Likelihood:
Under the Gaussian model:
$$
p(y \mid \beta, \sigma^2) \propto \exp\left(-\frac{1}{2\sigma^2}(y - X\beta)^\top(y - X\beta)\right)
$$

### Prior:
The prior on $\beta$ is:
$$
p(\beta) \propto \exp\left(-\frac{1}{2\sigma_b^2} \beta^\top \beta\right)
$$

### Combine Terms:
We combine terms involving $\beta$:
$$
-\frac{1}{2\sigma^2}(y - X\beta)^\top(y - X\beta) - \frac{1}{2\sigma_b^2} \beta^\top \beta
$$

Expanding:
$$
= -\frac{1}{2\sigma^2}(y^\top y - 2\beta^\top X^\top y + \beta^\top X^\top X \beta)
  - \frac{1}{2\sigma_b^2} \beta^\top \beta
$$

Collecting terms that depend on $\beta$:
$$
-\frac{1}{2} \left( \beta^\top \left(\frac{X^\top X}{\sigma^2} + \frac{I}{\sigma_b^2} \right) \beta
- 2 \beta^\top \frac{X^\top y}{\sigma^2} \right)
$$

## Completing the Square and Posterior Derivation for $\beta \mid \sigma^2, y$

To derive the posterior distribution of $\beta$ given $\sigma^2$ and $y$, we complete the square in the exponent.

We start by recognizing the standard identity:
$$
\beta^\top A \beta - 2\beta^\top b = (\beta - A^{-1}b)^\top A (\beta - A^{-1}b) - b^\top A^{-1}b
$$

In our case, apply this identity with:

- $A = \frac{X^\top X}{\sigma^2} + \frac{I}{\sigma_b^2}$
- $b = \frac{X^\top y}{\sigma^2}$

Then the exponent in the posterior becomes:
$$
-\frac{1}{2} \left( \beta^\top A \beta - 2\beta^\top b \right)
$$

Using the identity above, this simplifies to:
$$
-\frac{1}{2} (\beta - A^{-1}b)^\top A (\beta - A^{-1}b) + \text{const}
$$

Thus, the posterior distribution is:
$$
\beta \mid \sigma^2, y \sim \mathcal{N}\left( A^{-1}b,\; A^{-1} \right)
$$

or, equivalently:
$$
\beta \mid \sigma^2, y \sim \mathcal{N}\left( V X^\top y / \sigma^2,\; V \right)
$$

where:
$$
V = \left(\frac{X^\top X}{\sigma^2} + \frac{I}{\sigma_b^2} \right)^{-1}
$$

### Why Does $b^\top A^{-1} b$ Disappear?

The term $b^\top A^{-1} b$ is independent of $\beta$, so it becomes part of the normalizing constant in the distribution. Since we are only interested in the shape of the posterior (not its exact normalization), this constant is absorbed into the proportionality:
$$
p(\beta \mid \cdots) \propto \exp\left(-\frac{1}{2}(\beta^\top A \beta - 2\beta^\top b)\right)
\propto \exp\left(-\frac{1}{2}(\beta - A^{-1}b)^\top A (\beta - A^{-1}b)\right)
$$

This trick greatly simplifies the derivation and is one of the key steps in using conjugate priors in Bayesian linear regression.



## Deriving the Full Conditional Posterior for the Residual Variance ($\sigma^2 \mid \beta, y$)

We again use Bayes’ rule:
$$
p(\sigma^2 \mid \beta, y) \propto p(y \mid \beta, \sigma^2) \cdot p(\sigma^2)
$$

### Likelihood:
Under a Gaussian model:
$$
p(y \mid \beta, \sigma^2) \propto (\sigma^2)^{-n/2} \exp\left(-\frac{1}{2\sigma^2}(y - X\beta)^\top(y - X\beta)\right)
$$

### Prior:
The prior on $\sigma^2$ is:
$$
p(\sigma^2) \propto (\sigma^2)^{-(a+1)} \exp\left(-\frac{b}{\sigma^2}\right)
$$

### Combine Terms:
Multiply likelihood and prior:
$$
p(\sigma^2 \mid \beta, y) \propto
(\sigma^2)^{-(a + \frac{n}{2} + 1)} \exp\left( -\frac{1}{\sigma^2} \left[ b + \frac{1}{2}(y - X\beta)^\top(y - X\beta) \right] \right)
$$

This is the kernel of an **Inverse-Gamma distribution** with:

- Shape: $a + \frac{n}{2}$
- Scale: $b + \frac{1}{2}(y - X\beta)^\top(y - X\beta)$



## Summary

- The full conditional for $\beta$ is Gaussian due to conjugacy with a normal prior and likelihood.
- The full conditional for $\sigma^2$ is Inverse-Gamma due to the conjugate prior.
- Constant terms are dropped when working with proportional distributions.
- These results are the foundation for implementing **Gibbs sampling** for Bayesian linear regression.

\newpage

# Deriving Full Conditional Posterior Distributions for the Bayesian Linear Regression Model Used in Gibbs Sampling


### Updating $\sigma_b^2 \mid D$

We are interested in deriving the full conditional posterior distribution (FCPD) of the prior variance $\sigma_b^2$ for the regression coefficients $\beta$, conditional on the data $D$.

From the Bayesian rule, the density of the full conditional posterior of $\sigma_b^2$ is proportional to the product of the likelihood of $\beta$ and the prior on $\sigma_b^2$:

$$
p(\sigma_b^2 \mid D) \propto p(\beta \mid \sigma_b^2) \cdot p(\sigma_b^2)
$$

#### Likelihood term:

Assuming that the prior for $\beta$ given $\sigma_b^2$ is multivariate normal:

$$
\beta \mid \sigma_b^2 \sim \mathcal{N}(0, \sigma_b^2 I)
$$

Then the density is:

$$
p(\beta \mid \sigma_b^2) \propto (\sigma_b^2)^{-m/2} \exp\left( -\frac{\beta^\top \beta}{2\sigma_b^2} \right)
$$

where $m$ is the dimension (number of regression coefficients).

#### Prior on $\sigma_b^2$:

We place a **scaled inverse-chi-squared prior** on $\sigma_b^2$:

$$
\sigma_b^2 \sim S_b \chi^{-2}(v_b)
$$

Its density is proportional to:

$$
p(\sigma_b^2) \propto (\sigma_b^2)^{-(1 + v_b/2)} \exp\left(-\frac{v_b S_b}{2\sigma_b^2}\right)
$$

#### Combine terms:

Now, the full conditional posterior becomes:

$$
p(\sigma_b^2 \mid D) \propto 
(\sigma_b^2)^{-m/2} \exp\left(-\frac{\beta^\top \beta}{2\sigma_b^2} \right)
\cdot
(\sigma_b^2)^{-(1 + v_b/2)} \exp\left(-\frac{v_b S_b}{2\sigma_b^2}\right)
$$

Grouping terms:

$$
p(\sigma_b^2 \mid D) \propto 
(\sigma_b^2)^{-(1 + (v_b + m)/2)} 
\exp\left(-\frac{\beta^\top \beta + v_b S_b}{2\sigma_b^2} \right)
$$

This is the kernel of a **scaled inverse-chi-squared distribution** with:

- Updated degrees of freedom:
  $$
  \tilde{v}_b = v_b + m
  $$

- Updated scale:
  $$
  \tilde{S}_b = \frac{\beta^\top \beta + v_b S_b}{\tilde{v}_b}
  $$

Thus, the full conditional posterior is:

$$
\sigma_b^2 \mid D \sim \tilde{S}_b \, \chi^{-2}(\tilde{v}_b)
$$

This is useful in Gibbs sampling: at each iteration, a new value of $\sigma_b^2$ can be drawn from this scaled inverse-chi-squared distribution.

To **sample** from this distribution:

1. Draw $u \sim \chi^2(\tilde{v}_b)$  
2. Set $\sigma_b^2 = \tilde{v}_b \tilde{S}_b / u$



### Updating $\sigma^2 \mid D$

We now derive the full conditional posterior distribution (FCPD) of the **residual variance** $\sigma^2$, conditional on the data $D = \{y, X, \mu, \beta\}$.

From Bayes’ rule, the density is proportional to the product of the likelihood and the prior:

$$
p(\sigma^2 \mid D) \propto p(y \mid \mu, \beta, \sigma^2)\; p(\sigma^2)
$$


#### Likelihood

Assuming Gaussian residuals:

$$
y = 1\mu + X\beta + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2 I)
$$

Then the likelihood of the data $y$ given the parameters is:

$$
p(y \mid \mu, \beta, \sigma^2) \propto (\sigma^2)^{-n/2} \exp\left(
-\frac{(y - 1\mu - X\beta)^\top (y - 1\mu - X\beta)}{2\sigma^2}
\right)
$$


#### Prior on $\sigma^2$

We assign a **scaled inverse-chi-squared prior**:

$$
\sigma^2 \sim S \, \chi^{-2}(v)
$$

This has the following density:

$$
p(\sigma^2) \propto (\sigma^2)^{-(1 + v/2)} \exp\left(-\frac{v S}{2\sigma^2} \right)
$$


#### Combine Terms

Combining the likelihood and the prior:

$$
p(\sigma^2 \mid D) \propto (\sigma^2)^{-n/2} 
\exp\left(-\frac{(y - 1\mu - X\beta)^\top (y - 1\mu - X\beta)}{2\sigma^2}\right)
\cdot
(\sigma^2)^{-(1 + v/2)} \exp\left(-\frac{v S}{2\sigma^2} \right)
$$

Grouping exponents:

$$
p(\sigma^2 \mid D) \propto 
(\sigma^2)^{-(1 + (v + n)/2)} 
\exp\left( -\frac{(y - 1\mu - X\beta)^\top (y - 1\mu - X\beta) + v S}{2\sigma^2} \right)
$$


#### Identify Posterior Parameters

This is the kernel of a **scaled inverse-chi-squared distribution** with:

- Updated degrees of freedom:
  $$
  \tilde{v} = v + n
  $$

- Updated scale:
  $$
  \tilde{S} = \frac{(y - 1\mu - X\beta)^\top (y - 1\mu - X\beta) + v S}{\tilde{v}}
  $$

Therefore, the full conditional posterior is:

$$
\sigma^2 \mid D \sim \tilde{S} \, \chi^{-2}(\tilde{v})
$$


#### Sampling from the Posterior

To draw a sample from this distribution:

1. Sample $u \sim \chi^2(\tilde{v})$
2. Set:

$$
\sigma^2 = \frac{\tilde{v} \cdot \tilde{S}}{u}
$$


-->


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>