<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Peter Sørensen">
  <title>Bayesian Linear Regression – Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-2c1b5f745a11cfad616ebade4a4a7d24.css">
  <link rel="stylesheet" href="slides.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Peter Sørensen 
</div>
        <p class="quarto-title-affiliation">
            Center for Quantitative Genetics and Genomics
          </p>
        <p class="quarto-title-affiliation">
            Aarhus University
          </p>
    </div>
</div>

</section>
<section id="overview" class="slide level2">
<h2>Overview</h2>
<ul>
<li><strong>Classical Linear Regression</strong>
<ul>
<li>Model, inference, and limitations</li>
</ul></li>
<li><strong>Bayesian Linear Regression</strong>
<ul>
<li>Motivation, priors, and posteriors<br>
</li>
<li>Conditional posteriors and inference</li>
</ul></li>
<li><strong>Computation and Applications</strong>
<ul>
<li>MCMC and Gibbs sampling<br>
</li>
<li>Diagnostics and R implementation</li>
</ul></li>
</ul>
<aside class="notes">
<p>We begin with classical linear regression, focusing on the model formulation, inference methods, and the assumptions that often limit its use in complex data settings. Next, we introduce Bayesian linear regression, explaining how prior information and probabilistic reasoning extend classical inference. We will discuss how to define and interpret priors and posteriors and how conditional posteriors allow efficient inference. Finally, we will cover computational methods such as Markov chain Monte Carlo and Gibbs sampling, which make Bayesian estimation practical, and end with examples of diagnostics and R implementations. The goal is to provide both conceptual understanding and hands-on perspective for applying these models in real research.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<audio controls="" preload="auto" src="narration/bayesian_linear_regression_slides_with_narration_block01.mp3">
</audio>
</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<ul>
<li><strong>Bayesian Linear Regression (BLR)</strong> extends classical regression by incorporating <strong>prior information</strong> and producing <strong>posterior distributions</strong> over model parameters.<br>
</li>
<li><strong>Advantages:</strong>
<ul>
<li>Handles <strong>high-dimensional</strong> and <strong>small-sample</strong> problems.<br>
</li>
<li>Provides <strong>full uncertainty quantification</strong>.<br>
</li>
<li>Enables <strong>regularization</strong> and integration of <strong>prior biological knowledge</strong>.</li>
</ul></li>
</ul>
<aside class="notes">
<p>Unlike the classical approach, which produces single-point estimates of parameters, the Bayesian framework treats these parameters as random variables with their own probability distributions. This allows us to explicitly incorporate prior beliefs or biological information into the model. The resulting posterior distribution summarizes both the evidence from the data and the influence of the priors. One key advantage is that Bayesian models naturally handle situations where the number of predictors is large or the sample size is small. They also provide complete uncertainty quantification, enabling us to interpret results probabilistically rather than deterministically. Finally, the framework supports biologically informed priors, which can be useful when modeling genomic data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<audio controls="" preload="auto" src="narration/bayesian_linear_regression_slides_with_narration_block02.mp3">
</audio>
</section>
<section id="applications-in-genomics" class="slide level2">
<h2>Applications in Genomics</h2>
<ul>
<li><strong>Bayesian Linear Regression (BLR)</strong> is widely applied in quantitative genetics and genomics.<br>
</li>
<li><strong>Common use cases:</strong>
<ul>
<li>Genome-Wide Association Studies (<strong>GWAS</strong>) and <strong>fine-mapping</strong> of causal variants.<br>
</li>
<li><strong>Genetic prediction</strong> and <strong>heritability estimation</strong>.<br>
</li>
<li><strong>Pathway</strong> and <strong>gene-set enrichment</strong> analyses.<br>
</li>
<li>Integrative <strong>multi-omics</strong> modeling (genome, transcriptome, epigenome).</li>
</ul></li>
</ul>
<aside class="notes">
<p>In quantitative genetics, Bayesian methods are particularly useful for modeling large numbers of genetic markers simultaneously, while accounting for uncertainty in their effects. They are often used in genome-wide association studies to fine-map causal variants and quantify their contributions to complex traits. Another major application is genetic prediction, where Bayesian approaches can shrink noisy estimates and improve predictive accuracy. These models are also central to heritability estimation, helping partition genetic and environmental components of variance. Beyond single-variant analysis, Bayesian frameworks are used for pathway or gene-set enrichment studies, integrating biological structure into statistical modeling. Overall, Bayesian regression provides a flexible and coherent approach to studying molecular and genetic variation.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<audio controls="" preload="auto" src="narration/bayesian_linear_regression_slides_with_narration_block02.mp3">
</audio>
</section>
<section id="classical-linear-regression" class="slide level2">
<h2>Classical Linear Regression</h2>
<h3 id="model">Model</h3>
<p><span class="math display">\[
y = X\beta + e, \quad e \sim \mathcal{N}(0, \sigma^2 I_n)
\]</span> - <span class="math inline">\(y\)</span>: outcomes<br>
- <span class="math inline">\(X\)</span>: design matrix<br>
- <span class="math inline">\(\beta\)</span>: coefficients<br>
- <span class="math inline">\(e\)</span>: are the residuals<br>
- <span class="math inline">\(\sigma^2\)</span>: residual variance</p>
</section>
<section id="estimation" class="slide level2">
<h2>Estimation</h2>
<p>Regression effects: <span class="math display">\[
\hat{\beta} = (X^\top X)^{-1} X^\top y
\]</span></p>
<p>Residual variance: <span class="math display">\[
\hat{\sigma}^2 = \frac{1}{n-p}\sum_i (y_i - x_i^\top \hat{\beta})^2
\]</span></p>
<p>Inference via standard errors and <span class="math inline">\(t\)</span>-tests, confidence intervals, and prediction intervals.</p>
</section>
<section id="limitations" class="slide level2">
<h2>Limitations</h2>
<ul>
<li>No explicit control over <strong>effect size distribution</strong></li>
<li>Sensitive when <strong>collinearity</strong> is high</li>
<li><strong>Not identifiable</strong> when <strong><span class="math inline">\(p&gt;n\)</span></strong></li>
<li>Uncertainty largely <strong>asymptotic</strong> unless normality assumptions hold</li>
</ul>
</section>
<section id="why-bayesian-linear-regression" class="slide level2">
<h2>Why Bayesian Linear Regression?</h2>
<ul>
<li>Combines <strong>likelihood</strong> and <strong>prior</strong> to form the <strong>posterior</strong>.<br>
</li>
<li>Priors express beliefs about <strong>effect sizes</strong>:
<ul>
<li>Normal → many small effects<br>
</li>
<li>Spike-and-slab → sparse effects<br>
</li>
</ul></li>
<li>Acts as a <strong>regularizer</strong>:
<ul>
<li>Shrinks small/noisy effects toward <span class="math inline">\(0\)</span><br>
</li>
<li>Preserves large, important effects<br>
</li>
</ul></li>
<li><strong>Stable when <span class="math inline">\(p &gt; n\)</span></strong> due to prior information.<br>
</li>
<li>Provides <strong>full posterior distributions</strong> for <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
</section>
<section id="overview-bayesian-linear-regression" class="slide level2">
<h2>Overview: Bayesian Linear Regression</h2>
<ul>
<li>Combines data and prior knowledge using <strong>Bayes’ rule</strong>.<br>
</li>
<li>Uses <strong>conjugate priors</strong> to yield closed-form full conditionals.<br>
</li>
<li>Employs <strong>Gibbs sampling</strong> to approximate the posterior distribution.<br>
</li>
<li>Estimates <strong>parameters, uncertainty, and predictions</strong> from posterior draws.</li>
</ul>
</section>
<section id="bayesian-linear-regression-with-gaussian-priors" class="slide level2">
<h2>Bayesian Linear Regression with Gaussian Priors</h2>
<p>Bayesian linear regression starts with the same model structure as classical linear regression.</p>
<p><span class="math display">\[
y = X\beta + e, \quad e \sim \mathcal{N}(0, \sigma^2 I_n)
\]</span></p>
<ul>
<li><span class="math inline">\(y\)</span>: <span class="math inline">\(n \times 1\)</span> vector of observed outcomes<br>
</li>
<li><span class="math inline">\(X\)</span>: <span class="math inline">\(n \times p\)</span> design matrix of predictors<br>
</li>
<li><span class="math inline">\(\beta\)</span>: <span class="math inline">\(p \times 1\)</span> vector of unknown coefficients<br>
</li>
<li><span class="math inline">\(e\)</span>: Gaussian noise with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^2\)</span></li>
</ul>
</section>
<section id="likelihood-in-bayesian-linear-regression" class="slide level2">
<h2>Likelihood in Bayesian Linear Regression</h2>
<p>Because the residuals are Gaussian, it follows that the marginal distribution of <span class="math inline">\(y\)</span> is:</p>
<p><span class="math display">\[
e \sim \mathcal{N}(0, \sigma^2 I_n)
\]</span> The marginal distribution of <span class="math inline">\(y\)</span> is:</p>
<p><span class="math display">\[
y \sim \mathcal{N}(X\beta, \sigma^2 I_n)
\]</span> This defines the <strong>likelihood</strong> the probability of the observed data given parameters <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
p(y \mid X, \beta, \sigma^2) = \mathcal{N}(X\beta, \sigma^2 I_n)
\]</span></p>
</section>
<section id="introducing-priors" class="slide level2">
<h2>Introducing Priors</h2>
<p>In Bayesian linear regression, we specify <strong>prior distributions</strong> that express our beliefs about parameters before seeing the data.</p>
<p>A common <strong>conjugate prior</strong> for the regression coefficients is:</p>
<p><span class="math display">\[
\beta \mid \sigma_b^2 \sim \mathcal{N}(0, \sigma_b^2 I_p)
\]</span></p>
<p>This reflects the belief that most effect sizes are small and centered near zero — consistent with the <strong>polygenic assumption</strong> in genetics.</p>
</section>
<section id="role-of-the-prior-variance-sigma_b2" class="slide level2">
<h2>Role of the Prior Variance <span class="math inline">\(\sigma_b^2\)</span></h2>
<p>The parameter <span class="math inline">\(\sigma_b^2\)</span> acts as a <strong>shrinkage (regularization) parameter</strong>:</p>
<ul>
<li>Small <span class="math inline">\(\sigma_b^2\)</span> → stronger shrinkage toward zero.<br>
</li>
<li>Large <span class="math inline">\(\sigma_b^2\)</span> → weaker shrinkage, allowing larger effects.</li>
</ul>
<p>It controls the <strong>strength of regularization</strong> and is often treated as an <strong>unknown hyperparameter</strong> estimated from the data.</p>
</section>
<section id="priors-on-variance-components" class="slide level2">
<h2>Priors on Variance Components</h2>
<p>We also place priors on the variance components to complete the hierarchical model.</p>
<p><span class="math display">\[
\sigma_b^2 \mid S_b, v_b \sim S_b \, \chi^{-2}(v_b), \quad
\sigma^2 \mid S, v \sim S \, \chi^{-2}(v)
\]</span></p>
<p>Here:</p>
<ul>
<li><span class="math inline">\(S_b\)</span> and <span class="math inline">\(v_b\)</span> are user-defined hyperparameters that control the prior distribution on the <strong>variance of regression coefficients</strong>.<br>
</li>
<li><span class="math inline">\(S\)</span> and <span class="math inline">\(v\)</span> are hyperparameters for the <strong>residual variance</strong> <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
</section>
<section id="conjugate-priors-and-regularization" class="slide level2">
<h2>Conjugate Priors and Regularization</h2>
<p>Conjugate priors keep posteriors in the same family<br>
(e.g., scaled inverse-chi-squared), allowing <strong>closed-form Gibbs updates</strong>.</p>
<p>They also serve as <strong>regularizers</strong>:</p>
<ul>
<li>The prior on <span class="math inline">\(\beta\)</span> shrinks small or noisy effects toward zero.<br>
</li>
<li>Priors on variance components prevent overfitting, especially when <span class="math inline">\(p &gt; n\)</span>.</li>
</ul>
<p>Thus, conjugate priors make Bayesian linear regression <strong>efficient</strong> and <strong>stable</strong>.</p>
</section>
<section id="posterior-distribution" class="slide level2">
<h2>Posterior Distribution</h2>
<p>In Bayesian analysis, we combine the <strong>likelihood</strong> and <strong>priors</strong> using Bayes’ rule to obtain the <strong>joint posterior</strong>:</p>
<p><span class="math display">\[
p(\beta, \sigma_b^2, \sigma^2 \mid y) \propto
p(y \mid \beta, \sigma^2)\;
p(\beta \mid \sigma_b^2)\;
p(\sigma_b^2)\;
p(\sigma^2)
\]</span></p>
<p>This posterior captures all <strong>updated knowledge</strong> about the unknown parameters after observing the data.<br>
It forms the basis for computing <strong>posterior means</strong>, <strong>credible intervals</strong>, and <strong>predictions</strong>.</p>
</section>
<section id="conjugacy-and-gibbs-sampling" class="slide level2">
<h2>Conjugacy and Gibbs Sampling</h2>
<p>With <strong>conjugate priors</strong>, each parameter’s <strong>full conditional distribution</strong> has a closed-form solution.<br>
This makes <strong>Gibbs sampling</strong> a natural and efficient inference method.</p>
<ul>
<li>Parameters are updated one at a time, each from its conditional posterior.<br>
</li>
<li>The resulting Markov chain explores the <strong>joint posterior</strong> of<br>
<span class="math inline">\((\beta, \sigma_b^2, \sigma^2)\)</span>.</li>
</ul>
<p>Gibbs sampling thus provides an easy way to approximate the full posterior in Bayesian linear regression.</p>
</section>
<section id="full-conditional-for-beta" class="slide level2">
<h2>Full Conditional for <span class="math inline">\(\beta\)</span></h2>
<p>Given <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\sigma_b^2\)</span>, and the data <span class="math inline">\(y\)</span>, the regression coefficients have a <strong>multivariate normal</strong> conditional posterior:</p>
<p><span class="math display">\[
\beta \mid \sigma^2, \sigma_b^2, y \sim
\mathcal{N}(\mu_\beta, \Sigma_\beta)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\Sigma_\beta = \left( \frac{X^\top X}{\sigma^2} + \frac{I}{\sigma_b^2} \right)^{-1},
\quad
\mu_\beta = \Sigma_\beta \frac{X^\top y}{\sigma^2}
\]</span></p>
<p>This distribution represents our <strong>updated belief</strong> about <span class="math inline">\(\beta\)</span><br>
after observing the data, while holding <span class="math inline">\(\sigma_b^2\)</span> and <span class="math inline">\(\sigma^2\)</span> fixed.</p>
</section>
<section id="comparison-to-classical-ols" class="slide level2">
<h2>Comparison to Classical OLS</h2>
<p>In classical regression, the OLS estimator is</p>
<p><span class="math display">\[
\hat\beta_{\text{OLS}} = (X^\top X)^{-1} X^\top y,
\quad y \sim \mathcal{N}(X\beta, \sigma^2 I)
\]</span></p>
<p>The estimate of <span class="math inline">\(\beta\)</span> is <strong>independent of <span class="math inline">\(\sigma^2\)</span></strong>,<br>
since <span class="math inline">\(\sigma^2\)</span> only scales the likelihood, not its maximum.</p>
<p>In Bayesian regression, <span class="math inline">\(\sigma^2\)</span> appears explicitly in the posterior:</p>
<p><span class="math display">\[
\Sigma_\beta = \left( \frac{X^\top X}{\sigma^2} + \frac{I}{\sigma_b^2} \right)^{-1},
\quad
\mu_\beta = \Sigma_\beta \frac{X^\top y}{\sigma^2}
\]</span></p>
<p>The term <span class="math inline">\(\frac{I}{\sigma_b^2}\)</span> introduces <strong>shrinkage</strong>, regularizing estimates and stabilizing inference especially when <span class="math inline">\(p &gt; n\)</span> or predictors are highly correlated.</p>
<p>Thus, the Bayesian posterior mean is a <strong>regularized, uncertainty-aware generalization</strong> of OLS.</p>
</section>
<section id="full-conditional-for-beta_j" class="slide level2">
<h2>Full Conditional for <span class="math inline">\(\beta_j\)</span></h2>
<p>Instead of sampling <span class="math inline">\(\beta\)</span> jointly, we can update each coefficient <span class="math inline">\(\beta_j\)</span> <strong>one at a time</strong>, holding all others fixed efficient for large <span class="math inline">\(p\)</span> or spike-and-slab models.</p>
<p>Let <span class="math inline">\(X_j\)</span> be the <span class="math inline">\(j\)</span>th column of <span class="math inline">\(X\)</span> and define the <strong>partial residual</strong>:</p>
<p><span class="math display">\[
r_j = y - X_{-j} \beta_{-j}
\]</span></p>
<p>Then the conditional posterior for <span class="math inline">\(\beta_j\)</span> is univariate normal:</p>
<p><span class="math display">\[
\beta_j \mid D \sim \mathcal{N} \!\left(
\frac{X_j^\top r_j}{X_j^\top X_j + \sigma^2 / \sigma_b^2},\;
\frac{\sigma^2}{X_j^\top X_j + \sigma^2 / \sigma_b^2}
\right)
\]</span></p>
<p>This corresponds to a <strong>regularized least-squares update</strong>. Residual updates <strong>avoid matrix inversion</strong>, scale to high dimensions, and extend naturally to <strong>sparse (spike-and-slab)</strong> models.</p>
</section>
<section id="full-conditional-for-sigma_b2" class="slide level2">
<h2>Full Conditional for <span class="math inline">\(\sigma_b^2\)</span></h2>
<p>The conditional distribution of the <strong>prior variance</strong> <span class="math inline">\(\sigma_b^2\)</span>, given <span class="math inline">\(\beta\)</span> and the hyperparameters, is a <strong>scaled inverse-chi-squared</strong>:</p>
<p><span class="math display">\[
\sigma_b^2 \mid \beta \sim \tilde{S}_b \, \chi^{-2}(\tilde{v}_b)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\tilde{v}_b = v_b + p, \quad
\tilde{S}_b = \frac{\beta^\top \beta + v_b S_b}{\tilde{v}_b}
\]</span></p>
<p>At each Gibbs iteration, <span class="math inline">\(\sigma_b^2\)</span> is sampled directly given <span class="math inline">\(\beta\)</span>. This update reflects our revised belief about the <strong>variability of effect sizes</strong> after observing the current posterior draw of <span class="math inline">\(\beta\)</span>.</p>
</section>
<section id="full-conditional-for-sigma2" class="slide level2">
<h2>Full Conditional for <span class="math inline">\(\sigma^2\)</span></h2>
<p>The conditional distribution of the <strong>residual variance</strong> <span class="math inline">\(\sigma^2\)</span>,<br>
given <span class="math inline">\(\beta\)</span> and the data, is also <strong>scaled inverse-chi-squared</strong>:</p>
<p><span class="math display">\[
\sigma^2 \mid \beta, y \sim \tilde{S} \, \chi^{-2}(\tilde{v})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\tilde{v} = v + n, \quad
\tilde{S} = \frac{(y - X\beta)^\top (y - X\beta) + v S}{\tilde{v}}
\]</span></p>
<p>At each Gibbs iteration, <span class="math inline">\(\sigma^2\)</span> is sampled directly given <span class="math inline">\(\beta\)</span>.<br>
This captures our updated belief about the <strong>residual variability</strong><br>
after accounting for the current linear predictor <span class="math inline">\(X\beta\)</span>.</p>
</section>
<section id="gibbs-sampling-motivation" class="slide level2">
<h2>Gibbs Sampling: Motivation</h2>
<p>Bayesian inference often involves <strong>complex posteriors</strong> that lack closed-form solutions. To approximate these, we use <strong>Markov Chain Monte Carlo (MCMC)</strong> methods.</p>
<p>MCMC builds a <strong>Markov chain</strong> whose stationary distribution is the target posterior. Once the chain has <strong>converged</strong>, its samples can be used to estimate:</p>
<ul>
<li>Posterior means, variances, and credible intervals<br>
</li>
<li>Predictive distributions<br>
</li>
<li>Other functions of interest</li>
</ul>
<p>Among MCMC algorithms, the <strong>Gibbs sampler</strong> is especially useful when all <strong>full conditional distributions</strong> are available in <strong>closed form</strong>.</p>
</section>
<section id="gibbs-sampling-the-algorithm" class="slide level2">
<h2>Gibbs Sampling: The Algorithm</h2>
<p>For Bayesian linear regression with conjugate priors, the joint posterior is:</p>
<p><span class="math display">\[
p(\beta, \sigma_b^2 , \sigma^2 \mid y) \propto
p(y \mid \beta, \sigma^2)\; p(\beta \mid \sigma_b^2)\; p(\sigma_b^2)\; p(\sigma^2)
\]</span></p>
<p>We iteratively draw from the following <strong>full conditionals</strong>:</p>
<ol type="1">
<li>Sample <span class="math inline">\(\beta \mid \sigma_b^2, \sigma^2, y\)</span><br>
</li>
<li>Sample <span class="math inline">\(\sigma_b^2 \mid \beta\)</span><br>
</li>
<li>Sample <span class="math inline">\(\sigma^2 \mid \beta, y\)</span></li>
</ol>
<p>Each step updates one parameter given the latest values of the others. Repeating this sequence yields samples from the <strong>joint posterior</strong> <span class="math inline">\(p(\beta, \sigma_b^2, \sigma^2 \mid y)\)</span>.</p>
<p>Because each conditional is <strong>standard</strong> (Normal or scaled inverse-<span class="math inline">\(\chi^2\)</span>), Gibbs sampling is both <strong>efficient</strong> and <strong>easy to implement</strong>.</p>
</section>
<section id="posterior-summaries" class="slide level2">
<h2>Posterior Summaries</h2>
<p>After running the Gibbs sampler, we obtain posterior draws <span class="math inline">\(\{\theta^{(t)}\}_{t=1}^T\)</span> for parameters such as <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(\sigma^2\)</span>, or <span class="math inline">\(\sigma_b^2\)</span>.</p>
<p>We summarize the posterior distribution via:</p>
<ul>
<li><strong>Posterior mean</strong><br>
<span class="math display">\[
\mathbb{E}[\theta \mid y] \approx \frac{1}{T} \sum_{t=1}^{T} \theta^{(t)}
\]</span></li>
<li><strong>Posterior median</strong>: the median of <span class="math inline">\(\theta^{(t)}\)</span></li>
<li><strong>Credible interval (95%)</strong><br>
<span class="math display">\[
[\theta]_{0.025}, [\theta]_{0.975}
\]</span></li>
</ul>
<p>These summaries describe the most probable values of <span class="math inline">\(\theta\)</span><br>
and their uncertainty after combining data and prior beliefs.</p>
</section>
<section id="estimating-uncertainty" class="slide level2">
<h2>Estimating Uncertainty</h2>
<p>Bayesian inference provides <strong>full posterior distributions</strong>, not just point estimates. Uncertainty is quantified directly from the posterior samples:</p>
<ul>
<li><strong>Posterior standard deviation</strong><br>
<span class="math display">\[
\mathrm{SD}(\theta \mid y) \approx
\sqrt{\frac{1}{T-1} \sum_{t=1}^{T} (\theta^{(t)} - \bar{\theta})^2}
\]</span></li>
</ul>
<p>The <strong>width</strong> of the credible interval reflects this uncertainty. Parameters with broader posteriors are estimated with less precision, and the degree of uncertainty depends on both the data and the prior.</p>
</section>
<section id="posterior-prediction" class="slide level2">
<h2>Posterior Prediction</h2>
<p>Given a new observation <span class="math inline">\(x_{\text{new}}\)</span>, we can predict using posterior draws:</p>
<ol type="1">
<li>Compute predicted means for each sample: <span class="math display">\[
\hat{y}_{\text{new}}^{(t)} = x_{\text{new}}^\top \beta^{(t)}
\]</span></li>
<li>Add residual uncertainty: <span class="math display">\[
y_{\text{new}}^{(t)} \sim
\mathcal{N}\!\left(x_{\text{new}}^\top \beta^{(t)},\; \sigma^{2(t)}\right)
\]</span></li>
</ol>
<p>The resulting samples <span class="math inline">\(\{y_{\text{new}}^{(t)}\}\)</span> form a <strong>posterior predictive distribution</strong>, from which we can derive <strong>predictive intervals</strong> and evaluate <strong>predictive accuracy</strong>.</p>
</section>
<section id="model-checking-and-hypothesis-testing" class="slide level2">
<h2>Model Checking and Hypothesis Testing</h2>
<p>Posterior samples enable rich <strong>model diagnostics</strong> and <strong>hypothesis testing</strong>:</p>
<ul>
<li><p><strong>Posterior probability of an event</strong><br>
<span class="math display">\[
\Pr(\beta_j \ne 0 \mid y)
\approx \frac{1}{T} \sum_{t=1}^{T} \mathbf{1}\!\left(\beta_j^{(t)} \ne 0\right)
\]</span></p></li>
<li><p><strong>Posterior predictive checks</strong><br>
Simulate new datasets using posterior draws and compare them to the observed data to assess model fit.</p></li>
<li><p><strong>Model comparison</strong><br>
Bayes factors and marginal likelihoods can be approximated to formally test or compare competing models.</p></li>
</ul>
<p>These tools extend Bayesian inference beyond estimation to <strong>model validation</strong>, <strong>uncertainty quantification</strong>, and <strong>decision-making</strong>.</p>
</section>
<section id="convergence-diagnostics" class="slide level2">
<h2>Convergence Diagnostics</h2>
<p>Before interpreting MCMC results, we must check that the Gibbs sampler has <strong>converged</strong> to the target posterior distribution.</p>
<p>Convergence diagnostics assess whether the Markov chain has reached its <strong>stationary distribution</strong> and is producing valid samples.</p>
<p>Two basic strategies are:</p>
<ul>
<li><strong>Burn-in</strong> – Discard early iterations (e.g., first 1000) to remove dependence on starting values.<br>
</li>
<li><strong>Thinning</strong> – Keep every <span class="math inline">\(k\)</span>-th sample to reduce autocorrelation.</li>
</ul>
<p>These steps improve sample quality and ensure reliable posterior summaries.</p>
</section>
<section id="trace-plots" class="slide level2">
<h2>Trace Plots</h2>
<p>A simple yet powerful diagnostic is the <strong>trace plot</strong>,<br>
showing sampled parameter values <span class="math inline">\(\theta^{(t)}\)</span> over iterations <span class="math inline">\(t\)</span>.</p>
<ul>
<li>A <strong>converged chain</strong> fluctuates around a stable mean — no trend or drift.<br>
</li>
<li>Multiple chains from different starting points should <strong>overlap</strong> and <strong>mix well</strong>.</li>
</ul>
<p>Trace plots help detect: - Lack of stationarity (upward/downward trends) - Poor mixing or multimodality - Burn-in issues</p>
<p>Visual inspection is often the <strong>first step</strong> in assessing convergence.</p>
</section>
<section id="autocorrelation" class="slide level2">
<h2>Autocorrelation</h2>
<p>Samples from a Gibbs sampler are <strong>correlated</strong>, especially for tightly coupled parameters.<br>
The <strong>autocorrelation function (ACF)</strong> quantifies dependence across lags <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
\hat{\rho}_k =
\frac{\sum_{t=1}^{T-k} (\theta^{(t)} - \bar{\theta})(\theta^{(t+k)} - \bar{\theta})}
     {\sum_{t=1}^{T} (\theta^{(t)} - \bar{\theta})^2}
\]</span></p>
<ul>
<li>High <span class="math inline">\(\hat{\rho}_k\)</span> → slow mixing and fewer effective samples<br>
</li>
<li>Low <span class="math inline">\(\hat{\rho}_k\)</span> → better mixing and faster convergence</li>
</ul>
<p>Reducing autocorrelation may require <strong>more iterations</strong>,<br>
<strong>reparameterization</strong>, or <strong>thinning</strong> the chain.</p>
</section>
<section id="effective-sample-size-ess" class="slide level2">
<h2>Effective Sample Size (ESS)</h2>
<p>Autocorrelation reduces the number of <em>independent</em> samples obtained.<br>
The <strong>effective sample size (ESS)</strong> adjusts for this:</p>
<p><span class="math display">\[
\text{ESS}(\theta) =
\frac{T}{1 + 2 \sum_{k=1}^{K} \hat{\rho}_k}
\]</span></p>
<ul>
<li>Small ESS → chain is highly correlated, less informative<br>
</li>
<li>Rule of thumb: <span class="math inline">\(\text{ESS} &gt; 100\)</span> per parameter for stable inference</li>
</ul>
<p>ESS provides a quantitative measure of <strong>sampling efficiency</strong><br>
and helps determine whether more iterations are needed.</p>
</section>
<section id="gelmanrubin-diagnostic-hatr" class="slide level2">
<h2>Gelman–Rubin Diagnostic (<span class="math inline">\(\hat{R}\)</span>)</h2>
<p>When running multiple chains, the <strong>Gelman–Rubin statistic</strong> compares between-chain and within-chain variability.</p>
<p>For <span class="math inline">\(m\)</span> chains with <span class="math inline">\(T\)</span> iterations each:</p>
<p><span class="math display">\[
W = \frac{1}{m} \sum_{i=1}^{m} s_i^2, \quad
B = \frac{T}{m-1} \sum_{i=1}^{m} (\bar{\theta}_i - \bar{\theta})^2
\]</span></p>
<p>The potential scale reduction factor:</p>
<p><span class="math display">\[
\hat{R} = \sqrt{ \frac{\hat{V}}{W} }, \quad
\hat{V} = \frac{T-1}{T} W + \frac{1}{T} B
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{R} \approx 1\)</span> → convergence achieved<br>
</li>
<li><span class="math inline">\(\hat{R} &gt; 1.1\)</span> → chains have <strong>not converged</strong></li>
</ul>
</section>
<section id="geweke-diagnostic" class="slide level2">
<h2>Geweke Diagnostic</h2>
<p>The <strong>Geweke test</strong> checks whether early and late portions of a single chain have the same mean, indicating <strong>stationarity</strong>.</p>
<p><span class="math display">\[
Z =
\frac{\bar{\theta}_A - \bar{\theta}_B}
     {\sqrt{\text{Var}(\bar{\theta}_A) + \text{Var}(\bar{\theta}_B)}}
\]</span></p>
<p>Typically:</p>
<ul>
<li>Segment A = first 10% of the chain<br>
</li>
<li>Segment B = last 50% of the chain</li>
</ul>
<p>Under convergence, <span class="math inline">\(Z \sim \mathcal{N}(0,1)\)</span>.</p>
<ul>
<li><span class="math inline">\(|Z| \le 2\)</span> → chain likely stationary<br>
</li>
<li><span class="math inline">\(|Z| &gt; 2\)</span> → potential non-convergence</li>
</ul>
<p>These diagnostics ensure that posterior summaries reflect the <strong>true target distribution</strong>.</p>
</section>
<section id="spike-and-slab-bayesian-linear-regression" class="slide level2">
<h2>Spike-and-Slab Bayesian Linear Regression</h2>
<p>As in classical BLR, the outcome is modeled as:</p>
<p><span class="math display">\[
y = Xb + e, \quad e \sim \mathcal{N}(0, \sigma^2 I_n)
\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the <span class="math inline">\(n \times 1\)</span> response, <span class="math inline">\(X\)</span> the design matrix, <span class="math inline">\(b\)</span> the regression coefficients, and <span class="math inline">\(\sigma^2\)</span> the residual variance.</p>
<p>This defines the <strong>likelihood</strong>:</p>
<p><span class="math display">\[
y \mid b, \sigma^2 \sim \mathcal{N}(Xb, \sigma^2 I_n)
\]</span></p>
<p>The goal is to estimate <span class="math inline">\(b\)</span> and identify which predictors truly contribute to <span class="math inline">\(y\)</span>.</p>
</section>
<section id="motivation-for-the-spike-and-slab-prior" class="slide level2">
<h2>Motivation for the Spike-and-Slab Prior</h2>
<p>In standard Bayesian linear regression:</p>
<p><span class="math display">\[
\beta_j \sim \mathcal{N}(0, \sigma_b^2)
\]</span></p>
<p>This <strong>Gaussian (shrinkage) prior</strong> assumes all predictors have small effects, but it does <strong>not allow exact zeros</strong> — limiting variable selection.</p>
<p>The <strong>spike-and-slab prior</strong> addresses this by mixing two components:</p>
<ul>
<li>A <strong>spike</strong> at zero → excluded predictors<br>
</li>
<li>A <strong>slab</strong> (wide normal) → active predictors</li>
</ul>
<p>This yields <strong>sparse</strong>, interpretable models that select relevant variables.</p>
</section>
<section id="the-spike-and-slab-mixture-prior" class="slide level2">
<h2>The Spike-and-Slab Mixture Prior</h2>
<p>Each regression effect is drawn from a two-component mixture:</p>
<p><span class="math display">\[
p(b_i \mid \sigma_b^2, \pi)
= \pi\, \mathcal{N}(0, \sigma_b^2) + (1-\pi)\, \delta_0
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\pi\)</span> = prior probability that <span class="math inline">\(b_i\)</span> is non-zero<br>
</li>
<li><span class="math inline">\(\delta_0\)</span> = point mass at zero</li>
</ul>
<p>Thus, with probability <span class="math inline">\(\pi\)</span> a predictor is active (slab), and with probability <span class="math inline">\(1-\pi\)</span> it is excluded (spike).</p>
</section>
<section id="advantages-of-spike-and-slab-priors" class="slide level2">
<h2>Advantages of Spike-and-Slab Priors</h2>
<p>This hierarchical mixture prior provides several benefits:</p>
<ul>
<li><strong>Sparsity</strong> — allows exact zeros for irrelevant predictors<br>
</li>
<li><strong>Interpretability</strong> — binary indicators give posterior inclusion probabilities (PIPs)<br>
</li>
<li><strong>Adaptivity</strong> — the inclusion probability <span class="math inline">\(\pi\)</span> is learned from the data<br>
</li>
<li><strong>Balance</strong> — captures both strong signals (detection) and small effects (prediction)</li>
</ul>
<p>Hence, spike-and-slab models combine <strong>variable selection</strong> with <strong>Bayesian uncertainty quantification</strong>.</p>
</section>
<section id="hierarchical-representation" class="slide level2">
<h2>Hierarchical Representation</h2>
<p>We express each effect as:</p>
<p><span class="math display">\[
b_i = \alpha_i \, \delta_i
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\alpha_i \mid \sigma_b^2 \sim \mathcal{N}(0, \sigma_b^2), \quad
\delta_i \mid \pi \sim \text{Bernoulli}(\pi)
\]</span></p>
<ul>
<li><span class="math inline">\(\alpha_i\)</span>: effect size when predictor is included<br>
</li>
<li><span class="math inline">\(\delta_i\)</span>: binary inclusion indicator (0 or 1)</li>
</ul>
<p>Marginalizing over <span class="math inline">\(\delta_i\)</span> yields the spike-and-slab mixture prior above.</p>
</section>
<section id="prior-for-the-inclusion-probability-pi" class="slide level2">
<h2>Prior for the Inclusion Probability <span class="math inline">\(\pi\)</span></h2>
<p>The overall sparsity level is controlled by <span class="math inline">\(\pi\)</span>, assigned a <strong>Beta prior</strong>:</p>
<p><span class="math display">\[
\pi \sim \text{Beta}(\alpha, \beta)
\]</span></p>
<ul>
<li>Small <span class="math inline">\(\alpha\)</span>, large <span class="math inline">\(\beta\)</span> → favor sparser models<br>
</li>
<li><span class="math inline">\(\alpha = \beta = 1\)</span> → uniform prior<br>
</li>
<li>Larger <span class="math inline">\(\alpha\)</span> → denser models</li>
</ul>
<p>This prior lets the <strong>data determine the degree of sparsity</strong>.</p>
</section>
<section id="priors-for-variance-components" class="slide level2">
<h2>Priors for Variance Components</h2>
<p>Variance parameters use <strong>scaled inverse-chi-squared</strong> priors:</p>
<p><span class="math display">\[
\sigma_b^2 \sim S_b \chi^{-2}(v_b), \quad
\sigma^2 \sim S \chi^{-2}(v)
\]</span></p>
<p>These are conjugate, providing closed-form conditional updates. Hyperparameters <span class="math inline">\((S_b, v_b)\)</span> and <span class="math inline">\((S, v)\)</span> encode prior beliefs about effect size variability and residual noise.</p>
</section>
<section id="joint-posterior-structure" class="slide level2">
<h2>Joint Posterior Structure</h2>
<p>Combining the likelihood and priors, the joint posterior is:</p>
<p><span class="math display">\[
p(\mu, \alpha, \delta, \pi, \sigma_b^2, \sigma^2 \mid y)
\propto
p(y \mid \mu, \alpha, \delta, \sigma^2)\,
p(\alpha \mid \sigma_b^2)\,
p(\delta \mid \pi)\,
p(\pi)\,
p(\sigma_b^2)\,
p(\sigma^2)
\]</span></p>
<p>This captures our <strong>updated beliefs</strong> about effects, inclusion indicators, and variance components.</p>
</section>
<section id="gibbs-sampling-for-spike-and-slab-blr" class="slide level2">
<h2>Gibbs Sampling for Spike-and-Slab BLR</h2>
<p>Inference proceeds via <strong>Gibbs sampling</strong>, cycling through these conditional updates:</p>
<ol type="1">
<li><span class="math inline">\(\alpha \mid D\)</span><br>
</li>
<li><span class="math inline">\(\delta \mid D\)</span><br>
</li>
<li><span class="math inline">\(\pi \mid D\)</span><br>
</li>
<li><span class="math inline">\(\sigma_b^2 \mid D\)</span><br>
</li>
<li><span class="math inline">\(\sigma^2 \mid D\)</span></li>
</ol>
<p>Here, <strong><span class="math inline">\(D\)</span></strong> denotes the data and all other current parameter values.<br>
Each conditional follows a <strong>standard distribution</strong> (Normal, Bernoulli, Beta, scaled-<span class="math inline">\(\chi^{-2}\)</span>).<br>
Iterating these updates generates samples from the joint posterior.</p>
</section>
<section id="posterior-inclusion-probabilities" class="slide level2">
<h2>Posterior Inclusion Probabilities</h2>
<p>The <strong>posterior inclusion probability (PIP)</strong> measures how likely each predictor is truly associated with <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
\widehat{\Pr}(\delta_i = 1 \mid y)
= \frac{1}{T} \sum_{t=1}^{T} \delta_i^{(t)}
\]</span></p>
<ul>
<li>High PIP → predictor is likely important<br>
</li>
<li>Low PIP → predictor likely irrelevant</li>
</ul>
<p>PIPs summarize <strong>variable relevance</strong> and drive <strong>Bayesian feature selection</strong>.</p>
</section>
<section id="summary-of-bayesian-linear-regression" class="slide level2">
<h2>Summary of Bayesian Linear Regression</h2>
<p><strong>Bayesian Linear Regression</strong> combines <strong>likelihood</strong> and <strong>prior</strong> to form the <strong>posterior</strong>, enabling principled modeling, regularization, and uncertainty quantification.</p>
<ul>
<li>Inference via <strong>MCMC</strong> (often <strong>Gibbs sampling</strong>) with posterior draws for <strong>means</strong>, <strong>credible intervals</strong>, and <strong>predictions</strong>.<br>
</li>
<li><strong>Spike-and-slab priors</strong> enable <strong>sparsity</strong> and <strong>variable selection</strong>, assigning <strong>exact zeros</strong> to irrelevant predictors and identifying key variables via <strong>posterior inclusion probabilities (PIPs)</strong>.<br>
</li>
<li><strong>Conjugate and mixture priors</strong> yield <strong>efficient</strong> and <strong>robust inference</strong>, even when <strong><span class="math inline">\(p &gt; n\)</span></strong>.<br>
</li>
<li>With <strong>proper convergence checks</strong>, Bayesian models provide <strong>stable and reliable inference</strong> across a wide range of data settings.</li>
</ul>
</section>
<section id="applications-in-genomics-1" class="slide level2">
<h2>Applications in Genomics</h2>
<p>We have now seen the basic framework of <strong>Bayesian Linear Regression (BLR)</strong><br>
and will illustrate how it provides a <strong>unified approach</strong> for analyzing genetic and genomic data.</p>
<ul>
<li><strong>Genome-Wide Association Studies (GWAS)</strong> and <strong>fine-mapping</strong> of causal variants.<br>
</li>
<li><strong>Genetic prediction</strong> and <strong>heritability estimation</strong>.<br>
</li>
<li><strong>Pathway</strong> and <strong>gene-set enrichment</strong> analyses.</li>
</ul>
<p>These examples show how BLR connects <strong>statistical modeling</strong> with <strong>biological interpretation</strong> in quantitative genetics.</p>
<script>
document.addEventListener("DOMContentLoaded", function() {
  const backBtn = document.createElement("a");
  backBtn.href = "index.html";
  backBtn.target = "_self";
  backBtn.textContent = "← Back to Main Page";
  backBtn.style.cssText = `
    position: fixed; top: 10px; right: 20px;
    background: #fff; color: #000;
    padding: 8px 14px; border-radius: 6px;
    border: 1px solid #000;
    font-weight: 600; font-family: sans-serif;
    text-decoration: none; font-size: 14px;
    box-shadow: 0 0 6px rgba(0,0,0,0.15);
    transition: all 0.2s ease-in-out;
    z-index: 9999;
  `;
  backBtn.onmouseover = () => {
    backBtn.style.background = '#000';
    backBtn.style.color = '#fff';
  };
  backBtn.onmouseout = () => {
    backBtn.style.background = '#fff';
    backBtn.style.color = '#000';
  };
  document.body.appendChild(backBtn);
});
</script>



</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    

    <script>

      // htmlwidgets need to know to resize themselves when slides are shown/hidden.

      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current

      // slide changes (different for each slide format).

      (function () {

        // dispatch for htmlwidgets

        function fireSlideEnter() {

          const event = window.document.createEvent("Event");

          event.initEvent("slideenter", true, true);

          window.document.dispatchEvent(event);

        }

    

        function fireSlideChanged(previousSlide, currentSlide) {

          fireSlideEnter();

    

          // dispatch for shiny

          if (window.jQuery) {

            if (previousSlide) {

              window.jQuery(previousSlide).trigger("hidden");

            }

            if (currentSlide) {

              window.jQuery(currentSlide).trigger("shown");

            }

          }

        }

    

        // hookup for slidy

        if (window.w3c_slidy) {

          window.w3c_slidy.add_observer(function (slide_num) {

            // slide_num starts at position 1

            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);

          });

        }

    

      })();

    </script>

    

    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>