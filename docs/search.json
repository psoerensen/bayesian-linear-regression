[
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "",
    "text": "Bayesian linear regression (BLR) extends the classical linear regression framework by incorporating prior information into the model and producing full posterior distributions over parameters, rather than single-point estimates. This approach offers several key advantages, particularly in the context of modern data analysis challenges such as high dimensionality, small sample sizes, and the need for uncertainty quantification.\nIn genomics and other biological applications, BLR is widely used for tasks such as mapping genetic variants, predicting genetic predisposition (e.g., polygenic risk scores), estimating genetic parameters like heritability, and performing gene set enrichment or pathway analyses. These applications benefit from BLR’s ability to unify inference and prediction within a probabilistic framework.\nThe BLR model builds on the familiar linear regression formulation, where the observed outcome is modeled as a linear function of predictors plus Gaussian noise. However, unlike classical inference—which relies on least squares or maximum likelihood estimation and provides only point estimates and asymptotic intervals—Bayesian inference yields full posterior distributions over the unknown coefficients and variance. This allows for richer uncertainty quantification and more robust inference.\nSeveral motivations drive the use of Bayesian methods in linear regression. First, BLR naturally quantifies uncertainty through posterior distributions, allowing the analyst to compute credible intervals, posterior probabilities, and predictive distributions. Second, prior distributions act as regularizers, helping to stabilize estimation in noisy or underdetermined settings, such as when the number of predictors \\(p\\) exceeds the number of observations \\(n\\). Gaussian priors encourage shrinkage toward zero, while more structured priors (such as spike-and-slab) enable sparse or grouped solutions. Third, BLR makes it straightforward to incorporate external knowledge—such as biological relevance or prior experimental results—into the modeling process.\nThese notes begin by reviewing the classical linear regression model and its limitations. We then introduce the Bayesian linear regression model, outline the inference workflow, and show how to derive the full conditional posterior distributions for the model parameters using conjugate priors. Finally, we describe how posterior inference is performed using Gibbs sampling and conclude with practical considerations for implementation, diagnostics, and applications in R."
  },
  {
    "objectID": "notes.html#posterior-summaries-and-inference-from-gibbs-samples",
    "href": "notes.html#posterior-summaries-and-inference-from-gibbs-samples",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Summaries and Inference from Gibbs Samples",
    "text": "Posterior Summaries and Inference from Gibbs Samples\nAfter running the Gibbs sampler and obtaining \\(T\\) posterior draws of all parameters, we can use these samples to compute a wide range of quantities relevant to Bayesian inference. These include:\n\nPosterior means and medians as point estimates of parameters\nCredible intervals to quantify uncertainty\nPosterior standard deviations as measures of variability\nPosterior probabilities of hypotheses, such as \\(\\Pr(\\beta_j &gt; 0 \\mid y)\\)\nPosterior predictive distributions for new observations\nModel diagnostics such as convergence checks or residual analysis\n\nThese quantities allow us to summarize uncertainty, generate predictions, and make probabilistic statements about model parameters and data.\n\nPosterior Summaries\nOnce we have a collection of posterior draws for a parameter \\(\\theta\\) (e.g., \\(\\beta_j\\), \\(\\sigma^2\\), or \\(\\sigma_b^2\\)), we can summarize the posterior distribution using:\n\nPosterior mean: \\[\n\\mathbb{E}[\\theta \\mid y] \\approx \\frac{1}{T} \\sum_{t=1}^{T} \\theta^{(t)}\n\\]\nPosterior median: The median value of the sampled \\(\\theta^{(t)}\\).\nCredible intervals: For example, a 95% credible interval for \\(\\theta\\) can be obtained as the 2.5% and 97.5% quantiles of the posterior samples: \\[\n[\\theta]_{0.025}, [\\theta]_{0.975}\n\\]\n\nThese summaries provide insight into the likely values of the parameter after accounting for uncertainty in both the data and prior beliefs.\n\n\nEstimating Uncertainty\nBayesian inference provides full posterior distributions, not just point estimates. This allows us to directly quantify the uncertainty of parameters:\n\nPosterior standard deviation: \\[\n\\mathrm{SD}(\\theta \\mid y) \\approx \\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T} \\left( \\theta^{(t)} - \\bar{\\theta} \\right)^2}\n\\]\nThis uncertainty is reflected in the width of the credible intervals and can vary across different parameters or under different priors.\n\n\n\nPrediction\nGiven a new observation \\(x_{\\text{new}}\\), we can generate posterior predictive distributions using the sampled parameter values:\n\nFor each draw \\(t\\), compute: \\[\n\\hat{y}_{\\text{new}}^{(t)} = x_{\\text{new}}^\\top \\beta^{(t)}\n\\]\nOptionally, add residual noise from the corresponding draw of \\(\\sigma^{2(t)}\\): \\[\ny_{\\text{new}}^{(t)} \\sim \\mathcal{N}\\left(x_{\\text{new}}^\\top \\beta^{(t)},\\; \\sigma^{2(t)} \\right)\n\\]\nUse these \\(y_{\\text{new}}^{(t)}\\) samples to construct predictive intervals or evaluate predictive performance.\n\n\n\nModel Checking and Hypothesis Testing\nThe posterior draws can also be used for model diagnostics or hypothesis testing:\n\nPosterior probability of an event, such as a non-zero effect: \\[\n\\Pr(\\beta_j \\ne 0 \\mid y) \\approx \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{1}\\left( \\beta_j^{(t)} \\ne 0 \\right)\n\\]\nPosterior predictive checks: Simulate new datasets from the model using posterior draws and compare them to the observed data. Discrepancies may indicate model misfit.\nBayes factors and marginal likelihoods can be computed or approximated for formal hypothesis testing or model comparison, though these often require specialized methods beyond standard Gibbs output.\n\nThese procedures allow us to move beyond point estimates and engage in a full Bayesian analysis that accounts for uncertainty in parameter estimation, prediction, and decision-making."
  },
  {
    "objectID": "classical_linear_regression_simulation.html",
    "href": "classical_linear_regression_simulation.html",
    "title": "Classical Linear Regression Simulation in R",
    "section": "",
    "text": "This document demonstrates how to simulate data from a classical linear regression model and estimate parameters using Ordinary Least Squares (OLS), including inference and prediction intervals.\nWe also visualize the estimated vs. true regression coefficients."
  },
  {
    "objectID": "classical_linear_regression_simulation.html#introduction",
    "href": "classical_linear_regression_simulation.html#introduction",
    "title": "Classical Linear Regression Simulation in R",
    "section": "",
    "text": "This document demonstrates how to simulate data from a classical linear regression model and estimate parameters using Ordinary Least Squares (OLS), including inference and prediction intervals.\nWe also visualize the estimated vs. true regression coefficients."
  },
  {
    "objectID": "classical_linear_regression_simulation.html#r-code",
    "href": "classical_linear_regression_simulation.html#r-code",
    "title": "Classical Linear Regression Simulation in R",
    "section": "R Code",
    "text": "R Code\n\n\nCode\n# ============================================================\n# Classical Linear Regression: Simulation + OLS Estimation\n# ============================================================\n\nset.seed(123)\n\n# --- 1. Simulate data ---------------------------------------\nn &lt;- 100          # number of observations\np &lt;- 3            # number of predictors (excluding intercept)\n\nX &lt;- cbind(1, matrix(rnorm(n * p), n, p))  # include intercept\nbeta_true &lt;- c(2, 0.5, -1, 1.5)            # true coefficients\nsigma_true &lt;- 1                            # true residual SD\n\ny &lt;- X %*% beta_true + rnorm(n, 0, sigma_true)\n\n# --- 2. Estimate parameters via OLS --------------------------\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nresiduals &lt;- y - X %*% beta_hat\nsigma2_hat &lt;- as.numeric(t(residuals) %*% residuals / (n - (p + 1)))\n\n# --- 3. Inference: SEs, t-stats, p-values, CI ---------------\nvar_beta_hat &lt;- sigma2_hat * solve(t(X) %*% X)\nse_beta &lt;- sqrt(diag(var_beta_hat))\n\nt_stats &lt;- beta_hat / se_beta\np_values &lt;- 2 * (1 - pt(abs(t_stats), df = n - (p + 1)))\n\nalpha &lt;- 0.05\nt_crit &lt;- qt(1 - alpha/2, df = n - (p + 1))\nci_lower &lt;- beta_hat - t_crit * se_beta\nci_upper &lt;- beta_hat + t_crit * se_beta\n\n# Combine results into a table\nresults &lt;- data.frame(\n  Estimate = as.numeric(beta_hat),\n  SE = se_beta,\n  t_value = as.numeric(t_stats),\n  p_value = as.numeric(p_values),\n  CI_lower = as.numeric(ci_lower),\n  CI_upper = as.numeric(ci_upper)\n)\nrownames(results) &lt;-  paste0(\"beta\", 0:p)\nprint(round(results, 4))\n\n\n      Estimate     SE t_value p_value CI_lower CI_upper\nbeta0   1.9807 0.1073 18.4517   0e+00   1.7676   2.1937\nbeta1   0.4445 0.1169  3.8036   3e-04   0.2126   0.6765\nbeta2  -0.9538 0.1095 -8.7138   0e+00  -1.1711  -0.7365\nbeta3   1.4426 0.1122 12.8540   0e+00   1.2198   1.6654\n\n\nCode\n# --- 4. Prediction for a new observation ---------------------\nx_new &lt;- c(1, 0.5, -1, 1)  # include intercept term\ny_pred &lt;- as.numeric(x_new %*% beta_hat)\nvar_pred &lt;- sigma2_hat * (1 + t(x_new) %*% solve(t(X) %*% X) %*% x_new)\nse_pred &lt;- sqrt(var_pred)\n\n# 95% prediction interval\npred_interval &lt;- c(\n  lower = y_pred - t_crit * se_pred,\n  upper = y_pred + t_crit * se_pred\n)\n\ncat(\"\\nPredicted y_new =\", round(y_pred, 3), \"\\n\")\n\n\n\nPredicted y_new = 4.599 \n\n\nCode\ncat(\"95% Prediction interval: [\", round(pred_interval[1], 3), \",\",\n    round(pred_interval[2], 3), \"]\\n\")\n\n\n95% Prediction interval: [ 2.48 , 6.718 ]\n\n\nCode\n# --- 5. Visualization: True vs. Estimated Coefficients -------\npar(mar = c(5, 5, 4, 2))\nplot(beta_true, beta_hat, pch = 19, col = \"blue\", cex = 1.3,\n     xlab = \"True Coefficients\", ylab = \"Estimated Coefficients\",\n     main = \"OLS Estimates vs True Values\")\nabline(0, 1, col = \"red\", lwd = 2, lty = 2)\narrows(beta_true, ci_lower, beta_true, ci_upper, angle = 90, code = 3, length = 0.05, col = \"darkgray\")\nlegend(\"topleft\", legend = c(\"OLS Estimates\", \"y = x line\"), \n       col = c(\"blue\", \"red\"), pch = c(19, NA), lty = c(NA, 2))"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#overview",
    "href": "bayesian_linear_regression_slides.html#overview",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Overview",
    "text": "Overview\n\nClassical Linear Regression\n\nModel, inference, and limitations\n\nBayesian Linear Regression\n\nMotivation, priors, and posteriors\n\nConditional posteriors and inference\n\nComputation and Applications\n\nMCMC and Gibbs sampling\n\nDiagnostics and R implementation"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#introduction",
    "href": "bayesian_linear_regression_slides.html#introduction",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Linear Regression (BLR) extends classical regression by incorporating prior information and producing posterior distributions over model parameters.\n\nAdvantages:\n\nHandles high-dimensional and small-sample problems.\n\nProvides full uncertainty quantification.\n\nEnables regularization and integration of prior biological knowledge."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#applications-in-genomics",
    "href": "bayesian_linear_regression_slides.html#applications-in-genomics",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Applications in Genomics",
    "text": "Applications in Genomics\n\nBayesian Linear Regression (BLR) is widely applied in quantitative genetics and genomics.\n\nCommon use cases:\n\nGenome-Wide Association Studies (GWAS) and fine-mapping of causal variants.\n\nGenetic prediction and heritability estimation.\n\nPathway and gene-set enrichment analyses.\n\nIntegrative multi-omics modeling (genome, transcriptome, epigenome)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#classical-linear-regression",
    "href": "bayesian_linear_regression_slides.html#classical-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Classical Linear Regression",
    "text": "Classical Linear Regression\nModel\n\\[\ny = X\\beta + e, \\quad e \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n\\] - \\(y\\): outcomes\n- \\(X\\): design matrix\n- \\(\\beta\\): coefficients\n- \\(e\\): are the residuals\n- \\(\\sigma^2\\): residual variance"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#estimation",
    "href": "bayesian_linear_regression_slides.html#estimation",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Estimation",
    "text": "Estimation\nRegression effects: \\[\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n\\]\nResidual variance: \\[\n\\hat{\\sigma}^2 = \\frac{1}{n-p}\\sum_i (y_i - x_i^\\top \\hat{\\beta})^2\n\\]\nInference via standard errors and \\(t\\)-tests, confidence intervals, and prediction intervals."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#limitations",
    "href": "bayesian_linear_regression_slides.html#limitations",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Limitations",
    "text": "Limitations\n\nNo explicit control over effect size distribution\nSensitive when collinearity is high\nNot identifiable when \\(p&gt;n\\)\nUncertainty largely asymptotic unless normality assumptions hold"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#why-bayesian-linear-regression",
    "href": "bayesian_linear_regression_slides.html#why-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Why Bayesian Linear Regression?",
    "text": "Why Bayesian Linear Regression?\n\nCombines likelihood and prior to form the posterior.\n\nPriors express beliefs about effect sizes:\n\nNormal → many small effects\n\nSpike-and-slab → sparse effects\n\n\nActs as a regularizer:\n\nShrinks small/noisy effects toward \\(0\\)\n\nPreserves large, important effects\n\n\nStable when \\(p &gt; n\\) due to prior information.\n\nProvides full posterior distributions for \\(\\beta\\) and \\(\\sigma^2\\)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#overview-bayesian-linear-regression",
    "href": "bayesian_linear_regression_slides.html#overview-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Overview: Bayesian Linear Regression",
    "text": "Overview: Bayesian Linear Regression\n\nCombines data and prior knowledge using Bayes’ rule.\n\nUses conjugate priors to yield closed-form full conditionals.\n\nEmploys Gibbs sampling to approximate the posterior distribution.\n\nEstimates parameters, uncertainty, and predictions from posterior draws."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#bayesian-linear-regression-with-gaussian-priors",
    "href": "bayesian_linear_regression_slides.html#bayesian-linear-regression-with-gaussian-priors",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Bayesian Linear Regression with Gaussian Priors",
    "text": "Bayesian Linear Regression with Gaussian Priors\nBayesian linear regression starts with the same model structure as classical linear regression.\n\\[\ny = X\\beta + e, \\quad e \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n\\]\n\n\\(y\\): \\(n \\times 1\\) vector of observed outcomes\n\n\\(X\\): \\(n \\times p\\) design matrix of predictors\n\n\\(\\beta\\): \\(p \\times 1\\) vector of unknown coefficients\n\n\\(e\\): Gaussian noise with mean \\(0\\) and variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#likelihood-in-bayesian-linear-regression",
    "href": "bayesian_linear_regression_slides.html#likelihood-in-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Likelihood in Bayesian Linear Regression",
    "text": "Likelihood in Bayesian Linear Regression\nBecause the residuals are Gaussian, it follows that the marginal distribution of \\(y\\) is:\n\\[\ne \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n\\] The marginal distribution of \\(y\\) is:\n\\[\ny \\sim \\mathcal{N}(X\\beta, \\sigma^2 I_n)\n\\] This defines the likelihood the probability of the observed data given parameters \\(\\beta\\) and \\(\\sigma^2\\):\n\\[\np(y \\mid X, \\beta, \\sigma^2) = \\mathcal{N}(X\\beta, \\sigma^2 I_n)\n\\]"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#introducing-priors",
    "href": "bayesian_linear_regression_slides.html#introducing-priors",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Introducing Priors",
    "text": "Introducing Priors\nIn Bayesian linear regression, we specify prior distributions that express our beliefs about parameters before seeing the data.\nA common conjugate prior for the regression coefficients is:\n\\[\n\\beta \\mid \\sigma_b^2 \\sim \\mathcal{N}(0, \\sigma_b^2 I_p)\n\\]\nThis reflects the belief that most effect sizes are small and centered near zero — consistent with the polygenic assumption in genetics."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#role-of-the-prior-variance-sigma_b2",
    "href": "bayesian_linear_regression_slides.html#role-of-the-prior-variance-sigma_b2",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Role of the Prior Variance \\(\\sigma_b^2\\)",
    "text": "Role of the Prior Variance \\(\\sigma_b^2\\)\nThe parameter \\(\\sigma_b^2\\) acts as a shrinkage (regularization) parameter:\n\nSmall \\(\\sigma_b^2\\) → stronger shrinkage toward zero.\n\nLarge \\(\\sigma_b^2\\) → weaker shrinkage, allowing larger effects.\n\nIt controls the strength of regularization and is often treated as an unknown hyperparameter estimated from the data."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#priors-on-variance-components",
    "href": "bayesian_linear_regression_slides.html#priors-on-variance-components",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Priors on Variance Components",
    "text": "Priors on Variance Components\nWe also place priors on the variance components to complete the hierarchical model.\n\\[\n\\sigma_b^2 \\mid S_b, v_b \\sim S_b \\, \\chi^{-2}(v_b), \\quad\n\\sigma^2 \\mid S, v \\sim S \\, \\chi^{-2}(v)\n\\]\nHere:\n\n\\(S_b\\) and \\(v_b\\) are user-defined hyperparameters that control the prior distribution on the variance of regression coefficients.\n\n\\(S\\) and \\(v\\) are hyperparameters for the residual variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#conjugate-priors-and-regularization",
    "href": "bayesian_linear_regression_slides.html#conjugate-priors-and-regularization",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Conjugate Priors and Regularization",
    "text": "Conjugate Priors and Regularization\nConjugate priors keep posteriors in the same family\n(e.g., scaled inverse-chi-squared), allowing closed-form Gibbs updates.\nThey also serve as regularizers:\n\nThe prior on \\(\\beta\\) shrinks small or noisy effects toward zero.\n\nPriors on variance components prevent overfitting, especially when \\(p &gt; n\\).\n\nThus, conjugate priors make Bayesian linear regression efficient and stable."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#posterior-distribution",
    "href": "bayesian_linear_regression_slides.html#posterior-distribution",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\nIn Bayesian analysis, we combine the likelihood and priors using Bayes’ rule to obtain the joint posterior:\n\\[\np(\\beta, \\sigma_b^2, \\sigma^2 \\mid y) \\propto\np(y \\mid \\beta, \\sigma^2)\\;\np(\\beta \\mid \\sigma_b^2)\\;\np(\\sigma_b^2)\\;\np(\\sigma^2)\n\\]\nThis posterior captures all updated knowledge about the unknown parameters after observing the data.\nIt forms the basis for computing posterior means, credible intervals, and predictions."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#conjugacy-and-gibbs-sampling",
    "href": "bayesian_linear_regression_slides.html#conjugacy-and-gibbs-sampling",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Conjugacy and Gibbs Sampling",
    "text": "Conjugacy and Gibbs Sampling\nWith conjugate priors, each parameter’s full conditional distribution has a closed-form solution.\nThis makes Gibbs sampling a natural and efficient inference method.\n\nParameters are updated one at a time, each from its conditional posterior.\n\nThe resulting Markov chain explores the joint posterior of\n\\((\\beta, \\sigma_b^2, \\sigma^2)\\).\n\nGibbs sampling thus provides an easy way to approximate the full posterior in Bayesian linear regression."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#full-conditional-for-beta",
    "href": "bayesian_linear_regression_slides.html#full-conditional-for-beta",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Full Conditional for \\(\\beta\\)",
    "text": "Full Conditional for \\(\\beta\\)\nGiven \\(\\sigma^2\\), \\(\\sigma_b^2\\), and the data \\(y\\), the regression coefficients have a multivariate normal conditional posterior:\n\\[\n\\beta \\mid \\sigma^2, \\sigma_b^2, y \\sim\n\\mathcal{N}(\\mu_\\beta, \\Sigma_\\beta)\n\\]\nwhere\n\\[\n\\Sigma_\\beta = \\left( \\frac{X^\\top X}{\\sigma^2} + \\frac{I}{\\sigma_b^2} \\right)^{-1},\n\\quad\n\\mu_\\beta = \\Sigma_\\beta \\frac{X^\\top y}{\\sigma^2}\n\\]\nThis distribution represents our updated belief about \\(\\beta\\)\nafter observing the data, while holding \\(\\sigma_b^2\\) and \\(\\sigma^2\\) fixed."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#comparison-to-classical-ols",
    "href": "bayesian_linear_regression_slides.html#comparison-to-classical-ols",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Comparison to Classical OLS",
    "text": "Comparison to Classical OLS\nIn classical regression, the OLS estimator is\n\\[\n\\hat\\beta_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top y,\n\\quad y \\sim \\mathcal{N}(X\\beta, \\sigma^2 I)\n\\]\nThe estimate of \\(\\beta\\) is independent of \\(\\sigma^2\\),\nsince \\(\\sigma^2\\) only scales the likelihood, not its maximum.\nIn Bayesian regression, \\(\\sigma^2\\) appears explicitly in the posterior:\n\\[\n\\Sigma_\\beta = \\left( \\frac{X^\\top X}{\\sigma^2} + \\frac{I}{\\sigma_b^2} \\right)^{-1},\n\\quad\n\\mu_\\beta = \\Sigma_\\beta \\frac{X^\\top y}{\\sigma^2}\n\\]\nThe term \\(\\frac{I}{\\sigma_b^2}\\) introduces shrinkage, regularizing estimates and stabilizing inference especially when \\(p &gt; n\\) or predictors are highly correlated.\nThus, the Bayesian posterior mean is a regularized, uncertainty-aware generalization of OLS."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#full-conditional-for-beta_j",
    "href": "bayesian_linear_regression_slides.html#full-conditional-for-beta_j",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Full Conditional for \\(\\beta_j\\)",
    "text": "Full Conditional for \\(\\beta_j\\)\nInstead of sampling \\(\\beta\\) jointly, we can update each coefficient \\(\\beta_j\\) one at a time, holding all others fixed efficient for large \\(p\\) or spike-and-slab models.\nLet \\(X_j\\) be the \\(j\\)th column of \\(X\\) and define the partial residual:\n\\[\nr_j = y - X_{-j} \\beta_{-j}\n\\]\nThen the conditional posterior for \\(\\beta_j\\) is univariate normal:\n\\[\n\\beta_j \\mid D \\sim \\mathcal{N} \\!\\left(\n\\frac{X_j^\\top r_j}{X_j^\\top X_j + \\sigma^2 / \\sigma_b^2},\\;\n\\frac{\\sigma^2}{X_j^\\top X_j + \\sigma^2 / \\sigma_b^2}\n\\right)\n\\]\nThis corresponds to a regularized least-squares update. Residual updates avoid matrix inversion, scale to high dimensions, and extend naturally to sparse (spike-and-slab) models."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#full-conditional-for-sigma_b2",
    "href": "bayesian_linear_regression_slides.html#full-conditional-for-sigma_b2",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Full Conditional for \\(\\sigma_b^2\\)",
    "text": "Full Conditional for \\(\\sigma_b^2\\)\nThe conditional distribution of the prior variance \\(\\sigma_b^2\\), given \\(\\beta\\) and the hyperparameters, is a scaled inverse-chi-squared:\n\\[\n\\sigma_b^2 \\mid \\beta \\sim \\tilde{S}_b \\, \\chi^{-2}(\\tilde{v}_b)\n\\]\nwhere\n\\[\n\\tilde{v}_b = v_b + p, \\quad\n\\tilde{S}_b = \\frac{\\beta^\\top \\beta + v_b S_b}{\\tilde{v}_b}\n\\]\nAt each Gibbs iteration, \\(\\sigma_b^2\\) is sampled directly given \\(\\beta\\). This update reflects our revised belief about the variability of effect sizes after observing the current posterior draw of \\(\\beta\\)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#full-conditional-for-sigma2",
    "href": "bayesian_linear_regression_slides.html#full-conditional-for-sigma2",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Full Conditional for \\(\\sigma^2\\)",
    "text": "Full Conditional for \\(\\sigma^2\\)\nThe conditional distribution of the residual variance \\(\\sigma^2\\),\ngiven \\(\\beta\\) and the data, is also scaled inverse-chi-squared:\n\\[\n\\sigma^2 \\mid \\beta, y \\sim \\tilde{S} \\, \\chi^{-2}(\\tilde{v})\n\\]\nwhere\n\\[\n\\tilde{v} = v + n, \\quad\n\\tilde{S} = \\frac{(y - X\\beta)^\\top (y - X\\beta) + v S}{\\tilde{v}}\n\\]\nAt each Gibbs iteration, \\(\\sigma^2\\) is sampled directly given \\(\\beta\\).\nThis captures our updated belief about the residual variability\nafter accounting for the current linear predictor \\(X\\beta\\)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#gibbs-sampling-motivation",
    "href": "bayesian_linear_regression_slides.html#gibbs-sampling-motivation",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Gibbs Sampling: Motivation",
    "text": "Gibbs Sampling: Motivation\nBayesian inference often involves complex posteriors that lack closed-form solutions. To approximate these, we use Markov Chain Monte Carlo (MCMC) methods.\nMCMC builds a Markov chain whose stationary distribution is the target posterior. Once the chain has converged, its samples can be used to estimate:\n\nPosterior means, variances, and credible intervals\n\nPredictive distributions\n\nOther functions of interest\n\nAmong MCMC algorithms, the Gibbs sampler is especially useful when all full conditional distributions are available in closed form."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#gibbs-sampling-the-algorithm",
    "href": "bayesian_linear_regression_slides.html#gibbs-sampling-the-algorithm",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Gibbs Sampling: The Algorithm",
    "text": "Gibbs Sampling: The Algorithm\nFor Bayesian linear regression with conjugate priors, the joint posterior is:\n\\[\np(\\beta, \\sigma_b^2 , \\sigma^2 \\mid y) \\propto\np(y \\mid \\beta, \\sigma^2)\\; p(\\beta \\mid \\sigma_b^2)\\; p(\\sigma_b^2)\\; p(\\sigma^2)\n\\]\nWe iteratively draw from the following full conditionals:\n\nSample \\(\\beta \\mid \\sigma_b^2, \\sigma^2, y\\)\n\nSample \\(\\sigma_b^2 \\mid \\beta\\)\n\nSample \\(\\sigma^2 \\mid \\beta, y\\)\n\nEach step updates one parameter given the latest values of the others. Repeating this sequence yields samples from the joint posterior \\(p(\\beta, \\sigma_b^2, \\sigma^2 \\mid y)\\).\nBecause each conditional is standard (Normal or scaled inverse-\\(\\chi^2\\)), Gibbs sampling is both efficient and easy to implement."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#posterior-summaries",
    "href": "bayesian_linear_regression_slides.html#posterior-summaries",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\nAfter running the Gibbs sampler, we obtain posterior draws \\(\\{\\theta^{(t)}\\}_{t=1}^T\\) for parameters such as \\(\\beta_j\\), \\(\\sigma^2\\), or \\(\\sigma_b^2\\).\nWe summarize the posterior distribution via:\n\nPosterior mean\n\\[\n\\mathbb{E}[\\theta \\mid y] \\approx \\frac{1}{T} \\sum_{t=1}^{T} \\theta^{(t)}\n\\]\nPosterior median: the median of \\(\\theta^{(t)}\\)\nCredible interval (95%)\n\\[\n[\\theta]_{0.025}, [\\theta]_{0.975}\n\\]\n\nThese summaries describe the most probable values of \\(\\theta\\)\nand their uncertainty after combining data and prior beliefs."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#estimating-uncertainty",
    "href": "bayesian_linear_regression_slides.html#estimating-uncertainty",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Estimating Uncertainty",
    "text": "Estimating Uncertainty\nBayesian inference provides full posterior distributions, not just point estimates. Uncertainty is quantified directly from the posterior samples:\n\nPosterior standard deviation\n\\[\n\\mathrm{SD}(\\theta \\mid y) \\approx\n\\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T} (\\theta^{(t)} - \\bar{\\theta})^2}\n\\]\n\nThe width of the credible interval reflects this uncertainty. Parameters with broader posteriors are estimated with less precision, and the degree of uncertainty depends on both the data and the prior."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#posterior-prediction",
    "href": "bayesian_linear_regression_slides.html#posterior-prediction",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\nGiven a new observation \\(x_{\\text{new}}\\), we can predict using posterior draws:\n\nCompute predicted means for each sample: \\[\n\\hat{y}_{\\text{new}}^{(t)} = x_{\\text{new}}^\\top \\beta^{(t)}\n\\]\nAdd residual uncertainty: \\[\ny_{\\text{new}}^{(t)} \\sim\n\\mathcal{N}\\!\\left(x_{\\text{new}}^\\top \\beta^{(t)},\\; \\sigma^{2(t)}\\right)\n\\]\n\nThe resulting samples \\(\\{y_{\\text{new}}^{(t)}\\}\\) form a posterior predictive distribution, from which we can derive predictive intervals and evaluate predictive accuracy."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#model-checking-and-hypothesis-testing",
    "href": "bayesian_linear_regression_slides.html#model-checking-and-hypothesis-testing",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Model Checking and Hypothesis Testing",
    "text": "Model Checking and Hypothesis Testing\nPosterior samples enable rich model diagnostics and hypothesis testing:\n\nPosterior probability of an event\n\\[\n\\Pr(\\beta_j \\ne 0 \\mid y)\n\\approx \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{1}\\!\\left(\\beta_j^{(t)} \\ne 0\\right)\n\\]\nPosterior predictive checks\nSimulate new datasets using posterior draws and compare them to the observed data to assess model fit.\nModel comparison\nBayes factors and marginal likelihoods can be approximated to formally test or compare competing models.\n\nThese tools extend Bayesian inference beyond estimation to model validation, uncertainty quantification, and decision-making."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#convergence-diagnostics",
    "href": "bayesian_linear_regression_slides.html#convergence-diagnostics",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\nBefore interpreting MCMC results, we must check that the Gibbs sampler has converged to the target posterior distribution.\nConvergence diagnostics assess whether the Markov chain has reached its stationary distribution and is producing valid samples.\nTwo basic strategies are:\n\nBurn-in – Discard early iterations (e.g., first 1000) to remove dependence on starting values.\n\nThinning – Keep every \\(k\\)-th sample to reduce autocorrelation.\n\nThese steps improve sample quality and ensure reliable posterior summaries."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#trace-plots",
    "href": "bayesian_linear_regression_slides.html#trace-plots",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Trace Plots",
    "text": "Trace Plots\nA simple yet powerful diagnostic is the trace plot,\nshowing sampled parameter values \\(\\theta^{(t)}\\) over iterations \\(t\\).\n\nA converged chain fluctuates around a stable mean — no trend or drift.\n\nMultiple chains from different starting points should overlap and mix well.\n\nTrace plots help detect: - Lack of stationarity (upward/downward trends) - Poor mixing or multimodality - Burn-in issues\nVisual inspection is often the first step in assessing convergence."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#autocorrelation",
    "href": "bayesian_linear_regression_slides.html#autocorrelation",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nSamples from a Gibbs sampler are correlated, especially for tightly coupled parameters.\nThe autocorrelation function (ACF) quantifies dependence across lags \\(k\\):\n\\[\n\\hat{\\rho}_k =\n\\frac{\\sum_{t=1}^{T-k} (\\theta^{(t)} - \\bar{\\theta})(\\theta^{(t+k)} - \\bar{\\theta})}\n     {\\sum_{t=1}^{T} (\\theta^{(t)} - \\bar{\\theta})^2}\n\\]\n\nHigh \\(\\hat{\\rho}_k\\) → slow mixing and fewer effective samples\n\nLow \\(\\hat{\\rho}_k\\) → better mixing and faster convergence\n\nReducing autocorrelation may require more iterations,\nreparameterization, or thinning the chain."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#effective-sample-size-ess",
    "href": "bayesian_linear_regression_slides.html#effective-sample-size-ess",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Effective Sample Size (ESS)",
    "text": "Effective Sample Size (ESS)\nAutocorrelation reduces the number of independent samples obtained.\nThe effective sample size (ESS) adjusts for this:\n\\[\n\\text{ESS}(\\theta) =\n\\frac{T}{1 + 2 \\sum_{k=1}^{K} \\hat{\\rho}_k}\n\\]\n\nSmall ESS → chain is highly correlated, less informative\n\nRule of thumb: \\(\\text{ESS} &gt; 100\\) per parameter for stable inference\n\nESS provides a quantitative measure of sampling efficiency\nand helps determine whether more iterations are needed."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#gelmanrubin-diagnostic-hatr",
    "href": "bayesian_linear_regression_slides.html#gelmanrubin-diagnostic-hatr",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Gelman–Rubin Diagnostic (\\(\\hat{R}\\))",
    "text": "Gelman–Rubin Diagnostic (\\(\\hat{R}\\))\nWhen running multiple chains, the Gelman–Rubin statistic compares between-chain and within-chain variability.\nFor \\(m\\) chains with \\(T\\) iterations each:\n\\[\nW = \\frac{1}{m} \\sum_{i=1}^{m} s_i^2, \\quad\nB = \\frac{T}{m-1} \\sum_{i=1}^{m} (\\bar{\\theta}_i - \\bar{\\theta})^2\n\\]\nThe potential scale reduction factor:\n\\[\n\\hat{R} = \\sqrt{ \\frac{\\hat{V}}{W} }, \\quad\n\\hat{V} = \\frac{T-1}{T} W + \\frac{1}{T} B\n\\]\n\n\\(\\hat{R} \\approx 1\\) → convergence achieved\n\n\\(\\hat{R} &gt; 1.1\\) → chains have not converged"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#geweke-diagnostic",
    "href": "bayesian_linear_regression_slides.html#geweke-diagnostic",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Geweke Diagnostic",
    "text": "Geweke Diagnostic\nThe Geweke test checks whether early and late portions of a single chain have the same mean, indicating stationarity.\n\\[\nZ =\n\\frac{\\bar{\\theta}_A - \\bar{\\theta}_B}\n     {\\sqrt{\\text{Var}(\\bar{\\theta}_A) + \\text{Var}(\\bar{\\theta}_B)}}\n\\]\nTypically:\n\nSegment A = first 10% of the chain\n\nSegment B = last 50% of the chain\n\nUnder convergence, \\(Z \\sim \\mathcal{N}(0,1)\\).\n\n\\(|Z| \\le 2\\) → chain likely stationary\n\n\\(|Z| &gt; 2\\) → potential non-convergence\n\nThese diagnostics ensure that posterior summaries reflect the true target distribution."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#spike-and-slab-bayesian-linear-regression",
    "href": "bayesian_linear_regression_slides.html#spike-and-slab-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Spike-and-Slab Bayesian Linear Regression",
    "text": "Spike-and-Slab Bayesian Linear Regression\nAs in classical BLR, the outcome is modeled as:\n\\[\ny = Xb + e, \\quad e \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n\\]\nwhere \\(y\\) is the \\(n \\times 1\\) response, \\(X\\) the design matrix, \\(b\\) the regression coefficients, and \\(\\sigma^2\\) the residual variance.\nThis defines the likelihood:\n\\[\ny \\mid b, \\sigma^2 \\sim \\mathcal{N}(Xb, \\sigma^2 I_n)\n\\]\nThe goal is to estimate \\(b\\) and identify which predictors truly contribute to \\(y\\)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#motivation-for-the-spike-and-slab-prior",
    "href": "bayesian_linear_regression_slides.html#motivation-for-the-spike-and-slab-prior",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Motivation for the Spike-and-Slab Prior",
    "text": "Motivation for the Spike-and-Slab Prior\nIn standard Bayesian linear regression:\n\\[\n\\beta_j \\sim \\mathcal{N}(0, \\sigma_b^2)\n\\]\nThis Gaussian (shrinkage) prior assumes all predictors have small effects, but it does not allow exact zeros — limiting variable selection.\nThe spike-and-slab prior addresses this by mixing two components:\n\nA spike at zero → excluded predictors\n\nA slab (wide normal) → active predictors\n\nThis yields sparse, interpretable models that select relevant variables."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#the-spike-and-slab-mixture-prior",
    "href": "bayesian_linear_regression_slides.html#the-spike-and-slab-mixture-prior",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "The Spike-and-Slab Mixture Prior",
    "text": "The Spike-and-Slab Mixture Prior\nEach regression effect is drawn from a two-component mixture:\n\\[\np(b_i \\mid \\sigma_b^2, \\pi)\n= \\pi\\, \\mathcal{N}(0, \\sigma_b^2) + (1-\\pi)\\, \\delta_0\n\\]\nwhere:\n\n\\(\\pi\\) = prior probability that \\(b_i\\) is non-zero\n\n\\(\\delta_0\\) = point mass at zero\n\nThus, with probability \\(\\pi\\) a predictor is active (slab), and with probability \\(1-\\pi\\) it is excluded (spike)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#advantages-of-spike-and-slab-priors",
    "href": "bayesian_linear_regression_slides.html#advantages-of-spike-and-slab-priors",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Advantages of Spike-and-Slab Priors",
    "text": "Advantages of Spike-and-Slab Priors\nThis hierarchical mixture prior provides several benefits:\n\nSparsity — allows exact zeros for irrelevant predictors\n\nInterpretability — binary indicators give posterior inclusion probabilities (PIPs)\n\nAdaptivity — the inclusion probability \\(\\pi\\) is learned from the data\n\nBalance — captures both strong signals (detection) and small effects (prediction)\n\nHence, spike-and-slab models combine variable selection with Bayesian uncertainty quantification."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#hierarchical-representation",
    "href": "bayesian_linear_regression_slides.html#hierarchical-representation",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Hierarchical Representation",
    "text": "Hierarchical Representation\nWe express each effect as:\n\\[\nb_i = \\alpha_i \\, \\delta_i\n\\]\nwhere:\n\\[\n\\alpha_i \\mid \\sigma_b^2 \\sim \\mathcal{N}(0, \\sigma_b^2), \\quad\n\\delta_i \\mid \\pi \\sim \\text{Bernoulli}(\\pi)\n\\]\n\n\\(\\alpha_i\\): effect size when predictor is included\n\n\\(\\delta_i\\): binary inclusion indicator (0 or 1)\n\nMarginalizing over \\(\\delta_i\\) yields the spike-and-slab mixture prior above."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#prior-for-the-inclusion-probability-pi",
    "href": "bayesian_linear_regression_slides.html#prior-for-the-inclusion-probability-pi",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Prior for the Inclusion Probability \\(\\pi\\)",
    "text": "Prior for the Inclusion Probability \\(\\pi\\)\nThe overall sparsity level is controlled by \\(\\pi\\), assigned a Beta prior:\n\\[\n\\pi \\sim \\text{Beta}(\\alpha, \\beta)\n\\]\n\nSmall \\(\\alpha\\), large \\(\\beta\\) → favor sparser models\n\n\\(\\alpha = \\beta = 1\\) → uniform prior\n\nLarger \\(\\alpha\\) → denser models\n\nThis prior lets the data determine the degree of sparsity."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#priors-for-variance-components",
    "href": "bayesian_linear_regression_slides.html#priors-for-variance-components",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Priors for Variance Components",
    "text": "Priors for Variance Components\nVariance parameters use scaled inverse-chi-squared priors:\n\\[\n\\sigma_b^2 \\sim S_b \\chi^{-2}(v_b), \\quad\n\\sigma^2 \\sim S \\chi^{-2}(v)\n\\]\nThese are conjugate, providing closed-form conditional updates. Hyperparameters \\((S_b, v_b)\\) and \\((S, v)\\) encode prior beliefs about effect size variability and residual noise."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#joint-posterior-structure",
    "href": "bayesian_linear_regression_slides.html#joint-posterior-structure",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Joint Posterior Structure",
    "text": "Joint Posterior Structure\nCombining the likelihood and priors, the joint posterior is:\n\\[\np(\\mu, \\alpha, \\delta, \\pi, \\sigma_b^2, \\sigma^2 \\mid y)\n\\propto\np(y \\mid \\mu, \\alpha, \\delta, \\sigma^2)\\,\np(\\alpha \\mid \\sigma_b^2)\\,\np(\\delta \\mid \\pi)\\,\np(\\pi)\\,\np(\\sigma_b^2)\\,\np(\\sigma^2)\n\\]\nThis captures our updated beliefs about effects, inclusion indicators, and variance components."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#gibbs-sampling-for-spike-and-slab-blr",
    "href": "bayesian_linear_regression_slides.html#gibbs-sampling-for-spike-and-slab-blr",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Gibbs Sampling for Spike-and-Slab BLR",
    "text": "Gibbs Sampling for Spike-and-Slab BLR\nInference proceeds via Gibbs sampling, cycling through these conditional updates:\n\n\\(\\alpha \\mid D\\)\n\n\\(\\delta \\mid D\\)\n\n\\(\\pi \\mid D\\)\n\n\\(\\sigma_b^2 \\mid D\\)\n\n\\(\\sigma^2 \\mid D\\)\n\nEach has a standard distribution (Normal, Bernoulli, Beta, scaled-\\(\\chi^{-2}\\)). Iterating these updates generates samples from the joint posterior."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#posterior-inclusion-probabilities",
    "href": "bayesian_linear_regression_slides.html#posterior-inclusion-probabilities",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Inclusion Probabilities",
    "text": "Posterior Inclusion Probabilities\nThe posterior inclusion probability (PIP) measures how likely each predictor is truly associated with \\(y\\):\n\\[\n\\widehat{\\Pr}(\\delta_i = 1 \\mid y)\n= \\frac{1}{T} \\sum_{t=1}^{T} \\delta_i^{(t)}\n\\]\n\nHigh PIP → predictor is likely important\n\nLow PIP → predictor likely irrelevant\n\nPIPs summarize variable relevance and drive Bayesian feature selection."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#classical-vs-bayesian-linear-regression",
    "href": "bayesian_linear_regression_slides.html#classical-vs-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "🧠 Classical vs Bayesian Linear Regression",
    "text": "🧠 Classical vs Bayesian Linear Regression\n\nClassical (OLS)\n\nEstimates parameters by minimizing residual sum of squares.\n\nProvides point estimates and asymptotic uncertainty (SEs, \\(t\\)-tests).\n\nStruggles with collinearity or when \\(p &gt; n\\).\n\nBayesian (BLR)\n\nCombines likelihood and prior to form the posterior.\n\nIncorporates regularization through priors.\n\nProvides full uncertainty quantification via posterior samples."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#computation-inference",
    "href": "bayesian_linear_regression_slides.html#computation-inference",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "⚙️ Computation & Inference",
    "text": "⚙️ Computation & Inference\n\nBayesian inference often relies on MCMC to approximate the posterior.\n\nGibbs sampling is efficient when full conditionals are available in closed form.\n\nAfter convergence, posterior draws are used to:\n\nEstimate posterior means, medians, and credible intervals.\n\nCompute posterior inclusion probabilities (PIPs).\n\nGenerate posterior predictions with uncertainty intervals."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#spike-and-slab-priors",
    "href": "bayesian_linear_regression_slides.html#spike-and-slab-priors",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "🧩 Spike-and-Slab Priors",
    "text": "🧩 Spike-and-Slab Priors\n\nExtends BLR to enable variable selection and sparsity.\n\nEach coefficient:\n\\[\nb_i \\sim \\pi \\, \\mathcal{N}(0, \\sigma_b^2) + (1 - \\pi)\\, \\delta_0\n\\]\nAllows exact zeros for irrelevant predictors.\n\nPosterior inclusion probabilities identify important variables.\n\nBalances prediction (retain weak signals) and detection (highlight strong effects)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#model-diagnostics",
    "href": "bayesian_linear_regression_slides.html#model-diagnostics",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "🔍 Model Diagnostics",
    "text": "🔍 Model Diagnostics\n\nConvergence diagnostics ensure valid posterior inference:\n\nTrace plots → mixing and stationarity.\n\nAutocorrelation / ESS → assess independence of samples.\n\nGelman–Rubin (\\(\\hat{R}\\)) → compare across chains.\n\nGeweke \\(Z\\)-test → early vs late chain segments.\n\n\nReliable inference requires well-mixed, stationary chains."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#key-takeaways",
    "href": "bayesian_linear_regression_slides.html#key-takeaways",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "📊 Key Takeaways",
    "text": "📊 Key Takeaways\n\nBayesian regression integrates prior knowledge and uncertainty.\n\nConjugate priors → closed-form Gibbs updates and efficient inference.\n\nSpike-and-slab priors → sparse, interpretable, high-dimensional models.\n\nPosterior sampling enables comprehensive uncertainty quantification.\n\nWith proper convergence checks, Bayesian models provide robust inference even when \\(p &gt; n\\)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#practical-summary",
    "href": "bayesian_linear_regression_slides.html#practical-summary",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "🧰 Practical Summary",
    "text": "🧰 Practical Summary\n\nModel definition: specify likelihood + priors.\n\nComputation: run Gibbs sampler (3–6 conditional updates).\n\nDiagnostics: check trace plots, ESS, and \\(\\hat{R}\\).\n\nPosterior summaries:\n\n\\(\\mathbb{E}[\\beta \\mid y]\\), \\(\\text{SD}(\\beta \\mid y)\\), credible intervals.\n\nPosterior inclusion probabilities (PIPs) for feature importance.\n\n\nPrediction: simulate \\(y_{\\text{new}}\\) from posterior draws.\n\nBayesian linear regression provides a principled and flexible framework for modeling,\nregularization, and interpretation in modern data analysis."
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html",
    "href": "bayesian_linear_regression_conjugate.html",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "",
    "text": "This document demonstrates Bayesian Linear Regression using conjugate Gaussian and scaled inverse-chi-squared priors. The model and priors follow the theory outlined in the notes above. We implement a Gibbs sampler for inference, summarize posterior distributions, and assess convergence diagnostics."
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#introduction",
    "href": "bayesian_linear_regression_conjugate.html#introduction",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "",
    "text": "This document demonstrates Bayesian Linear Regression using conjugate Gaussian and scaled inverse-chi-squared priors. The model and priors follow the theory outlined in the notes above. We implement a Gibbs sampler for inference, summarize posterior distributions, and assess convergence diagnostics."
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#model-setup-and-data-simulation",
    "href": "bayesian_linear_regression_conjugate.html#model-setup-and-data-simulation",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Model Setup and Data Simulation",
    "text": "Model Setup and Data Simulation\n\n\nCode\nset.seed(123)\n\n# Simulate data (same as in the classical regression example)\nn &lt;- 100\np &lt;- 3\nX &lt;- cbind(1, matrix(rnorm(n * p), n, p))\nbeta_true &lt;- c(2, 0.5, -1, 1.5)\nsigma_true &lt;- 1\n\ny &lt;- X %*% beta_true + rnorm(n, 0, sigma_true)"
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#gibbs-sampler-implementation",
    "href": "bayesian_linear_regression_conjugate.html#gibbs-sampler-implementation",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Gibbs Sampler Implementation",
    "text": "Gibbs Sampler Implementation\nWe use conjugate priors:\n\n\\(\\beta \\mid \\sigma^2, \\sigma_b^2 \\sim \\mathcal{N}(0, \\sigma_b^2 I_p)\\)\n\\(\\sigma_b^2 \\sim S_b \\chi^{-2}(v_b)\\)\n\\(\\sigma^2 \\sim S \\chi^{-2}(v)\\)\n\n\n\nCode\n# Hyperparameters\nv_b &lt;- 4\nS_b &lt;- 1\nv &lt;- 4\nS &lt;- 1\n\n# Initialization\nbeta &lt;- rep(0, ncol(X))\nsigma2 &lt;- 1\nsigma2_b &lt;- 1\n\nn_iter &lt;- 5000\nburn_in &lt;- 1000\n\n# Store samples\nbeta_samples &lt;- matrix(NA, n_iter, ncol(X))\nsigma2_samples &lt;- numeric(n_iter)\nsigma2_b_samples &lt;- numeric(n_iter)\n\nfor (t in 1:n_iter) {\n  # Sample beta | sigma2, sigma2_b, y\n  Sigma_beta &lt;- solve(t(X) %*% X / sigma2 + diag(1 / sigma2_b, ncol(X)))\n  mu_beta &lt;- Sigma_beta %*% t(X) %*% y / sigma2\n  beta &lt;- as.numeric(mu_beta + t(chol(Sigma_beta)) %*% rnorm(ncol(X)))\n  \n  # Sample sigma_b^2 | beta\n  v_b_tilde &lt;- v_b + ncol(X)\n  S_b_tilde &lt;- (sum(beta^2) + v_b * S_b) / v_b_tilde\n  sigma2_b &lt;- v_b_tilde * S_b_tilde / rchisq(1, df = v_b_tilde)\n  \n  # Sample sigma^2 | beta, y\n  v_tilde &lt;- v + n\n  resid &lt;- y - X %*% beta\n  S_tilde &lt;- (sum(resid^2) + v * S) / v_tilde\n  sigma2 &lt;- v_tilde * S_tilde / rchisq(1, df = v_tilde)\n  \n  # Store samples\n  beta_samples[t, ] &lt;- beta\n  sigma2_samples[t] &lt;- sigma2\n  sigma2_b_samples[t] &lt;- sigma2_b\n}"
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#posterior-summaries",
    "href": "bayesian_linear_regression_conjugate.html#posterior-summaries",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\n\nCode\nposterior_summary &lt;- function(samples, probs = c(0.025, 0.5, 0.975)) {\n  c(\n    mean = mean(samples),\n    sd = sd(samples),\n    quantile(samples, probs = probs)\n  )\n}\n\n# Summaries for betas\nbeta_summary &lt;- t(apply(beta_samples[burn_in:n_iter, ], 2, posterior_summary))\nrownames(beta_summary) &lt;- paste0(\"beta\", 0:p)\n\n# Summaries for variance parameters\nsigma2_summary &lt;- posterior_summary(sigma2_samples[burn_in:n_iter])\nsigma2b_summary &lt;- posterior_summary(sigma2_b_samples[burn_in:n_iter])\n\nround(beta_summary, 4)\n\n\n         mean     sd    2.5%     50%   97.5%\nbeta0  1.9673 0.1077  1.7537  1.9669  2.1769\nbeta1  0.4405 0.1186  0.2063  0.4425  0.6727\nbeta2 -0.9460 0.1105 -1.1559 -0.9470 -0.7262\nbeta3  1.4319 0.1124  1.2171  1.4315  1.6524\n\n\nCode\nround(rbind(sigma2 = sigma2_summary, sigma2_b = sigma2b_summary), 4)\n\n\n           mean     sd  2.5%    50%  97.5%\nsigma2   1.1276 0.1638 0.852 1.1154 1.5078\nsigma2_b 1.8685 1.2652 0.625 1.5398 4.9971"
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#trace-plots",
    "href": "bayesian_linear_regression_conjugate.html#trace-plots",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\nCode\npar(mfrow = c(2, 2))\nfor (j in 1:ncol(X)) {\n  plot(beta_samples[, j], type = \"l\", main = paste(\"Trace: beta\", j - 1),\n       xlab = \"Iteration\", ylab = expression(beta))\n  abline(h = beta_true[j], col = \"red\", lwd = 2, lty = 2)\n}"
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#autocorrelation-plots",
    "href": "bayesian_linear_regression_conjugate.html#autocorrelation-plots",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Autocorrelation Plots",
    "text": "Autocorrelation Plots\n\n\nCode\npar(mfrow = c(2, 2))\nfor (j in 1:ncol(X)) {\n  acf(beta_samples[burn_in:n_iter, j], main = paste(\"ACF: beta\", j - 1))\n}"
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#convergence-diagnostics",
    "href": "bayesian_linear_regression_conjugate.html#convergence-diagnostics",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\nWe compute autocorrelation, Monte Carlo standard error, Geweke Z-score, Gelman-Rubin (), and effective sample size.\n\n\nCode\n# Simple diagnostics for one chain\nconvergence_stats &lt;- function(samples) {\n  n &lt;- length(samples)\n  ac1 &lt;- cor(samples[-1], samples[-n])\n  mcse &lt;- sd(samples) * sqrt((1 + ac1) / n)\n  a &lt;- floor(0.1 * n); b &lt;- floor(0.5 * n)\n  z &lt;- (mean(samples[1:a]) - mean(samples[(n - b + 1):n])) /\n       sqrt(var(samples[1:a]) / a + var(samples[(n - b + 1):n]) / b)\n  ess &lt;- n / (1 + 2 * sum(acf(samples, plot = FALSE)$acf[-1]))\n  c(autocorr1 = ac1, mcse = mcse, geweke_z = z, ess = ess)\n}\n\nconv_results &lt;- t(apply(beta_samples[burn_in:n_iter, ], 2, convergence_stats))\nrownames(conv_results) &lt;- paste0(\"beta\", 0:p)\nround(conv_results, 4)\n\n\n      autocorr1   mcse geweke_z      ess\nbeta0   -0.0055 0.0017   1.0590 4562.500\nbeta1    0.0308 0.0019   0.8929 3641.556\nbeta2   -0.0256 0.0017  -2.4631 4014.290\nbeta3    0.0189 0.0018   0.1831 3634.925"
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#posterior-mean-vs-true-values",
    "href": "bayesian_linear_regression_conjugate.html#posterior-mean-vs-true-values",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Posterior Mean vs True Values",
    "text": "Posterior Mean vs True Values\n\n\nCode\npar(mar = c(5, 5, 4, 2))\nplot(beta_true, beta_summary[, \"mean\"], pch = 19, col = \"blue\",\n     xlab = \"True Coefficients\", ylab = \"Posterior Mean Estimates\",\n     main = \"Posterior Mean vs True Coefficients\")\nabline(0, 1, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topleft\", legend = c(\"Posterior Means\", \"y = x line\"), \n       col = c(\"blue\", \"red\"), pch = c(19, NA), lty = c(NA, 2))"
  },
  {
    "objectID": "bayesian_spike_and_slab.html",
    "href": "bayesian_spike_and_slab.html",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "",
    "text": "This document demonstrates Bayesian Linear Regression using spike-and-slab priors, as described in the notes. We implement a Gibbs sampler that updates parameters from their full conditionals, computes posterior summaries (including posterior inclusion probabilities), and evaluates convergence using diagnostic statistics."
  },
  {
    "objectID": "bayesian_spike_and_slab.html#introduction",
    "href": "bayesian_spike_and_slab.html#introduction",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "",
    "text": "This document demonstrates Bayesian Linear Regression using spike-and-slab priors, as described in the notes. We implement a Gibbs sampler that updates parameters from their full conditionals, computes posterior summaries (including posterior inclusion probabilities), and evaluates convergence using diagnostic statistics."
  },
  {
    "objectID": "bayesian_spike_and_slab.html#model-setup-and-data-simulation",
    "href": "bayesian_spike_and_slab.html#model-setup-and-data-simulation",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Model Setup and Data Simulation",
    "text": "Model Setup and Data Simulation\n\n\nCode\nset.seed(123)\n\n# Simulate data\nn &lt;- 100\np &lt;- 5\nX &lt;- cbind(1, matrix(rnorm(n * p), n, p))\nbeta_true &lt;- c(2, 1.2, 0, 0, 0, 1.5)  # some zeros (spike)\nsigma_true &lt;- 1\n\ny &lt;- X %*% beta_true + rnorm(n, 0, sigma_true)"
  },
  {
    "objectID": "bayesian_spike_and_slab.html#gibbs-sampler-implementation",
    "href": "bayesian_spike_and_slab.html#gibbs-sampler-implementation",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Gibbs Sampler Implementation",
    "text": "Gibbs Sampler Implementation\nWe use the following priors:\n\n\\(\\alpha_i \\mid \\sigma_b^2 \\sim \\mathcal{N}(0, \\sigma_b^2)\\)\n\\(\\delta_i \\mid \\pi \\sim \\text{Bernoulli}(\\pi)\\)\n\\(\\pi \\sim \\text{Beta}(a, b)\\)\n\\(\\sigma_b^2 \\sim S_b \\chi^{-2}(v_b)\\)\n\\(\\sigma^2 \\sim S \\chi^{-2}(v)\\)\n\n\n\nCode\n# Hyperparameters\na_pi &lt;- 1\nb_pi &lt;- 1\nv_b &lt;- 4\nS_b &lt;- 1\nv &lt;- 4\nS &lt;- 1\n\n# Gibbs sampler setup\nn_iter &lt;- 4000\nburn_in &lt;- 1000\nchains &lt;- 2\n\n# Storage\nbeta_samples &lt;- vector(\"list\", chains)\ndelta_samples &lt;- vector(\"list\", chains)\npi_samples &lt;- vector(\"list\", chains)\nsigma2_samples &lt;- vector(\"list\", chains)\nsigma2b_samples &lt;- vector(\"list\", chains)\n\nfor (c in 1:chains) {\n  # Initial values\n  alpha &lt;- rnorm(ncol(X), 0, 1)\n  delta &lt;- rbinom(ncol(X), 1, 0.5)\n  sigma2 &lt;- 1\n  sigma2_b &lt;- 1\n  pi &lt;- 0.5\n  \n  # Containers\n  alpha_chain &lt;- matrix(NA, n_iter, ncol(X))\n  delta_chain &lt;- matrix(NA, n_iter, ncol(X))\n  sigma2_chain &lt;- numeric(n_iter)\n  sigma2b_chain &lt;- numeric(n_iter)\n  pi_chain &lt;- numeric(n_iter)\n  \n  for (t in 1:n_iter) {\n    # Sample each alpha_i given delta_i\n    for (j in 1:ncol(X)) {\n      X_j &lt;- X[, j]\n      r_j &lt;- y - X %*% (alpha * delta) + X_j * alpha[j] * delta[j]\n      \n      if (delta[j] == 1) {\n        var_j &lt;- sigma2 / (t(X_j) %*% X_j + sigma2 / sigma2_b)\n        mean_j &lt;- as.numeric(var_j * t(X_j) %*% r_j / sigma2)\n        alpha[j] &lt;- rnorm(1, mean_j, sqrt(var_j))\n      } else {\n        alpha[j] &lt;- rnorm(1, 0, sqrt(sigma2_b))  # prior draw\n      }\n    }\n    \n    # Sample delta_i given alpha_i\n    for (j in 1:ncol(X)) {\n      X_j &lt;- X[, j]\n      r_j &lt;- y - X %*% (alpha * delta) + X_j * alpha[j] * delta[j]\n      rss0 &lt;- sum((r_j)^2)\n      rss1 &lt;- sum((r_j - X_j * alpha[j])^2)\n      \n      logodds &lt;- 0.5 / sigma2 * (rss0 - rss1) + log(pi) - log(1 - pi)\n      p1 &lt;- 1 / (1 + exp(-logodds))\n      delta[j] &lt;- rbinom(1, 1, p1)\n    }\n    \n    # Update pi\n    pi &lt;- rbeta(1, a_pi + sum(delta), b_pi + ncol(X) - sum(delta))\n    \n    # Update sigma_b^2\n    p_incl &lt;- sum(delta)\n    v_b_tilde &lt;- v_b + p_incl\n    S_b_tilde &lt;- (sum((alpha * delta)^2) + v_b * S_b) / v_b_tilde\n    sigma2_b &lt;- v_b_tilde * S_b_tilde / rchisq(1, df = v_b_tilde)\n    \n    # Update sigma^2\n    resid &lt;- y - X %*% (alpha * delta)\n    v_tilde &lt;- v + n\n    S_tilde &lt;- (sum(resid^2) + v * S) / v_tilde\n    sigma2 &lt;- v_tilde * S_tilde / rchisq(1, df = v_tilde)\n    \n    # Store\n    alpha_chain[t, ] &lt;- alpha * delta\n    delta_chain[t, ] &lt;- delta\n    pi_chain[t] &lt;- pi\n    sigma2_chain[t] &lt;- sigma2\n    sigma2b_chain[t] &lt;- sigma2_b\n  }\n  \n  beta_samples[[c]] &lt;- alpha_chain\n  delta_samples[[c]] &lt;- delta_chain\n  pi_samples[[c]] &lt;- pi_chain\n  sigma2_samples[[c]] &lt;- sigma2_chain\n  sigma2b_samples[[c]] &lt;- sigma2b_chain\n}"
  },
  {
    "objectID": "bayesian_spike_and_slab.html#posterior-summaries",
    "href": "bayesian_spike_and_slab.html#posterior-summaries",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\n\nCode\nposterior_summary &lt;- function(samples, probs = c(0.025, 0.5, 0.975)) {\n  c(mean = mean(samples), sd = sd(samples), quantile(samples, probs = probs))\n}\n\n# Combine chains\nbeta_all &lt;- do.call(rbind, beta_samples)\npi_all &lt;- unlist(pi_samples)\ndelta_all &lt;- do.call(rbind, delta_samples)\n\nbeta_summary &lt;- t(apply(beta_all[burn_in:nrow(beta_all), ], 2, posterior_summary))\nrownames(beta_summary) &lt;- paste0(\"beta\", 0:p)\n\nPIP &lt;- colMeans(delta_all[burn_in:nrow(delta_all), ])\n\nround(beta_summary, 4)\n\n\n         mean     sd    2.5%    50%  97.5%\nbeta0  1.9340 0.0975  1.7469 1.9348 2.1217\nbeta1  1.1750 0.1057  0.9651 1.1759 1.3812\nbeta2  0.0330 0.0750  0.0000 0.0000 0.2550\nbeta3  0.0027 0.0352 -0.0568 0.0000 0.1062\nbeta4 -0.0139 0.0487 -0.1804 0.0000 0.0082\nbeta5  1.6871 0.0974  1.4967 1.6865 1.8798\n\n\nCode\ncat(\"\\nPosterior Inclusion Probabilities (PIP):\\n\")\n\n\n\nPosterior Inclusion Probabilities (PIP):\n\n\nCode\nround(PIP, 3)\n\n\n[1] 1.000 1.000 0.250 0.124 0.159 1.000"
  },
  {
    "objectID": "bayesian_spike_and_slab.html#trace-plots",
    "href": "bayesian_spike_and_slab.html#trace-plots",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\nCode\npar(mfrow = c(3, 2))\nfor (j in 1:ncol(X)) {\n  plot(beta_samples[[1]][, j], type = \"l\", main = paste(\"Trace: beta\", j - 1, \"(chain 1)\"),\n       xlab = \"Iteration\", ylab = expression(beta))\n  abline(h = beta_true[j], col = \"red\", lwd = 2, lty = 2)\n}"
  },
  {
    "objectID": "bayesian_spike_and_slab.html#autocorrelation-plots",
    "href": "bayesian_spike_and_slab.html#autocorrelation-plots",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Autocorrelation Plots",
    "text": "Autocorrelation Plots\n\n\nCode\npar(mfrow = c(3, 2))\nfor (j in 1:ncol(X)) {\n  acf(beta_samples[[1]][burn_in:n_iter, j], main = paste(\"ACF: beta\", j - 1))\n}"
  },
  {
    "objectID": "bayesian_spike_and_slab.html#convergence-diagnostics",
    "href": "bayesian_spike_and_slab.html#convergence-diagnostics",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\n\n\nCode\nconvergence_stats &lt;- function(samples) {\n  n &lt;- length(samples)\n  ac1 &lt;- cor(samples[-1], samples[-n])\n  mcse &lt;- sd(samples) * sqrt((1 + ac1) / n)\n  a &lt;- floor(0.1 * n); b &lt;- floor(0.5 * n)\n  z &lt;- (mean(samples[1:a]) - mean(samples[(n - b + 1):n])) /\n       sqrt(var(samples[1:a]) / a + var(samples[(n - b + 1):n]) / b)\n  ess &lt;- n / (1 + 2 * sum(acf(samples, plot = FALSE)$acf[-1]))\n  c(autocorr1 = ac1, mcse = mcse, geweke_z = z, ess = ess)\n}\n\n# Apply to chain 1\nconv_results &lt;- t(apply(beta_samples[[1]][burn_in:n_iter, ], 2, convergence_stats))\nrownames(conv_results) &lt;- paste0(\"beta\", 0:p)\nround(conv_results, 4)\n\n\n      autocorr1   mcse geweke_z       ess\nbeta0    0.0232 0.0018  -0.9796 2745.9833\nbeta1    0.0374 0.0020   0.5127 2185.3118\nbeta2    0.3956 0.0016   0.4830  878.5174\nbeta3    0.0291 0.0006   0.9637 2597.4558\nbeta4    0.2506 0.0010  -0.4140  971.6853\nbeta5    0.0490 0.0018  -2.8212 3758.3315\n\n\n\nGelman–Rubin R-hat\n\n\nCode\nRhat &lt;- function(ch1, ch2) {\n  n &lt;- nrow(ch1)\n  m &lt;- 2\n  chain_means &lt;- c(colMeans(ch1), colMeans(ch2))\n  overall_mean &lt;- colMeans(rbind(ch1, ch2))\n  B &lt;- n * apply(rbind(colMeans(ch1), colMeans(ch2)), 2, var)\n  W &lt;- (apply(ch1, 2, var) + apply(ch2, 2, var)) / 2\n  var_hat &lt;- ((n - 1) / n) * W + (1 / n) * B\n  sqrt(var_hat / W)\n}\n\nRhat_values &lt;- Rhat(beta_samples[[1]][burn_in:n_iter, ], beta_samples[[2]][burn_in:n_iter, ])\nround(Rhat_values, 3)\n\n\n[1] 1 1 1 1 1 1"
  },
  {
    "objectID": "bayesian_spike_and_slab.html#posterior-mean-vs-true-values",
    "href": "bayesian_spike_and_slab.html#posterior-mean-vs-true-values",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Posterior Mean vs True Values",
    "text": "Posterior Mean vs True Values\n\n\nCode\npar(mar = c(5, 5, 4, 2))\nplot(beta_true, beta_summary[, \"mean\"], pch = 19, col = \"blue\",\n     xlab = \"True Coefficients\", ylab = \"Posterior Mean Estimates\",\n     main = \"Posterior Mean vs True Coefficients\")\nabline(0, 1, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topleft\", legend = c(\"Posterior Means\", \"y = x line\"), \n       col = c(\"blue\", \"red\"), pch = c(19, NA), lty = c(NA, 2))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Classical and Bayesian Linear Regression",
    "section": "",
    "text": "This website provides theoretical notes, slides, and practical examples for learning and teaching Bayesian Linear Regression and related topics.\nThe materials cover both classical and Bayesian approaches to regression, including model estimation, prior specification, Gibbs sampling, and convergence diagnostics.\nExplore the sections below to find the corresponding materials.\n\n\n\n\n\n\nSection\nDescription\n\n\n\n\nNotes\nTheoretical notes on Bayesian linear regression, Gibbs sampling, and convergence diagnostics.\n\n\nSlides\nLecture slides summarizing key theoretical concepts and derivations.\n\n\nClassical Regression\nSimulation and estimation using ordinary least squares (OLS).\n\n\nBayesian (Gaussian Prior)\nBayesian regression with conjugate Gaussian priors and closed-form Gibbs sampling.\n\n\nBayesian (Spike & Slab)\nBayesian regression with spike-and-slab priors for variable selection and sparsity.\n\n\n\n?? Download Notes (PDF)\n?? Download Slides (PDF)\n\n\n\n\nYou can clone or download this repository and render the documents using Quarto in RStudio or from the terminal:\n```bash quarto render"
  },
  {
    "objectID": "index.html#overview-of-materials",
    "href": "index.html#overview-of-materials",
    "title": "Classical and Bayesian Linear Regression",
    "section": "",
    "text": "Section\nDescription\n\n\n\n\nNotes\nTheoretical notes on Bayesian linear regression, Gibbs sampling, and convergence diagnostics.\n\n\nSlides\nLecture slides summarizing key theoretical concepts and derivations.\n\n\nClassical Regression\nSimulation and estimation using ordinary least squares (OLS).\n\n\nBayesian (Gaussian Prior)\nBayesian regression with conjugate Gaussian priors and closed-form Gibbs sampling.\n\n\nBayesian (Spike & Slab)\nBayesian regression with spike-and-slab priors for variable selection and sparsity.\n\n\n\n?? Download Notes (PDF)\n?? Download Slides (PDF)"
  },
  {
    "objectID": "index.html#how-to-reproduce-locally",
    "href": "index.html#how-to-reproduce-locally",
    "title": "Classical and Bayesian Linear Regression",
    "section": "",
    "text": "You can clone or download this repository and render the documents using Quarto in RStudio or from the terminal:\n```bash quarto render"
  }
]