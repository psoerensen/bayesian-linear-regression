[
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "",
    "text": "Bayesian linear regression (BLR) extends the classical linear regression framework by incorporating prior information into the model and producing full posterior distributions over parameters, rather than single-point estimates. This approach offers several key advantages, particularly in the context of modern data analysis challenges such as high dimensionality, small sample sizes, and the need for uncertainty quantification.\nIn genomics and other biological applications, BLR is widely used for tasks such as mapping genetic variants, predicting genetic predisposition (e.g., polygenic risk scores), estimating genetic parameters like heritability, and performing gene set enrichment or pathway analyses. These applications benefit from BLR’s ability to unify inference and prediction within a probabilistic framework.\nThe BLR model builds on the familiar linear regression formulation, where the observed outcome is modeled as a linear function of predictors plus Gaussian noise. However, unlike classical inference—which relies on least squares or maximum likelihood estimation and provides only point estimates and asymptotic intervals—Bayesian inference yields full posterior distributions over the unknown coefficients and variance. This allows for richer uncertainty quantification and more robust inference.\nSeveral motivations drive the use of Bayesian methods in linear regression. First, BLR naturally quantifies uncertainty through posterior distributions, allowing the analyst to compute credible intervals, posterior probabilities, and predictive distributions. Second, prior distributions act as regularizers, helping to stabilize estimation in noisy or underdetermined settings, such as when the number of predictors \\(p\\) exceeds the number of observations \\(n\\). Gaussian priors encourage shrinkage toward zero, while more structured priors (such as spike-and-slab) enable sparse or grouped solutions. Third, BLR makes it straightforward to incorporate external knowledge—such as biological relevance or prior experimental results—into the modeling process.\nThese notes begin by reviewing the classical linear regression model and its limitations. We then introduce the Bayesian linear regression model, outline the inference workflow, and show how to derive the full conditional posterior distributions for the model parameters using conjugate priors. Finally, we describe how posterior inference is performed using Gibbs sampling and conclude with practical considerations for implementation, diagnostics, and applications in R."
  },
  {
    "objectID": "notes.html#posterior-summaries-and-inference-from-gibbs-samples",
    "href": "notes.html#posterior-summaries-and-inference-from-gibbs-samples",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Summaries and Inference from Gibbs Samples",
    "text": "Posterior Summaries and Inference from Gibbs Samples\nAfter running the Gibbs sampler and obtaining \\(T\\) posterior draws of all parameters, we can use these samples to compute a wide range of quantities relevant to Bayesian inference. These include:\n\nPosterior means and medians as point estimates of parameters\nCredible intervals to quantify uncertainty\nPosterior standard deviations as measures of variability\nPosterior probabilities of hypotheses, such as \\(\\Pr(\\beta_j &gt; 0 \\mid y)\\)\nPosterior predictive distributions for new observations\nModel diagnostics such as convergence checks or residual analysis\n\nThese quantities allow us to summarize uncertainty, generate predictions, and make probabilistic statements about model parameters and data.\n\nPosterior Summaries\nOnce we have a collection of posterior draws for a parameter \\(\\theta\\) (e.g., \\(\\beta_j\\), \\(\\sigma^2\\), or \\(\\sigma_b^2\\)), we can summarize the posterior distribution using:\n\nPosterior mean: \\[\n\\mathbb{E}[\\theta \\mid y] \\approx \\frac{1}{T} \\sum_{t=1}^{T} \\theta^{(t)}\n\\]\nPosterior median: The median value of the sampled \\(\\theta^{(t)}\\).\nCredible intervals: For example, a 95% credible interval for \\(\\theta\\) can be obtained as the 2.5% and 97.5% quantiles of the posterior samples: \\[\n[\\theta]_{0.025}, [\\theta]_{0.975}\n\\]\n\nThese summaries provide insight into the likely values of the parameter after accounting for uncertainty in both the data and prior beliefs.\n\n\nEstimating Uncertainty\nBayesian inference provides full posterior distributions, not just point estimates. This allows us to directly quantify the uncertainty of parameters:\n\nPosterior standard deviation: \\[\n\\mathrm{SD}(\\theta \\mid y) \\approx \\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T} \\left( \\theta^{(t)} - \\bar{\\theta} \\right)^2}\n\\]\nThis uncertainty is reflected in the width of the credible intervals and can vary across different parameters or under different priors.\n\n\n\nPrediction\nGiven a new observation \\(x_{\\text{new}}\\), we can generate posterior predictive distributions using the sampled parameter values:\n\nFor each draw \\(t\\), compute: \\[\n\\hat{y}_{\\text{new}}^{(t)} = x_{\\text{new}}^\\top \\beta^{(t)}\n\\]\nOptionally, add residual noise from the corresponding draw of \\(\\sigma^{2(t)}\\): \\[\ny_{\\text{new}}^{(t)} \\sim \\mathcal{N}\\left(x_{\\text{new}}^\\top \\beta^{(t)},\\; \\sigma^{2(t)} \\right)\n\\]\nUse these \\(y_{\\text{new}}^{(t)}\\) samples to construct predictive intervals or evaluate predictive performance.\n\n\n\nModel Checking and Hypothesis Testing\nThe posterior draws can also be used for model diagnostics or hypothesis testing:\n\nPosterior probability of an event, such as a non-zero effect: \\[\n\\Pr(\\beta_j \\ne 0 \\mid y) \\approx \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{1}\\left( \\beta_j^{(t)} \\ne 0 \\right)\n\\]\nPosterior predictive checks: Simulate new datasets from the model using posterior draws and compare them to the observed data. Discrepancies may indicate model misfit.\nBayes factors and marginal likelihoods can be computed or approximated for formal hypothesis testing or model comparison, though these often require specialized methods beyond standard Gibbs output.\n\nThese procedures allow us to move beyond point estimates and engage in a full Bayesian analysis that accounts for uncertainty in parameter estimation, prediction, and decision-making."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Linear Regression",
    "section": "",
    "text": "The following materials include theoretical notes, slides, and practical R examples for exploring Bayesian Linear Regression. It introduces both classical and Bayesian regression methods, showing how to estimate parameters, define priors, perform posterior inference via Gibbs sampling, and assess convergence - all through practical R code.\nExplore the sections below to find the corresponding materials."
  },
  {
    "objectID": "index.html#overview-of-materials",
    "href": "index.html#overview-of-materials",
    "title": "Bayesian Linear Regression",
    "section": "Overview of Materials",
    "text": "Overview of Materials\n\n\n\nSection\nDescription\n\n\n\n\nBLR notes\nTheoretical notes on Bayesian linear regression, Gibbs sampling, and convergence diagnostics.\n\n\nBLR slides\nLecture slides summarizing key theoretical concepts and derivations in Bayesian Linear Regression Analyses.\n\n\nGSEA slides\nLecture slides introducing Gene Set Analyses using Bayesian Linear Regression Models.\n\n\nClassical Regression tutorial\nSimulation and estimation using ordinary least squares (OLS) in R.\n\n\nBayesian (Gaussian Prior) tutorial\nBayesian regression with conjugate Gaussian priors and closed-form Gibbs sampling in R.\n\n\nBayesian (Spike & Slab) tutorial\nBayesian regression with spike-and-slab priors for variable selection and sparsity in R.\n\n\nBayesian MAGMA tutorial\nBayesian gene set analysis in R.\n\n\n\nDownload Notes (PDF)\nDownload Slides (PDF)\n\n\n\nFurther Reading\nFurther details on the theory and computation behind Bayesian linear regression, Gibbs sampling, and hierarchical modeling can be found in\nSorensen, D. (2025). Statistical Learning in Genetics: An Introduction Using R. Springer.\nThis book provides a rigorous and accessible introduction to Bayesian modeling, hierarchical inference, and statistical learning methods in quantitative genetics and genomics."
  },
  {
    "objectID": "index.html#the-qgg-r-package",
    "href": "index.html#the-qgg-r-package",
    "title": "Bayesian Linear Regression",
    "section": "The qgg R Package",
    "text": "The qgg R Package\nqgg provides tools for statistical modeling and analysis of large-scale genomic data, including:\n\nFine-mapping of genomic regions using Bayesian Linear Regression (BLR) models\n\nPolygenic scoring using Bayesian Linear Regression (BLR) models\n\nGene set enrichment analysis using Bayesian Linear Regression (BLR) models\n\nqgg handles large-scale genomic data through efficient algorithms and sparse matrix techniques, combined with multi-core processing using OpenMP, multithreaded matrix operations via BLAS libraries (e.g., OpenBLAS, ATLAS, or MKL), and fast, memory-efficient batch processing of genotype data stored in\nbinary formats such as PLINK .bed files."
  },
  {
    "objectID": "index.html#the-gact-r-package",
    "href": "index.html#the-gact-r-package",
    "title": "Bayesian Linear Regression",
    "section": "The gact R Package",
    "text": "The gact R Package\ngact provides an infrastructure for efficient processing of large-scale genomic association data, with core functions for:\n\nEstablishing and populating a database of genomic associations\n\nDownloading and processing biological databases\n\nHandling and processing GWAS summary statistics\n\nLinking genetic markers to genes, proteins, metabolites, and biological pathways\n\nIntegrates with statistical machine learning tools in the qgg R package\n\ngact is intended to serve as a practical implementation of integrative genomics, bridging statistical modeling and biological interpretation, and supporting reproducible and extensible workflows.\nTutorials using the qgg and gact R packages"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Bayesian Linear Regression",
    "section": "References",
    "text": "References\nSørensen P, Rohde PD. A Versatile Data Repository for GWAS Summary Statistics-Based Downstream Genomic Analysis of Human Complex Traits. medRxiv (2025). https://doi.org/10.1101/2025.10.01.25337099\nSørensen IF, Sørensen P. Privacy-Preserving Multivariate Bayesian Regression Models for Overcoming Data Sharing Barriers in Health and Genomics. medRxiv (2025). https://doi.org/10.1101/2025.07.30.25332448\nHjelholt AJ, Gholipourshahraki T, Bai Z, Shrestha M, Kjølby M, Sørensen P, Rohde P. Leveraging Genetic Correlations to Prioritize Drug Groups for Repurposing in Type 2 Diabetes. medRxiv (2025). https://doi.org/10.1101/2025.06.13.25329590\nGholipourshahraki T, Bai Z, Shrestha M, Hjelholt A, Rohde P, Fuglsang MK, Sørensen P. Evaluation of Bayesian Linear Regression Models for Gene Set Prioritization in Complex Diseases. PLOS Genetics 20(11): e1011463 (2025). https://doi.org/10.1371/journal.pgen.1011463\nBai Z, Gholipourshahraki T, Shrestha M, Hjelholt A, Rohde P, Fuglsang MK, Sørensen P. Evaluation of Bayesian Linear Regression Derived Gene Set Test Methods. BMC Genomics 25(1): 1236 (2024). https://doi.org/10.1186/s12864-024-11026-2\nShrestha M, Bai Z, Gholipourshahraki T, Hjelholt A, Rohde P, Fuglsang MK, Sørensen P. Enhanced Genetic Fine Mapping Accuracy with Bayesian Linear Regression Models in Diverse Genetic Architectures. PLOS Genetics 21(7): e1011783 (2025). https://doi.org/10.1371/journal.pgen.1011783\nKunkel D, Sørensen P, Shankar V, Morgante F. Improving Polygenic Prediction from Summary Data by Learning Patterns of Effect Sharing Across Multiple Phenotypes. PLOS Genetics 21(1): e1011519 (2025). https://doi.org/10.1371/journal.pgen.1011519\nRohde P, Sørensen IF, Sørensen P. Expanded Utility of the R Package qgg with Applications within Genomic Medicine. Bioinformatics 39:11 (2023). https://doi.org/10.1093/bioinformatics/btad656\nRohde P, Sørensen IF, Sørensen P. qgg: An R Package for Large-Scale Quantitative Genetic Analyses. Bioinformatics 36(8): 2614–2615 (2020). https://doi.org/10.1093/bioinformatics/btz955"
  },
  {
    "objectID": "bayesian_spike_and_slab.html",
    "href": "bayesian_spike_and_slab.html",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "",
    "text": "This document demonstrates Bayesian Linear Regression using spike-and-slab priors, as described in the notes. We implement a Gibbs sampler that updates parameters from their full conditionals, computes posterior summaries (including posterior inclusion probabilities), and evaluates convergence using diagnostic statistics."
  },
  {
    "objectID": "bayesian_spike_and_slab.html#introduction",
    "href": "bayesian_spike_and_slab.html#introduction",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "",
    "text": "This document demonstrates Bayesian Linear Regression using spike-and-slab priors, as described in the notes. We implement a Gibbs sampler that updates parameters from their full conditionals, computes posterior summaries (including posterior inclusion probabilities), and evaluates convergence using diagnostic statistics."
  },
  {
    "objectID": "bayesian_spike_and_slab.html#model-setup-and-data-simulation",
    "href": "bayesian_spike_and_slab.html#model-setup-and-data-simulation",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Model Setup and Data Simulation",
    "text": "Model Setup and Data Simulation\n\n\nCode\nset.seed(123)\n\n# Simulate data\nn &lt;- 100\np &lt;- 5\nX &lt;- cbind(1, matrix(rnorm(n * p), n, p))\nbeta_true &lt;- c(2, 1.2, 0, 0, 0, 1.5)  # some zeros (spike)\nsigma_true &lt;- 1\n\ny &lt;- X %*% beta_true + rnorm(n, 0, sigma_true)"
  },
  {
    "objectID": "bayesian_spike_and_slab.html#gibbs-sampler-implementation",
    "href": "bayesian_spike_and_slab.html#gibbs-sampler-implementation",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Gibbs Sampler Implementation",
    "text": "Gibbs Sampler Implementation\nWe use the following priors:\n\n\\(\\alpha_i \\mid \\sigma_b^2 \\sim \\mathcal{N}(0, \\sigma_b^2)\\)\n\\(\\delta_i \\mid \\pi \\sim \\text{Bernoulli}(\\pi)\\)\n\\(\\pi \\sim \\text{Beta}(a, b)\\)\n\\(\\sigma_b^2 \\sim S_b \\chi^{-2}(v_b)\\)\n\\(\\sigma^2 \\sim S \\chi^{-2}(v)\\)\n\n\n\nCode\n# Hyperparameters\na_pi &lt;- 1\nb_pi &lt;- 1\nv_b &lt;- 4\nS_b &lt;- 1\nv &lt;- 4\nS &lt;- 1\n\n# Gibbs sampler setup\nn_iter &lt;- 4000\nburn_in &lt;- 1000\nchains &lt;- 2\n\n# Storage\nbeta_samples &lt;- vector(\"list\", chains)\ndelta_samples &lt;- vector(\"list\", chains)\npi_samples &lt;- vector(\"list\", chains)\nsigma2_samples &lt;- vector(\"list\", chains)\nsigma2b_samples &lt;- vector(\"list\", chains)\n\nfor (c in 1:chains) {\n  # Initial values\n  alpha &lt;- rnorm(ncol(X), 0, 1)\n  delta &lt;- rbinom(ncol(X), 1, 0.5)\n  sigma2 &lt;- 1\n  sigma2_b &lt;- 1\n  pi &lt;- 0.5\n  \n  # Containers\n  alpha_chain &lt;- matrix(NA, n_iter, ncol(X))\n  delta_chain &lt;- matrix(NA, n_iter, ncol(X))\n  sigma2_chain &lt;- numeric(n_iter)\n  sigma2b_chain &lt;- numeric(n_iter)\n  pi_chain &lt;- numeric(n_iter)\n  \n  for (t in 1:n_iter) {\n    # Sample each alpha_i given delta_i\n    for (j in 1:ncol(X)) {\n      X_j &lt;- X[, j]\n      r_j &lt;- y - X %*% (alpha * delta) + X_j * alpha[j] * delta[j]\n      \n      if (delta[j] == 1) {\n        var_j &lt;- sigma2 / (t(X_j) %*% X_j + sigma2 / sigma2_b)\n        mean_j &lt;- as.numeric(var_j * t(X_j) %*% r_j / sigma2)\n        alpha[j] &lt;- rnorm(1, mean_j, sqrt(var_j))\n      } else {\n        alpha[j] &lt;- rnorm(1, 0, sqrt(sigma2_b))  # prior draw\n      }\n    }\n    \n    # Sample delta_i given alpha_i\n    for (j in 1:ncol(X)) {\n      X_j &lt;- X[, j]\n      r_j &lt;- y - X %*% (alpha * delta) + X_j * alpha[j] * delta[j]\n      rss0 &lt;- sum((r_j)^2)\n      rss1 &lt;- sum((r_j - X_j * alpha[j])^2)\n      \n      logodds &lt;- 0.5 / sigma2 * (rss0 - rss1) + log(pi) - log(1 - pi)\n      p1 &lt;- 1 / (1 + exp(-logodds))\n      delta[j] &lt;- rbinom(1, 1, p1)\n    }\n    \n    # Update pi\n    pi &lt;- rbeta(1, a_pi + sum(delta), b_pi + ncol(X) - sum(delta))\n    \n    # Update sigma_b^2\n    p_incl &lt;- sum(delta)\n    v_b_tilde &lt;- v_b + p_incl\n    S_b_tilde &lt;- (sum((alpha * delta)^2) + v_b * S_b) / v_b_tilde\n    sigma2_b &lt;- v_b_tilde * S_b_tilde / rchisq(1, df = v_b_tilde)\n    \n    # Update sigma^2\n    resid &lt;- y - X %*% (alpha * delta)\n    v_tilde &lt;- v + n\n    S_tilde &lt;- (sum(resid^2) + v * S) / v_tilde\n    sigma2 &lt;- v_tilde * S_tilde / rchisq(1, df = v_tilde)\n    \n    # Store\n    alpha_chain[t, ] &lt;- alpha * delta\n    delta_chain[t, ] &lt;- delta\n    pi_chain[t] &lt;- pi\n    sigma2_chain[t] &lt;- sigma2\n    sigma2b_chain[t] &lt;- sigma2_b\n  }\n  \n  beta_samples[[c]] &lt;- alpha_chain\n  delta_samples[[c]] &lt;- delta_chain\n  pi_samples[[c]] &lt;- pi_chain\n  sigma2_samples[[c]] &lt;- sigma2_chain\n  sigma2b_samples[[c]] &lt;- sigma2b_chain\n}"
  },
  {
    "objectID": "bayesian_spike_and_slab.html#posterior-summaries",
    "href": "bayesian_spike_and_slab.html#posterior-summaries",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\n\nCode\nposterior_summary &lt;- function(samples, probs = c(0.025, 0.5, 0.975)) {\n  c(mean = mean(samples), sd = sd(samples), quantile(samples, probs = probs))\n}\n\n# Combine chains\nbeta_all &lt;- do.call(rbind, beta_samples)\npi_all &lt;- unlist(pi_samples)\ndelta_all &lt;- do.call(rbind, delta_samples)\n\nbeta_summary &lt;- t(apply(beta_all[burn_in:nrow(beta_all), ], 2, posterior_summary))\nrownames(beta_summary) &lt;- paste0(\"beta\", 0:p)\n\nPIP &lt;- colMeans(delta_all[burn_in:nrow(delta_all), ])\n\nround(beta_summary, 4)\n\n\n         mean     sd    2.5%    50%  97.5%\nbeta0  1.9340 0.0975  1.7469 1.9348 2.1217\nbeta1  1.1750 0.1057  0.9651 1.1759 1.3812\nbeta2  0.0330 0.0750  0.0000 0.0000 0.2550\nbeta3  0.0027 0.0352 -0.0568 0.0000 0.1062\nbeta4 -0.0139 0.0487 -0.1804 0.0000 0.0082\nbeta5  1.6871 0.0974  1.4967 1.6865 1.8798\n\n\nCode\ncat(\"\\nPosterior Inclusion Probabilities (PIP):\\n\")\n\n\n\nPosterior Inclusion Probabilities (PIP):\n\n\nCode\nround(PIP, 3)\n\n\n[1] 1.000 1.000 0.250 0.124 0.159 1.000"
  },
  {
    "objectID": "bayesian_spike_and_slab.html#trace-plots",
    "href": "bayesian_spike_and_slab.html#trace-plots",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\nCode\npar(mfrow = c(3, 2))\nfor (j in 1:ncol(X)) {\n  plot(beta_samples[[1]][, j], type = \"l\", main = paste(\"Trace: beta\", j - 1, \"(chain 1)\"),\n       xlab = \"Iteration\", ylab = expression(beta))\n  abline(h = beta_true[j], col = \"red\", lwd = 2, lty = 2)\n}"
  },
  {
    "objectID": "bayesian_spike_and_slab.html#autocorrelation-plots",
    "href": "bayesian_spike_and_slab.html#autocorrelation-plots",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Autocorrelation Plots",
    "text": "Autocorrelation Plots\n\n\nCode\npar(mfrow = c(3, 2))\nfor (j in 1:ncol(X)) {\n  acf(beta_samples[[1]][burn_in:n_iter, j], main = paste(\"ACF: beta\", j - 1))\n}"
  },
  {
    "objectID": "bayesian_spike_and_slab.html#convergence-diagnostics",
    "href": "bayesian_spike_and_slab.html#convergence-diagnostics",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\n\n\nCode\nconvergence_stats &lt;- function(samples) {\n  n &lt;- length(samples)\n  ac1 &lt;- cor(samples[-1], samples[-n])\n  mcse &lt;- sd(samples) * sqrt((1 + ac1) / n)\n  a &lt;- floor(0.1 * n); b &lt;- floor(0.5 * n)\n  z &lt;- (mean(samples[1:a]) - mean(samples[(n - b + 1):n])) /\n       sqrt(var(samples[1:a]) / a + var(samples[(n - b + 1):n]) / b)\n  ess &lt;- n / (1 + 2 * sum(acf(samples, plot = FALSE)$acf[-1]))\n  c(autocorr1 = ac1, mcse = mcse, geweke_z = z, ess = ess)\n}\n\n# Apply to chain 1\nconv_results &lt;- t(apply(beta_samples[[1]][burn_in:n_iter, ], 2, convergence_stats))\nrownames(conv_results) &lt;- paste0(\"beta\", 0:p)\nround(conv_results, 4)\n\n\n      autocorr1   mcse geweke_z       ess\nbeta0    0.0232 0.0018  -0.9796 2745.9833\nbeta1    0.0374 0.0020   0.5127 2185.3118\nbeta2    0.3956 0.0016   0.4830  878.5174\nbeta3    0.0291 0.0006   0.9637 2597.4558\nbeta4    0.2506 0.0010  -0.4140  971.6853\nbeta5    0.0490 0.0018  -2.8212 3758.3315\n\n\n\nGelman–Rubin R-hat\n\n\nCode\nRhat &lt;- function(ch1, ch2) {\n  n &lt;- nrow(ch1)\n  m &lt;- 2\n  chain_means &lt;- c(colMeans(ch1), colMeans(ch2))\n  overall_mean &lt;- colMeans(rbind(ch1, ch2))\n  B &lt;- n * apply(rbind(colMeans(ch1), colMeans(ch2)), 2, var)\n  W &lt;- (apply(ch1, 2, var) + apply(ch2, 2, var)) / 2\n  var_hat &lt;- ((n - 1) / n) * W + (1 / n) * B\n  sqrt(var_hat / W)\n}\n\nRhat_values &lt;- Rhat(beta_samples[[1]][burn_in:n_iter, ], beta_samples[[2]][burn_in:n_iter, ])\nround(Rhat_values, 3)\n\n\n[1] 1 1 1 1 1 1"
  },
  {
    "objectID": "bayesian_spike_and_slab.html#posterior-mean-vs-true-values",
    "href": "bayesian_spike_and_slab.html#posterior-mean-vs-true-values",
    "title": "Bayesian Linear Regression with Spike-and-Slab Priors",
    "section": "Posterior Mean vs True Values",
    "text": "Posterior Mean vs True Values\n\n\nCode\npar(mar = c(5, 5, 4, 2))\nplot(beta_true, beta_summary[, \"mean\"], pch = 19, col = \"blue\",\n     xlab = \"True Coefficients\", ylab = \"Posterior Mean Estimates\",\n     main = \"Posterior Mean vs True Coefficients\")\nabline(0, 1, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topleft\", legend = c(\"Posterior Means\", \"y = x line\"), \n       col = c(\"blue\", \"red\"), pch = c(19, NA), lty = c(NA, 2))"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#overview",
    "href": "bayesian_linear_regression_slides.html#overview",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Overview",
    "text": "Overview\n\nClassical Linear Regression\n\nModel, inference, and limitations\n\nBayesian Linear Regression\n\nMotivation, priors, and posteriors\n\nConditional posteriors and inference\n\nComputation and Applications\n\nMCMC and Gibbs sampling\n\nDiagnostics and R implementation"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#introduction",
    "href": "bayesian_linear_regression_slides.html#introduction",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Linear Regression (BLR) extends classical regression by incorporating prior information and producing posterior distributions over model parameters.\n\nAdvantages:\n\nHandles high-dimensional and small-sample problems.\n\nProvides full uncertainty quantification.\n\nEnables regularization and integration of prior biological knowledge."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#applications-in-genomics",
    "href": "bayesian_linear_regression_slides.html#applications-in-genomics",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Applications in Genomics",
    "text": "Applications in Genomics\n\nBayesian Linear Regression (BLR) is widely applied in quantitative genetics and genomics.\n\nCommon use cases:\n\nGenome-Wide Association Studies (GWAS) and fine-mapping of causal variants.\n\nGenetic prediction and heritability estimation.\n\nPathway and gene-set enrichment analyses.\n\nIntegrative multi-omics modeling (genome, transcriptome, epigenome)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#classical-linear-regression",
    "href": "bayesian_linear_regression_slides.html#classical-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Classical Linear Regression",
    "text": "Classical Linear Regression\nModel\n\\[\ny = X\\beta + e, \\quad e \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n\\] - \\(y\\): outcomes\n- \\(X\\): design matrix\n- \\(\\beta\\): coefficients\n- \\(e\\): are the residuals\n- \\(\\sigma^2\\): residual variance"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#estimation",
    "href": "bayesian_linear_regression_slides.html#estimation",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Estimation",
    "text": "Estimation\nRegression effects: \\[\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n\\]\nResidual variance: \\[\n\\hat{\\sigma}^2 = \\frac{1}{n-p}\\sum_i (y_i - x_i^\\top \\hat{\\beta})^2\n\\]\nInference via standard errors and \\(t\\)-tests, confidence intervals, and prediction intervals."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#limitations",
    "href": "bayesian_linear_regression_slides.html#limitations",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Limitations",
    "text": "Limitations\n\nNo explicit control over effect size distribution\nSensitive when collinearity is high\nNot identifiable when \\(p&gt;n\\)\nUncertainty largely asymptotic unless normality assumptions hold"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#why-bayesian-linear-regression",
    "href": "bayesian_linear_regression_slides.html#why-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Why Bayesian Linear Regression?",
    "text": "Why Bayesian Linear Regression?\n\nCombines likelihood and prior to form the posterior.\n\nPriors express beliefs about effect sizes:\n\nNormal → many small effects\n\nSpike-and-slab → sparse effects\n\n\nActs as a regularizer:\n\nShrinks small/noisy effects toward \\(0\\)\n\nPreserves large, important effects\n\n\nStable when \\(p &gt; n\\) due to prior information.\n\nProvides full posterior distributions for \\(\\beta\\) and \\(\\sigma^2\\)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#overview-bayesian-linear-regression",
    "href": "bayesian_linear_regression_slides.html#overview-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Overview: Bayesian Linear Regression",
    "text": "Overview: Bayesian Linear Regression\n\nCombines data and prior knowledge using Bayes’ rule.\n\nUses conjugate priors to yield closed-form full conditionals.\n\nEmploys Gibbs sampling to approximate the posterior distribution.\n\nEstimates parameters, uncertainty, and predictions from posterior draws."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#bayesian-linear-regression-with-gaussian-priors",
    "href": "bayesian_linear_regression_slides.html#bayesian-linear-regression-with-gaussian-priors",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Bayesian Linear Regression with Gaussian Priors",
    "text": "Bayesian Linear Regression with Gaussian Priors\nBayesian linear regression starts with the same model structure as classical linear regression.\n\\[\ny = X\\beta + e, \\quad e \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n\\]\n\n\\(y\\): \\(n \\times 1\\) vector of observed outcomes\n\n\\(X\\): \\(n \\times p\\) design matrix of predictors\n\n\\(\\beta\\): \\(p \\times 1\\) vector of unknown coefficients\n\n\\(e\\): Gaussian noise with mean \\(0\\) and variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#likelihood-in-bayesian-linear-regression",
    "href": "bayesian_linear_regression_slides.html#likelihood-in-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Likelihood in Bayesian Linear Regression",
    "text": "Likelihood in Bayesian Linear Regression\nBecause the residuals are Gaussian, it follows that the marginal distribution of \\(y\\) is:\n\\[\ne \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n\\] The marginal distribution of \\(y\\) is:\n\\[\ny \\sim \\mathcal{N}(X\\beta, \\sigma^2 I_n)\n\\] This defines the likelihood the probability of the observed data given parameters \\(\\beta\\) and \\(\\sigma^2\\):\n\\[\np(y \\mid X, \\beta, \\sigma^2) = \\mathcal{N}(X\\beta, \\sigma^2 I_n)\n\\]"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#introducing-priors",
    "href": "bayesian_linear_regression_slides.html#introducing-priors",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Introducing Priors",
    "text": "Introducing Priors\nIn Bayesian linear regression, we specify prior distributions that express our beliefs about parameters before seeing the data.\nA common conjugate prior for the regression coefficients is:\n\\[\n\\beta \\mid \\sigma_b^2 \\sim \\mathcal{N}(0, \\sigma_b^2 I_p)\n\\]\nThis reflects the belief that most effect sizes are small and centered near zero — consistent with the polygenic assumption in genetics."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#role-of-the-prior-variance-sigma_b2",
    "href": "bayesian_linear_regression_slides.html#role-of-the-prior-variance-sigma_b2",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Role of the Prior Variance \\(\\sigma_b^2\\)",
    "text": "Role of the Prior Variance \\(\\sigma_b^2\\)\nThe parameter \\(\\sigma_b^2\\) acts as a shrinkage (regularization) parameter:\n\nSmall \\(\\sigma_b^2\\) → stronger shrinkage toward zero.\n\nLarge \\(\\sigma_b^2\\) → weaker shrinkage, allowing larger effects.\n\nIt controls the strength of regularization and is often treated as an unknown hyperparameter estimated from the data."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#priors-on-variance-components",
    "href": "bayesian_linear_regression_slides.html#priors-on-variance-components",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Priors on Variance Components",
    "text": "Priors on Variance Components\nWe also place priors on the variance components to complete the hierarchical model.\n\\[\n\\sigma_b^2 \\mid S_b, v_b \\sim S_b \\, \\chi^{-2}(v_b), \\quad\n\\sigma^2 \\mid S, v \\sim S \\, \\chi^{-2}(v)\n\\]\nHere:\n\n\\(S_b\\) and \\(v_b\\) are user-defined hyperparameters that control the prior distribution on the variance of regression coefficients.\n\n\\(S\\) and \\(v\\) are hyperparameters for the residual variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#conjugate-priors-and-regularization",
    "href": "bayesian_linear_regression_slides.html#conjugate-priors-and-regularization",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Conjugate Priors and Regularization",
    "text": "Conjugate Priors and Regularization\nConjugate priors keep posteriors in the same family\n(e.g., scaled inverse-chi-squared), allowing closed-form Gibbs updates.\nThey also serve as regularizers:\n\nThe prior on \\(\\beta\\) shrinks small or noisy effects toward zero.\n\nPriors on variance components prevent overfitting, especially when \\(p &gt; n\\).\n\nThus, conjugate priors make Bayesian linear regression efficient and stable."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#posterior-distribution",
    "href": "bayesian_linear_regression_slides.html#posterior-distribution",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\nIn Bayesian analysis, we combine the likelihood and priors using Bayes’ rule to obtain the joint posterior:\n\\[\np(\\beta, \\sigma_b^2, \\sigma^2 \\mid y) \\propto\np(y \\mid \\beta, \\sigma^2)\\;\np(\\beta \\mid \\sigma_b^2)\\;\np(\\sigma_b^2)\\;\np(\\sigma^2)\n\\]\nThis posterior captures all updated knowledge about the unknown parameters after observing the data.\nIt forms the basis for computing posterior means, credible intervals, and predictions."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#conjugacy-and-gibbs-sampling",
    "href": "bayesian_linear_regression_slides.html#conjugacy-and-gibbs-sampling",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Conjugacy and Gibbs Sampling",
    "text": "Conjugacy and Gibbs Sampling\nWith conjugate priors, each parameter’s full conditional distribution has a closed-form solution.\nThis makes Gibbs sampling a natural and efficient inference method.\n\nParameters are updated one at a time, each from its conditional posterior.\n\nThe resulting Markov chain explores the joint posterior of\n\\((\\beta, \\sigma_b^2, \\sigma^2)\\).\n\nGibbs sampling thus provides an easy way to approximate the full posterior in Bayesian linear regression."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#full-conditional-for-beta",
    "href": "bayesian_linear_regression_slides.html#full-conditional-for-beta",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Full Conditional for \\(\\beta\\)",
    "text": "Full Conditional for \\(\\beta\\)\nGiven \\(\\sigma^2\\), \\(\\sigma_b^2\\), and the data \\(y\\), the regression coefficients have a multivariate normal conditional posterior:\n\\[\n\\beta \\mid \\sigma^2, \\sigma_b^2, y \\sim\n\\mathcal{N}(\\mu_\\beta, \\Sigma_\\beta)\n\\]\nwhere\n\\[\n\\Sigma_\\beta = \\left( \\frac{X^\\top X}{\\sigma^2} + \\frac{I}{\\sigma_b^2} \\right)^{-1},\n\\quad\n\\mu_\\beta = \\Sigma_\\beta \\frac{X^\\top y}{\\sigma^2}\n\\]\nThis distribution represents our updated belief about \\(\\beta\\)\nafter observing the data, while holding \\(\\sigma_b^2\\) and \\(\\sigma^2\\) fixed."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#comparison-to-classical-ols",
    "href": "bayesian_linear_regression_slides.html#comparison-to-classical-ols",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Comparison to Classical OLS",
    "text": "Comparison to Classical OLS\nIn classical regression, the OLS estimator is\n\\[\n\\hat\\beta_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top y,\n\\quad y \\sim \\mathcal{N}(X\\beta, \\sigma^2 I)\n\\]\nThe estimate of \\(\\beta\\) is independent of \\(\\sigma^2\\),\nsince \\(\\sigma^2\\) only scales the likelihood, not its maximum.\nIn Bayesian regression, \\(\\sigma^2\\) appears explicitly in the posterior:\n\\[\n\\Sigma_\\beta = \\left( \\frac{X^\\top X}{\\sigma^2} + \\frac{I}{\\sigma_b^2} \\right)^{-1},\n\\quad\n\\mu_\\beta = \\Sigma_\\beta \\frac{X^\\top y}{\\sigma^2}\n\\]\nThe term \\(\\frac{I}{\\sigma_b^2}\\) introduces shrinkage, regularizing estimates and stabilizing inference especially when \\(p &gt; n\\) or predictors are highly correlated.\nThus, the Bayesian posterior mean is a regularized, uncertainty-aware generalization of OLS."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#full-conditional-for-beta_j",
    "href": "bayesian_linear_regression_slides.html#full-conditional-for-beta_j",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Full Conditional for \\(\\beta_j\\)",
    "text": "Full Conditional for \\(\\beta_j\\)\nInstead of sampling \\(\\beta\\) jointly, we can update each coefficient \\(\\beta_j\\) one at a time, holding all others fixed efficient for large \\(p\\) or spike-and-slab models.\nLet \\(X_j\\) be the \\(j\\)th column of \\(X\\) and define the partial residual:\n\\[\nr_j = y - X_{-j} \\beta_{-j}\n\\]\nThen the conditional posterior for \\(\\beta_j\\) is univariate normal:\n\\[\n\\beta_j \\mid D \\sim \\mathcal{N} \\!\\left(\n\\frac{X_j^\\top r_j}{X_j^\\top X_j + \\sigma^2 / \\sigma_b^2},\\;\n\\frac{\\sigma^2}{X_j^\\top X_j + \\sigma^2 / \\sigma_b^2}\n\\right)\n\\]\nThis corresponds to a regularized least-squares update. Residual updates avoid matrix inversion, scale to high dimensions, and extend naturally to sparse (spike-and-slab) models."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#full-conditional-for-sigma_b2",
    "href": "bayesian_linear_regression_slides.html#full-conditional-for-sigma_b2",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Full Conditional for \\(\\sigma_b^2\\)",
    "text": "Full Conditional for \\(\\sigma_b^2\\)\nThe conditional distribution of the prior variance \\(\\sigma_b^2\\), given \\(\\beta\\) and the hyperparameters, is a scaled inverse-chi-squared:\n\\[\n\\sigma_b^2 \\mid \\beta \\sim \\tilde{S}_b \\, \\chi^{-2}(\\tilde{v}_b)\n\\]\nwhere\n\\[\n\\tilde{v}_b = v_b + p, \\quad\n\\tilde{S}_b = \\frac{\\beta^\\top \\beta + v_b S_b}{\\tilde{v}_b}\n\\]\nAt each Gibbs iteration, \\(\\sigma_b^2\\) is sampled directly given \\(\\beta\\). This update reflects our revised belief about the variability of effect sizes after observing the current posterior draw of \\(\\beta\\)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#full-conditional-for-sigma2",
    "href": "bayesian_linear_regression_slides.html#full-conditional-for-sigma2",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Full Conditional for \\(\\sigma^2\\)",
    "text": "Full Conditional for \\(\\sigma^2\\)\nThe conditional distribution of the residual variance \\(\\sigma^2\\),\ngiven \\(\\beta\\) and the data, is also scaled inverse-chi-squared:\n\\[\n\\sigma^2 \\mid \\beta, y \\sim \\tilde{S} \\, \\chi^{-2}(\\tilde{v})\n\\]\nwhere\n\\[\n\\tilde{v} = v + n, \\quad\n\\tilde{S} = \\frac{(y - X\\beta)^\\top (y - X\\beta) + v S}{\\tilde{v}}\n\\]\nAt each Gibbs iteration, \\(\\sigma^2\\) is sampled directly given \\(\\beta\\).\nThis captures our updated belief about the residual variability\nafter accounting for the current linear predictor \\(X\\beta\\)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#gibbs-sampling-motivation",
    "href": "bayesian_linear_regression_slides.html#gibbs-sampling-motivation",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Gibbs Sampling: Motivation",
    "text": "Gibbs Sampling: Motivation\nBayesian inference often involves complex posteriors that lack closed-form solutions. To approximate these, we use Markov Chain Monte Carlo (MCMC) methods.\nMCMC builds a Markov chain whose stationary distribution is the target posterior. Once the chain has converged, its samples can be used to estimate:\n\nPosterior means, variances, and credible intervals\n\nPredictive distributions\n\nOther functions of interest\n\nAmong MCMC algorithms, the Gibbs sampler is especially useful when all full conditional distributions are available in closed form."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#gibbs-sampling-the-algorithm",
    "href": "bayesian_linear_regression_slides.html#gibbs-sampling-the-algorithm",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Gibbs Sampling: The Algorithm",
    "text": "Gibbs Sampling: The Algorithm\nFor Bayesian linear regression with conjugate priors, the joint posterior is:\n\\[\np(\\beta, \\sigma_b^2 , \\sigma^2 \\mid y) \\propto\np(y \\mid \\beta, \\sigma^2)\\; p(\\beta \\mid \\sigma_b^2)\\; p(\\sigma_b^2)\\; p(\\sigma^2)\n\\]\nWe iteratively draw from the following full conditionals:\n\nSample \\(\\beta \\mid \\sigma_b^2, \\sigma^2, y\\)\n\nSample \\(\\sigma_b^2 \\mid \\beta\\)\n\nSample \\(\\sigma^2 \\mid \\beta, y\\)\n\nEach step updates one parameter given the latest values of the others. Repeating this sequence yields samples from the joint posterior \\(p(\\beta, \\sigma_b^2, \\sigma^2 \\mid y)\\).\nBecause each conditional is standard (Normal or scaled inverse-\\(\\chi^2\\)), Gibbs sampling is both efficient and easy to implement."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#posterior-summaries",
    "href": "bayesian_linear_regression_slides.html#posterior-summaries",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\nAfter running the Gibbs sampler, we obtain posterior draws \\(\\{\\theta^{(t)}\\}_{t=1}^T\\) for parameters such as \\(\\beta_j\\), \\(\\sigma^2\\), or \\(\\sigma_b^2\\).\nWe summarize the posterior distribution via:\n\nPosterior mean\n\\[\n\\mathbb{E}[\\theta \\mid y] \\approx \\frac{1}{T} \\sum_{t=1}^{T} \\theta^{(t)}\n\\]\nPosterior median: the median of \\(\\theta^{(t)}\\)\nCredible interval (95%)\n\\[\n[\\theta]_{0.025}, [\\theta]_{0.975}\n\\]\n\nThese summaries describe the most probable values of \\(\\theta\\)\nand their uncertainty after combining data and prior beliefs."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#estimating-uncertainty",
    "href": "bayesian_linear_regression_slides.html#estimating-uncertainty",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Estimating Uncertainty",
    "text": "Estimating Uncertainty\nBayesian inference provides full posterior distributions, not just point estimates. Uncertainty is quantified directly from the posterior samples:\n\nPosterior standard deviation\n\\[\n\\mathrm{SD}(\\theta \\mid y) \\approx\n\\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T} (\\theta^{(t)} - \\bar{\\theta})^2}\n\\]\n\nThe width of the credible interval reflects this uncertainty. Parameters with broader posteriors are estimated with less precision, and the degree of uncertainty depends on both the data and the prior."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#posterior-prediction",
    "href": "bayesian_linear_regression_slides.html#posterior-prediction",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\nGiven a new observation \\(x_{\\text{new}}\\), we can predict using posterior draws:\n\nCompute predicted means for each sample: \\[\n\\hat{y}_{\\text{new}}^{(t)} = x_{\\text{new}}^\\top \\beta^{(t)}\n\\]\nAdd residual uncertainty: \\[\ny_{\\text{new}}^{(t)} \\sim\n\\mathcal{N}\\!\\left(x_{\\text{new}}^\\top \\beta^{(t)},\\; \\sigma^{2(t)}\\right)\n\\]\n\nThe resulting samples \\(\\{y_{\\text{new}}^{(t)}\\}\\) form a posterior predictive distribution, from which we can derive predictive intervals and evaluate predictive accuracy."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#model-checking-and-hypothesis-testing",
    "href": "bayesian_linear_regression_slides.html#model-checking-and-hypothesis-testing",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Model Checking and Hypothesis Testing",
    "text": "Model Checking and Hypothesis Testing\nPosterior samples enable rich model diagnostics and hypothesis testing:\n\nPosterior probability of an event\n\\[\n\\Pr(\\beta_j \\ne 0 \\mid y)\n\\approx \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{1}\\!\\left(\\beta_j^{(t)} \\ne 0\\right)\n\\]\nPosterior predictive checks\nSimulate new datasets using posterior draws and compare them to the observed data to assess model fit.\nModel comparison\nBayes factors and marginal likelihoods can be approximated to formally test or compare competing models.\n\nThese tools extend Bayesian inference beyond estimation to model validation, uncertainty quantification, and decision-making."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#convergence-diagnostics",
    "href": "bayesian_linear_regression_slides.html#convergence-diagnostics",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\nBefore interpreting MCMC results, we must check that the Gibbs sampler has converged to the target posterior distribution.\nConvergence diagnostics assess whether the Markov chain has reached its stationary distribution and is producing valid samples.\nTwo basic strategies are:\n\nBurn-in – Discard early iterations (e.g., first 1000) to remove dependence on starting values.\n\nThinning – Keep every \\(k\\)-th sample to reduce autocorrelation.\n\nThese steps improve sample quality and ensure reliable posterior summaries."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#trace-plots",
    "href": "bayesian_linear_regression_slides.html#trace-plots",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Trace Plots",
    "text": "Trace Plots\nA simple yet powerful diagnostic is the trace plot,\nshowing sampled parameter values \\(\\theta^{(t)}\\) over iterations \\(t\\).\n\nA converged chain fluctuates around a stable mean — no trend or drift.\n\nMultiple chains from different starting points should overlap and mix well.\n\nTrace plots help detect: - Lack of stationarity (upward/downward trends) - Poor mixing or multimodality - Burn-in issues\nVisual inspection is often the first step in assessing convergence."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#autocorrelation",
    "href": "bayesian_linear_regression_slides.html#autocorrelation",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nSamples from a Gibbs sampler are correlated, especially for tightly coupled parameters.\nThe autocorrelation function (ACF) quantifies dependence across lags \\(k\\):\n\\[\n\\hat{\\rho}_k =\n\\frac{\\sum_{t=1}^{T-k} (\\theta^{(t)} - \\bar{\\theta})(\\theta^{(t+k)} - \\bar{\\theta})}\n     {\\sum_{t=1}^{T} (\\theta^{(t)} - \\bar{\\theta})^2}\n\\]\n\nHigh \\(\\hat{\\rho}_k\\) → slow mixing and fewer effective samples\n\nLow \\(\\hat{\\rho}_k\\) → better mixing and faster convergence\n\nReducing autocorrelation may require more iterations,\nreparameterization, or thinning the chain."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#effective-sample-size-ess",
    "href": "bayesian_linear_regression_slides.html#effective-sample-size-ess",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Effective Sample Size (ESS)",
    "text": "Effective Sample Size (ESS)\nAutocorrelation reduces the number of independent samples obtained.\nThe effective sample size (ESS) adjusts for this:\n\\[\n\\text{ESS}(\\theta) =\n\\frac{T}{1 + 2 \\sum_{k=1}^{K} \\hat{\\rho}_k}\n\\]\n\nSmall ESS → chain is highly correlated, less informative\n\nRule of thumb: \\(\\text{ESS} &gt; 100\\) per parameter for stable inference\n\nESS provides a quantitative measure of sampling efficiency\nand helps determine whether more iterations are needed."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#gelmanrubin-diagnostic-hatr",
    "href": "bayesian_linear_regression_slides.html#gelmanrubin-diagnostic-hatr",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Gelman–Rubin Diagnostic (\\(\\hat{R}\\))",
    "text": "Gelman–Rubin Diagnostic (\\(\\hat{R}\\))\nWhen running multiple chains, the Gelman–Rubin statistic compares between-chain and within-chain variability.\nFor \\(m\\) chains with \\(T\\) iterations each:\n\\[\nW = \\frac{1}{m} \\sum_{i=1}^{m} s_i^2, \\quad\nB = \\frac{T}{m-1} \\sum_{i=1}^{m} (\\bar{\\theta}_i - \\bar{\\theta})^2\n\\]\nThe potential scale reduction factor:\n\\[\n\\hat{R} = \\sqrt{ \\frac{\\hat{V}}{W} }, \\quad\n\\hat{V} = \\frac{T-1}{T} W + \\frac{1}{T} B\n\\]\n\n\\(\\hat{R} \\approx 1\\) → convergence achieved\n\n\\(\\hat{R} &gt; 1.1\\) → chains have not converged"
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#geweke-diagnostic",
    "href": "bayesian_linear_regression_slides.html#geweke-diagnostic",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Geweke Diagnostic",
    "text": "Geweke Diagnostic\nThe Geweke test checks whether early and late portions of a single chain have the same mean, indicating stationarity.\n\\[\nZ =\n\\frac{\\bar{\\theta}_A - \\bar{\\theta}_B}\n     {\\sqrt{\\text{Var}(\\bar{\\theta}_A) + \\text{Var}(\\bar{\\theta}_B)}}\n\\]\nTypically:\n\nSegment A = first 10% of the chain\n\nSegment B = last 50% of the chain\n\nUnder convergence, \\(Z \\sim \\mathcal{N}(0,1)\\).\n\n\\(|Z| \\le 2\\) → chain likely stationary\n\n\\(|Z| &gt; 2\\) → potential non-convergence\n\nThese diagnostics ensure that posterior summaries reflect the true target distribution."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#spike-and-slab-bayesian-linear-regression",
    "href": "bayesian_linear_regression_slides.html#spike-and-slab-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Spike-and-Slab Bayesian Linear Regression",
    "text": "Spike-and-Slab Bayesian Linear Regression\nAs in classical BLR, the outcome is modeled as:\n\\[\ny = Xb + e, \\quad e \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n\\]\nwhere \\(y\\) is the \\(n \\times 1\\) response, \\(X\\) the design matrix, \\(b\\) the regression coefficients, and \\(\\sigma^2\\) the residual variance.\nThis defines the likelihood:\n\\[\ny \\mid b, \\sigma^2 \\sim \\mathcal{N}(Xb, \\sigma^2 I_n)\n\\]\nThe goal is to estimate \\(b\\) and identify which predictors truly contribute to \\(y\\)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#motivation-for-the-spike-and-slab-prior",
    "href": "bayesian_linear_regression_slides.html#motivation-for-the-spike-and-slab-prior",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Motivation for the Spike-and-Slab Prior",
    "text": "Motivation for the Spike-and-Slab Prior\nIn standard Bayesian linear regression:\n\\[\n\\beta_j \\sim \\mathcal{N}(0, \\sigma_b^2)\n\\]\nThis Gaussian (shrinkage) prior assumes all predictors have small effects, but it does not allow exact zeros — limiting variable selection.\nThe spike-and-slab prior addresses this by mixing two components:\n\nA spike at zero → excluded predictors\n\nA slab (wide normal) → active predictors\n\nThis yields sparse, interpretable models that select relevant variables."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#the-spike-and-slab-mixture-prior",
    "href": "bayesian_linear_regression_slides.html#the-spike-and-slab-mixture-prior",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "The Spike-and-Slab Mixture Prior",
    "text": "The Spike-and-Slab Mixture Prior\nEach regression effect is drawn from a two-component mixture:\n\\[\np(b_i \\mid \\sigma_b^2, \\pi)\n= \\pi\\, \\mathcal{N}(0, \\sigma_b^2) + (1-\\pi)\\, \\delta_0\n\\]\nwhere:\n\n\\(\\pi\\) = prior probability that \\(b_i\\) is non-zero\n\n\\(\\delta_0\\) = point mass at zero\n\nThus, with probability \\(\\pi\\) a predictor is active (slab), and with probability \\(1-\\pi\\) it is excluded (spike)."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#advantages-of-spike-and-slab-priors",
    "href": "bayesian_linear_regression_slides.html#advantages-of-spike-and-slab-priors",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Advantages of Spike-and-Slab Priors",
    "text": "Advantages of Spike-and-Slab Priors\nThis hierarchical mixture prior provides several benefits:\n\nSparsity — allows exact zeros for irrelevant predictors\n\nInterpretability — binary indicators give posterior inclusion probabilities (PIPs)\n\nAdaptivity — the inclusion probability \\(\\pi\\) is learned from the data\n\nBalance — captures both strong signals (detection) and small effects (prediction)\n\nHence, spike-and-slab models combine variable selection with Bayesian uncertainty quantification."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#hierarchical-representation",
    "href": "bayesian_linear_regression_slides.html#hierarchical-representation",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Hierarchical Representation",
    "text": "Hierarchical Representation\nWe express each effect as:\n\\[\nb_i = \\alpha_i \\, \\delta_i\n\\]\nwhere:\n\\[\n\\alpha_i \\mid \\sigma_b^2 \\sim \\mathcal{N}(0, \\sigma_b^2), \\quad\n\\delta_i \\mid \\pi \\sim \\text{Bernoulli}(\\pi)\n\\]\n\n\\(\\alpha_i\\): effect size when predictor is included\n\n\\(\\delta_i\\): binary inclusion indicator (0 or 1)\n\nMarginalizing over \\(\\delta_i\\) yields the spike-and-slab mixture prior above."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#prior-for-the-inclusion-probability-pi",
    "href": "bayesian_linear_regression_slides.html#prior-for-the-inclusion-probability-pi",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Prior for the Inclusion Probability \\(\\pi\\)",
    "text": "Prior for the Inclusion Probability \\(\\pi\\)\nThe overall sparsity level is controlled by \\(\\pi\\), assigned a Beta prior:\n\\[\n\\pi \\sim \\text{Beta}(\\alpha, \\beta)\n\\]\n\nSmall \\(\\alpha\\), large \\(\\beta\\) → favor sparser models\n\n\\(\\alpha = \\beta = 1\\) → uniform prior\n\nLarger \\(\\alpha\\) → denser models\n\nThis prior lets the data determine the degree of sparsity."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#priors-for-variance-components",
    "href": "bayesian_linear_regression_slides.html#priors-for-variance-components",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Priors for Variance Components",
    "text": "Priors for Variance Components\nVariance parameters use scaled inverse-chi-squared priors:\n\\[\n\\sigma_b^2 \\sim S_b \\chi^{-2}(v_b), \\quad\n\\sigma^2 \\sim S \\chi^{-2}(v)\n\\]\nThese are conjugate, providing closed-form conditional updates. Hyperparameters \\((S_b, v_b)\\) and \\((S, v)\\) encode prior beliefs about effect size variability and residual noise."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#joint-posterior-structure",
    "href": "bayesian_linear_regression_slides.html#joint-posterior-structure",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Joint Posterior Structure",
    "text": "Joint Posterior Structure\nCombining the likelihood and priors, the joint posterior is:\n\\[\np(\\mu, \\alpha, \\delta, \\pi, \\sigma_b^2, \\sigma^2 \\mid y)\n\\propto\np(y \\mid \\mu, \\alpha, \\delta, \\sigma^2)\\,\np(\\alpha \\mid \\sigma_b^2)\\,\np(\\delta \\mid \\pi)\\,\np(\\pi)\\,\np(\\sigma_b^2)\\,\np(\\sigma^2)\n\\]\nThis captures our updated beliefs about effects, inclusion indicators, and variance components."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#gibbs-sampling-for-spike-and-slab-blr",
    "href": "bayesian_linear_regression_slides.html#gibbs-sampling-for-spike-and-slab-blr",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Gibbs Sampling for Spike-and-Slab BLR",
    "text": "Gibbs Sampling for Spike-and-Slab BLR\nInference proceeds via Gibbs sampling, cycling through these conditional updates:\n\n\\(\\alpha \\mid D\\)\n\n\\(\\delta \\mid D\\)\n\n\\(\\pi \\mid D\\)\n\n\\(\\sigma_b^2 \\mid D\\)\n\n\\(\\sigma^2 \\mid D\\)\n\nHere, \\(D\\) denotes the data and all other current parameter values.\nEach conditional follows a standard distribution (Normal, Bernoulli, Beta, scaled-\\(\\chi^{-2}\\)).\nIterating these updates generates samples from the joint posterior."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#posterior-inclusion-probabilities",
    "href": "bayesian_linear_regression_slides.html#posterior-inclusion-probabilities",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Inclusion Probabilities",
    "text": "Posterior Inclusion Probabilities\nThe posterior inclusion probability (PIP) measures how likely each predictor is truly associated with \\(y\\):\n\\[\n\\widehat{\\Pr}(\\delta_i = 1 \\mid y)\n= \\frac{1}{T} \\sum_{t=1}^{T} \\delta_i^{(t)}\n\\]\n\nHigh PIP → predictor is likely important\n\nLow PIP → predictor likely irrelevant\n\nPIPs summarize variable relevance and drive Bayesian feature selection."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#summary-of-bayesian-linear-regression",
    "href": "bayesian_linear_regression_slides.html#summary-of-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Summary of Bayesian Linear Regression",
    "text": "Summary of Bayesian Linear Regression\nBayesian Linear Regression combines likelihood and prior to form the posterior, enabling principled modeling, regularization, and uncertainty quantification.\n\nInference via MCMC (often Gibbs sampling) with posterior draws for means, credible intervals, and predictions.\n\nSpike-and-slab priors enable sparsity and variable selection, assigning exact zeros to irrelevant predictors and identifying key variables via posterior inclusion probabilities (PIPs).\n\nConjugate and mixture priors yield efficient and robust inference, even when \\(p &gt; n\\).\n\nWith proper convergence checks, Bayesian models provide stable and reliable inference across a wide range of data settings."
  },
  {
    "objectID": "bayesian_linear_regression_slides.html#applications-in-genomics-1",
    "href": "bayesian_linear_regression_slides.html#applications-in-genomics-1",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Applications in Genomics",
    "text": "Applications in Genomics\nWe have now seen the basic framework of Bayesian Linear Regression (BLR)\nand will illustrate how it provides a unified approach for analyzing genetic and genomic data.\n\nGenome-Wide Association Studies (GWAS) and fine-mapping of causal variants.\n\nGenetic prediction and heritability estimation.\n\nPathway and gene-set enrichment analyses.\n\nThese examples show how BLR connects statistical modeling with biological interpretation in quantitative genetics."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#overview",
    "href": "bayesian_gene_set_slides.html#overview",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Overview",
    "text": "Overview\n\n\nIntroduction to Bayesian Linear Regression Models used in Gene Set Analyses\n\nGene Set Analyses using Bayesian MAGMA\nIntegrative Genomics Analyses using the gact and qgg R packages"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#bayesian-linear-regression-models",
    "href": "bayesian_gene_set_slides.html#bayesian-linear-regression-models",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Bayesian Linear Regression Models",
    "text": "Bayesian Linear Regression Models\nBayesian Linear Regression Models provide a flexible statistical framework for modeling complex biological and healthcare data. They support key applications such as:\n\nGenome-wide association studies (GWAS) and fine-mapping of causal variants\n\nPolygenic risk scoring (PRS) for predicting complex traits and disease risk\n\nGene and pathway enrichment analyses to test biological hypotheses\n\nIntegrative multi-omics modeling across the genome, transcriptome, epigenome, and proteome\n\nApplications to registry-based healthcare data, enabling population-level risk prediction and disease modeling"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#the-bayesian-linear-regression-model",
    "href": "bayesian_gene_set_slides.html#the-bayesian-linear-regression-model",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "The Bayesian Linear Regression Model",
    "text": "The Bayesian Linear Regression Model\nThe Bayesian Linear Regression (BLR) model builds:\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\qquad\n\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I})\n\\]\n\n\\(\\mathbf{Y}\\) represents the observed outcomes or association measures corresponding to the features in \\(\\mathbf{X}\\).\n\\(\\mathbf{X}\\) represents molecular or genomic predictors (e.g., genotypes, gene scores, annotations, pathway indicators).\n\n\\(\\boldsymbol{\\beta}\\) — effect sizes quantifying how features in \\(\\mathbf{X}\\) explain variation in \\(\\mathbf{Y}\\)\n\n\\(\\boldsymbol{\\varepsilon}\\) — residual noise capturing unexplained variation\n\nIn the Bayesian formulation, each \\(\\beta_j\\) is assigned a prior distribution reflecting beliefs about effect size magnitude or sparsity and determine how information is shared across features or biological layers."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#why-bayesian-linear-regression-models",
    "href": "bayesian_gene_set_slides.html#why-bayesian-linear-regression-models",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Why Bayesian Linear Regression Models?",
    "text": "Why Bayesian Linear Regression Models?\nRegression effects can be estimated in many ways, but we focus on a Bayesian hierarchical framework because it:\n\nCombines data and prior knowledge to improve inference\n\nProvides a natural way to regularize and handle noisy or high-dimensional data\n\nEnables flexible modeling of diverse effect patterns:\n\nMany small vs. few large effects\n\nStructured effects (e.g., by pathway, gene set, or omic layer)\n\n\nReturns uncertainty estimates for all parameters → improving interpretability and model comparison\n\nThrough their hierarchical structure, BLR models naturally integrate multiple biological layers — linking genomic, transcriptomic, and other molecular data"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#hyperpriors-in-bayesian-modeling",
    "href": "bayesian_gene_set_slides.html#hyperpriors-in-bayesian-modeling",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Hyperpriors in Bayesian Modeling",
    "text": "Hyperpriors in Bayesian Modeling\nThe BLR model extends the linear framework by introducing hierarchical priors on parameters. Suppose we model regression coefficients \\(b_j\\) with a normal prior:\n\\[\nb_j \\sim \\mathcal{N}(0, \\sigma_b^2)\n\\]\nHere, \\(\\sigma_b^2\\) (the variance of the prior) controls how large the effects \\(b_j\\) are expected to be.\nInstead of fixing \\(\\sigma_b^2\\) we treat it as unknown and assign it its own prior — the hyperprior:\n\\[\n\\sigma_b^2 \\sim \\text{Inv-}\\chi^2(\\nu, S^2)\n\\]\nIn practice, this parameter is learned from the data during estimation rather than fixed in advance."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#hierarchical-structure",
    "href": "bayesian_gene_set_slides.html#hierarchical-structure",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Hierarchical Structure",
    "text": "Hierarchical Structure\nThe three levels in the model:\n\n\n\n\n\n\n\n\n\nLevel\nDescription\nExample\n\n\n\n\n1\nDescribes how data are generated given parameters\n\\(y \\sim \\mathcal{N}(Xb, \\sigma^2 I)\\)\n\n\n2\nDescribes our beliefs about the parameters before seeing data\n\\(b_i \\sim \\mathcal{N}(0, \\sigma_b^2)\\)\n\n\n3\nDescribes uncertainty about the prior’s parameters\n\\(\\sigma_b^2 \\sim \\text{Inv-}\\chi^2(\\nu, S^2)\\)\n\n\n\n\n\nThis hierarchical structure allows the model to learn how strongly to shrink effect estimates from the data, while accounting for uncertainty in prior parameters and automatically regularizing effect sizes.\nSimple and robust, but may not capture diverse effect-size distributions."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#adapting-to-complex-biological-architectures",
    "href": "bayesian_gene_set_slides.html#adapting-to-complex-biological-architectures",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Adapting to Complex Biological Architectures",
    "text": "Adapting to Complex Biological Architectures\nComplex traits arise from heterogeneous effect-size distributions — some features have large effects, many have small, and others are likely null. To capture this diversity, the BLR framework can be extended in two ways:\n\nData-driven grouping of molecular features:\nThe model learns effect-size classes from the data using a mixture of variances \\(\\{\\tau_k^2\\}\\) with probabilities \\(\\{\\pi_k\\}\\).\nBiologically informed grouping of molecular features:\nThe model uses prior biological knowledge to assign features to groups a priori, each with its own variance \\(\\tau_g^2\\) capturing within-group variability.\n\nBoth approaches enable the model to adapt to complex genetic and molecular architectures and share information across related features or omic layers"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#multiple-component-blr",
    "href": "bayesian_gene_set_slides.html#multiple-component-blr",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Multiple-Component BLR",
    "text": "Multiple-Component BLR\nA more flexible formulation assumes that effects come from a mixture of normal distributions:\n\\[\n\\beta_j \\mid d_j = k \\sim \\mathcal{N}(0, \\tau_k^2), \\qquad\nP(d_j = k) = \\pi_k\n\\]\n\n\\(d_j\\) is a latent indicator variable assigning effect \\(j\\) to component \\(k\\).\n\nEach component \\(k\\) has its own variance \\(\\tau_k^2\\), controlling expected effect size.\n\n\\(\\pi_k\\) represents the probability of membership in each component.\n\nThis structure allows the model to capture both large and small effects, including potential nulls.\nIn practice, both \\(\\pi_k\\) and \\(\\tau_k^2\\) are learned from the data, making the model data-driven and adaptive, though not necessarily biologically informed."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#hierarchical-group-structured-blr",
    "href": "bayesian_gene_set_slides.html#hierarchical-group-structured-blr",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Hierarchical (Group-Structured) BLR",
    "text": "Hierarchical (Group-Structured) BLR\nIn this biologically informed extension, we assign features to predefined groups (e.g., genes, pathways, or protein complexes) a priori and model group-specific variances:\n\\[\n\\beta_j \\sim \\mathcal{N}\\!\\big(0, \\tau_{g(j)}^2\\big), \\qquad\n\\tau_g^2 \\sim p(\\tau_g^2)\n\\]\n\nEach feature \\(j\\) belongs to a group \\(g(j)\\) defined before analysis.\n\nEach group \\(g\\) has its own variance \\(\\tau_g^2\\), controlling effect size variability within that group.\n\nThis approach allows the model to share information among related features and test enrichment across biological groups.\nIn practice, the group-level variances \\(\\tau_g^2\\) are learned from the data, enabling the model to adapt shrinkage across biological structures."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#indicator-variables-and-posterior-inclusion-probabilities",
    "href": "bayesian_gene_set_slides.html#indicator-variables-and-posterior-inclusion-probabilities",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Indicator Variables and Posterior Inclusion Probabilities",
    "text": "Indicator Variables and Posterior Inclusion Probabilities\nIn Bayesian variable selection, each feature \\(j\\) is assigned an indicator variable:\n\\[\n\\delta_j =\n\\begin{cases}\n1, & \\text{if feature $j$ has a non-zero effect} \\\\\n0, & \\text{if feature $j$ has no effect.}\n\\end{cases}\n\\]\nThe model can be written as: \\[\n\\beta_j = \\alpha_j \\, \\delta_j, \\qquad\n\\alpha_j \\sim \\mathcal{N}(0, \\tau^2), \\quad\n\\delta_j \\sim \\text{Bernoulli}(\\pi)\n\\]\nwhere \\(\\pi\\) is the prior inclusion probability.\nAfter inference, we estimate \\(\\text{PIP}_j = P(\\delta_j = 1 \\mid \\text{data})\\) — the posterior inclusion probability for feature j."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#understanding-the-outputs-of-bayesian-linear-regression-blr",
    "href": "bayesian_gene_set_slides.html#understanding-the-outputs-of-bayesian-linear-regression-blr",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Understanding the Outputs of Bayesian Linear Regression (BLR)",
    "text": "Understanding the Outputs of Bayesian Linear Regression (BLR)\nThe BLR model produces posterior summaries that describe the evidence, magnitude, and uncertainty of feature effects.\nKey outputs:\n\nPosterior means of \\(\\boldsymbol{\\beta}\\) (effect sizes)\n\nPosterior inclusion probabilities (PIPs)\n\nVariance component estimates (\\(\\sigma^2\\), \\(\\tau^2\\), …)\n\nEach plays a distinct and complementary role in interpretation."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#posterior-means-of-effect-sizes-boldsymbolbeta",
    "href": "bayesian_gene_set_slides.html#posterior-means-of-effect-sizes-boldsymbolbeta",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Posterior Means of Effect Sizes (\\(\\boldsymbol{\\beta}\\))",
    "text": "Posterior Means of Effect Sizes (\\(\\boldsymbol{\\beta}\\))\n\nRepresent the estimated effect of each feature (gene, SNP, or set)\n\nComputed as posterior averages:\n\\[\n\\hat{\\beta}_j = \\mathbb{E}[\\beta_j \\mid \\mathbf{Y}, \\mathbf{X}]\n\\]\nInterpretation:\n\nMagnitude → direction and strength of association\n\nSign → positive or negative effect\n\nShrinkage → smaller estimates for weak or uncertain effects\n\nThese are directly analogous to regression coefficients, but account for prior information and uncertainty."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#posterior-inclusion-probabilities-pips",
    "href": "bayesian_gene_set_slides.html#posterior-inclusion-probabilities-pips",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Posterior Inclusion Probabilities (PIPs)",
    "text": "Posterior Inclusion Probabilities (PIPs)\n\nRepresent the probability that feature \\(j\\) has a nonzero effect: \\[\n\\text{PIP}_j = \\Pr(\\beta_j \\neq 0 \\mid \\mathbf{Y}, \\mathbf{X})\n\\]\nQuantifies evidence of inclusion in the model:\n\nHigh PIP → strong support for inclusion\n\nLow PIP → likely irrelevant or redundant feature\n\nInterpretation:\n\nPIPs act as Bayesian significance measures\n\nUseful for ranking, fine-mapping, and feature prioritization\n\n\nIn pathway analyses (e.g., Bayesian MAGMA), PIPs correspond to gene- or set-level importance scores."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#variance-component-estimates-tau2-sigma2",
    "href": "bayesian_gene_set_slides.html#variance-component-estimates-tau2-sigma2",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Variance Component Estimates (\\(\\tau^2\\), \\(\\sigma^2\\))",
    "text": "Variance Component Estimates (\\(\\tau^2\\), \\(\\sigma^2\\))\nVariance components describe uncertainty and heterogeneity in effect sizes:\n\\[\n\\beta_j \\sim \\mathcal{N}(0, \\tau^2)\n\\qquad \\text{and} \\qquad\n\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\n\\(\\tau^2\\) is the variance component for the effect sizes\n- Small \\(\\tau^2\\): most effects are close to zero\n- Large \\(\\tau^2\\): more large-effect features expected\n\\(\\sigma^2\\) is the variance component for the residuals\n- Captures unexplained variation after accounting for \\(\\mathbf{X}\\)\nTogether, these components describe the genetic architecture, enabling heritability estimation, uncertainty quantification, and enrichment analysis across sets or traits."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#interpreting-the-outputs-together",
    "href": "bayesian_gene_set_slides.html#interpreting-the-outputs-together",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Interpreting the Outputs Together",
    "text": "Interpreting the Outputs Together\n\n\n\n\n\n\n\n\nQuantity\nInterpretation\nTypical use\n\n\n\n\n\\(\\hat{\\beta}_j\\)\nDirection and magnitude of effect\nEffect estimation, prediction\n\n\n\\(\\text{PIP}_j\\)\nProbability feature is truly associated\nFine-mapping, feature ranking\n\n\n\\(\\tau^2\\), \\(\\sigma^2\\)\nVariance in effect sizes and residuals\nGenetic architecture, model fit\n\n\n\nThese summaries are synergistic:\n\n\\(\\boldsymbol{\\beta}\\) tells how much\n\nPIP tells how confident\n\nVariance components tell how complex"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#summary-of-blr-model-structures",
    "href": "bayesian_gene_set_slides.html#summary-of-blr-model-structures",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Summary of BLR Model Structures",
    "text": "Summary of BLR Model Structures\n\n\n\n\n\n\n\n\n\nModel Type\nPrior Structure\nBiological Interpretation\n\n\n\n\nSingle-component BLR\nOne global variance \\(\\tau^2\\)\nAll features (across layers) share the same level of shrinkage — equal contribution assumption\n\n\nMultiple-component BLR\nMixture of variances \\(\\{\\tau_k^2\\}\\)\nFeatures belong to different effect-size classes (e.g., large, small, null); grouping learned from data\n\n\nHierarchical (Biologically informed) BLR\nGroup-specific mixtures of variances \\(\\{\\tau_{gk}^2\\}\\)\nFeatures grouped a priori (e.g., by genes, pathways, or omic layers); within each group, effects can vary in size and sparsity\n\n\n\n\nThese models form a hierarchy of increasing flexibility and biological realism —\nfrom global shrinkage → to data-driven heterogeneity → to biologically structured mixtures that model variation within and between feature sets."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#motivation-for-multivariate-blr",
    "href": "bayesian_gene_set_slides.html#motivation-for-multivariate-blr",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Motivation for Multivariate BLR",
    "text": "Motivation for Multivariate BLR\nMany traits and molecular layers are correlated — they share genetic architecture and biological pathways.\n\nTo model these dependencies, we extend BLR to the multivariate setting:\n\nJointly models multiple traits or omic layers\n→ captures shared genetic or molecular effects\n\nBorrows strength across correlated traits\n→ improves fine-mapping resolution and prediction accuracy\n\nEstimates cross-trait effect patterns\n→ helps identify pleiotropic genes and shared biological pathways"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#the-multivariate-bayesian-linear-regression-mv-blr-model",
    "href": "bayesian_gene_set_slides.html#the-multivariate-bayesian-linear-regression-mv-blr-model",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "The Multivariate Bayesian Linear Regression (MV-BLR) Model",
    "text": "The Multivariate Bayesian Linear Regression (MV-BLR) Model\nIn the multivariate BLR model, we model multiple correlated outcomes jointly:\n\\[\n\\mathbf{Y} = \\mathbf{X}\\mathbf{B} + \\mathbf{E}\n\\]\n\n\\(\\mathbf{Y}\\): \\((n \\times T)\\) matrix of outcomes\n(e.g., association measures for \\(T\\) traits or omic layers)\n\n\\(\\mathbf{X}\\): \\((n \\times p)\\) feature matrix\n\n\\(\\mathbf{B}\\): \\((p \\times T)\\) matrix of effect sizes\n\n\\(\\mathbf{E}\\): \\((n \\times T)\\) residual matrix\n\nEach row of \\(\\mathbf{Y}\\) corresponds to an observation or gene, and each column to a trait, phenotype, or molecular layer."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#multivariate-error-and-effect-priors",
    "href": "bayesian_gene_set_slides.html#multivariate-error-and-effect-priors",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Multivariate Error and Effect Priors",
    "text": "Multivariate Error and Effect Priors\nWe extend the univariate priors to the multivariate setting:\n\\[\n\\mathbf{e}_{i\\cdot} \\sim \\mathcal{N}_T(\\mathbf{0}, \\boldsymbol{\\Sigma}_e)\n\\] \\[\n\\mathbf{b}_j \\sim \\mathcal{N}_T(\\mathbf{0}, \\boldsymbol{\\Sigma}_b)\n\\]\n\n\\(\\boldsymbol{\\Sigma}_e\\): residual covariance among traits\n\n\\(\\boldsymbol{\\Sigma}_b\\): covariance of effect sizes across traits\n\nWhen \\(\\boldsymbol{\\Sigma}_e\\) and \\(\\boldsymbol{\\Sigma}_b\\) are diagonal, the model reduces to \\(T\\) independent univariate BLR models.\n\nAllows information sharing across correlated traits or omic layers and can be used to identify pleiotropic effects and cross-trait genetic architectures."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#multivariate-blr-structured-mv-blr",
    "href": "bayesian_gene_set_slides.html#multivariate-blr-structured-mv-blr",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Multivariate BLR (Structured MV-BLR)",
    "text": "Multivariate BLR (Structured MV-BLR)\nThe hierarchical structure can be extended to model multiple traits\nwhile preserving biological grouping of features:\n\\[\n\\mathbf{b}_j \\sim \\mathcal{N}_T\\!\\big(\\mathbf{0}, \\boldsymbol{\\Sigma}_{b, g(j)}\\big), \\qquad\n\\boldsymbol{\\Sigma}_{b, g} \\sim p(\\boldsymbol{\\Sigma}_{b, g})\n\\]\n\nEach biological group \\(g\\) has its own trait-level covariance matrix \\(\\boldsymbol{\\Sigma}_{b,g}\\)\n\n\\(\\boldsymbol{\\Sigma}_{b,g}\\) captures correlations and scale of effects across traits within that group\n\nEnables information sharing both within biological sets and across correlated traits."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#indicator-variables-and-pips-multivariate-blr",
    "href": "bayesian_gene_set_slides.html#indicator-variables-and-pips-multivariate-blr",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Indicator Variables and PIPs (Multivariate BLR)",
    "text": "Indicator Variables and PIPs (Multivariate BLR)\nIn the multivariate BLR, each feature \\(j\\) may affect multiple outcomes (traits).\nWe extend the indicator variable to capture cross-trait activity patterns:\n\\[\n\\boldsymbol{\\delta}_j =\n\\begin{bmatrix}\n\\delta_{j1} \\\\ \\delta_{j2} \\\\ \\vdots \\\\ \\delta_{jT}\n\\end{bmatrix},\n\\qquad\n\\delta_{jt} =\n\\begin{cases}\n1, & \\text{if feature $j$ affects trait $t$} \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n\\]\nAfter inference, we estimate \\(\\text{PIP}_{jt} = P(\\delta_{jt} = 1 \\mid \\text{data})\\) — the posterior inclusion probability that feature j affects trait t."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#multivariate-blr-outputs",
    "href": "bayesian_gene_set_slides.html#multivariate-blr-outputs",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Multivariate BLR Outputs",
    "text": "Multivariate BLR Outputs\nIn the multivariate setting, we generalize each posterior quantity:\n\n\n\n\n\n\n\nParameter\nInterpretation\n\n\n\n\n\\(\\mathbf{B} = [\\beta_{jt}]\\)\nEffect matrix across traits (\\(j\\): feature, \\(t\\): trait)\n\n\n\\(\\text{PIP}_j\\)\nProbability that feature \\(j\\) affects ≥1 trait\n\n\n\\(\\boldsymbol{\\Sigma}_b\\)\nCovariance of effects across traits\n\n\n\\(\\boldsymbol{\\Sigma}_e\\)\nResidual covariance among traits\n\n\n\nThese allow us to identify:\n\nShared genetic effects (pleiotropy)\n\nTrait-specific vs. shared signals\n\nCross-trait enrichment of biological sets"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#overview-of-blr-models-used-in-gene-set-analyses",
    "href": "bayesian_gene_set_slides.html#overview-of-blr-models-used-in-gene-set-analyses",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Overview of BLR Models used in Gene Set Analyses",
    "text": "Overview of BLR Models used in Gene Set Analyses\n\n\n\n\n\n\n\n\n\n\n\nModel Type\nFeature Integration\nGrouping Basis\nPrior Structure\nWhat It Captures\n\n\n\n\nSingle-component BLR\nCombines all biological features in one model\nNone\nOne global variance (\\(\\tau^2\\))\nAll features contribute equally; uniform shrinkage\n\n\nMultiple-component BLR\nIntegrates all layers but allows heterogeneous contributions\nLearned from data\nMixture of variances (\\(\\{\\tau_k^2\\}\\))\nLarge, small, and null effect classes\n\n\nHierarchical BLR\nGroups features by biological structure (e.g., genes, pathways)\nDefined a priori\nGroup-specific mixture of variances (\\(\\{\\tau_{gk}^2\\}\\))\nWithin-group heterogeneity; enrichment and structured shrinkage\n\n\nMultivariate BLR\nJointly models multiple correlated traits or outcomes\nNone or by trait\nShared covariance (\\(\\boldsymbol{\\Sigma}_b\\)) across traits\nGenetic/molecular correlations; pleiotropy\n\n\nHierarchical MV-BLR\nCombines biological grouping and multiple outcomes\nDefined a priori\nGroup- and trait-specific covariance mixtures (\\(\\{\\boldsymbol{\\Sigma}_{b,gk}\\}\\))\nShared biological mechanisms across traits and layers"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#learning-at-different-levels",
    "href": "bayesian_gene_set_slides.html#learning-at-different-levels",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Learning at Different Levels",
    "text": "Learning at Different Levels\n\n\n\n\n\n\n\n\n\n\n\nModel Level\nKey Parameters Learned\nWhat They Represent\nHow They Are Learned\nWhat We Learn Biologically\n\n\n\n\nEffect sizes\n\\(\\boldsymbol{\\beta}\\)\nStrength and direction of association for each feature\nPosterior mean/median given priors and data\nWhich features drive the outcome\n\n\nIndicator variables\n\\(\\delta_j\\) (single trait), \\(\\boldsymbol{\\delta}_j\\) (multi-trait)\nWhether feature \\(j\\) is active (and for which traits)\nEstimated as posterior inclusion probabilities (PIPs)\nWhich features are relevant, and whether effects are shared or trait-specific\n\n\nVariance components\n\\(\\tau^2\\), \\(\\{\\tau_k^2\\}\\), \\(\\{\\tau_{gk}^2\\}\\)\nMagnitude of expected effect sizes; heterogeneity across layers or groups\nInferred hierarchically from the data (via MCMC or EM)\nHow strongly different groups or omic layers contribute\n\n\nCovariance components\n\\(\\boldsymbol{\\Sigma}_b\\), \\(\\{\\boldsymbol{\\Sigma}_{b,g}\\}\\)\nCorrelation of effects across traits or molecular layers\nEstimated from joint posterior\nShared pathways, pleiotropy, and cross-layer architecture"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#estimating-variance-components-during-model-fitting",
    "href": "bayesian_gene_set_slides.html#estimating-variance-components-during-model-fitting",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Estimating Variance Components During Model Fitting",
    "text": "Estimating Variance Components During Model Fitting\nIn Bayesian Linear Regression, the variance components are not fixed — they are estimated jointly with the effect sizes \\(\\boldsymbol{\\beta}\\).\n\nThe model defines hierarchical priors: \\[\n\\beta_j \\sim \\mathcal{N}(0, \\tau^2)\n\\qquad \\text{and} \\qquad\n\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\] where \\(\\tau^2\\) and \\(\\sigma^2\\) are unknown parameters.\nThese variances are updated iteratively during MCMC or EM optimization:\n\n\\(\\tau^2\\) reflects the inferred spread of true effects\n\n\\(\\sigma^2\\) reflects residual noise or unexplained variability\n\n\nInference alternates between sampling (or updating) \\(\\boldsymbol{\\beta}\\) and re-estimating the variance components given the data and current effects."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#hierarchical-structure-full-bayesian-estimation",
    "href": "bayesian_gene_set_slides.html#hierarchical-structure-full-bayesian-estimation",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Hierarchical Structure: Full Bayesian Estimation",
    "text": "Hierarchical Structure: Full Bayesian Estimation\nEach variance component has its own prior, enabling uncertainty propagation:\n\\[\n\\begin{aligned}\n\\mathbf{Y} &= \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, & \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0, \\sigma^2\\mathbf{I}) \\\\\n\\beta_j &\\sim \\mathcal{N}(0, \\tau^2) & \\tau^2 \\sim \\text{Inv-}\\chi^2(\\nu_\\tau, S_\\tau^2) \\\\\n\\sigma^2 &\\sim \\text{Inv-}\\chi^2(\\nu_\\sigma, S_\\sigma^2)\n\\end{aligned}\n\\]\n\nThe model integrates over the uncertainty in \\(\\tau^2\\) and \\(\\sigma^2\\),\nnot just point estimates\n\nPosterior samples of these parameters describe:\n\nThe degree of polygenicity (via \\(\\tau^2\\))\n\nThe signal-to-noise ratio (via \\(\\tau^2 / \\sigma^2\\))\n\n\nThis full hierarchical treatment improves stability and interpretability compared to fixing variance components a priori."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#in-grouped-or-multi-component-blr-models",
    "href": "bayesian_gene_set_slides.html#in-grouped-or-multi-component-blr-models",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "In Grouped or Multi-Component BLR Models",
    "text": "In Grouped or Multi-Component BLR Models\nVariance components can be set-specific or mixture-specific,\nand are all estimated from the data:\n\\[\n\\beta_j \\sim \\mathcal{N}\\!\\big(0, \\tau_{g(j)}^2\\big), \\qquad\n\\tau_g^2 \\sim p(\\tau_g^2)\n\\]\nor\n\\[\n\\beta_j \\sim \\sum_{k=1}^{K} \\pi_k\\,\\mathcal{N}(0, \\tau_k^2), \\qquad\n\\tau_k^2 \\sim p(\\tau_k^2)\n\\]\n\nEach \\(\\tau_g^2\\) (or \\(\\tau_k^2\\)) is estimated adaptively\n\nGroups or components with strong evidence receive larger \\(\\tau^2\\)\n(less shrinkage, more signal)\n\nNoisy or irrelevant groups shrink toward smaller \\(\\tau^2\\)\n\nVariance components act as adaptive shrinkage parameters, controlling model complexity based on the data."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#in-the-multivariate-blr-model",
    "href": "bayesian_gene_set_slides.html#in-the-multivariate-blr-model",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "In the Multivariate BLR Model",
    "text": "In the Multivariate BLR Model\nVariance components generalize to covariance matrices:\n\\[\n\\mathbf{b}_j \\sim \\mathcal{N}_T(\\mathbf{0}, \\boldsymbol{\\Sigma}_b), \\qquad\n\\mathbf{E}_{i\\cdot} \\sim \\mathcal{N}_T(\\mathbf{0}, \\boldsymbol{\\Sigma}_e)\n\\]\n\n\\(\\boldsymbol{\\Sigma}_b\\): covariance of effects across traits\n→ estimated from shared signal among outcomes\n\n\\(\\boldsymbol{\\Sigma}_e\\): residual covariance among traits\n→ estimated from correlated noise or shared environment\n\nEstimating \\(\\boldsymbol{\\Sigma}_b\\) and \\(\\boldsymbol{\\Sigma}_e\\) enables discovery of pleiotropy and cross-trait genetic structure."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#why-variance-component-estimation-matters",
    "href": "bayesian_gene_set_slides.html#why-variance-component-estimation-matters",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Why Variance Component Estimation Matters",
    "text": "Why Variance Component Estimation Matters\n\n\n\n\n\n\n\n\n\nParameter\nRole\nInterpretation\n\n\n\n\n\\(\\tau^2\\)\nEffect-size variance\nHow much true genetic signal exists\n\n\n\\(\\sigma^2\\)\nResidual variance\nHow much variation remains unexplained\n\n\n\\(\\tau_g^2\\) / \\(\\tau_k^2\\)\nGroup or component variance\nWhich sets/components are enriched\n\n\n\\(\\boldsymbol{\\Sigma}_b\\)\nCross-trait covariance\nPleiotropy or shared mechanisms\n\n\n\n\nThese variance components are not tuning parameters — they are learned quantities that describe the underlying biology. Their estimation is central to the interpretability of BLR."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#summary-variance-components-as-model-driven-insights",
    "href": "bayesian_gene_set_slides.html#summary-variance-components-as-model-driven-insights",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Summary: Variance Components as Model-Driven Insights",
    "text": "Summary: Variance Components as Model-Driven Insights\n\nEstimated jointly with effect sizes and inclusion probabilities\n\nControl the degree of shrinkage and complexity in the model\n\nReveal biological structure (e.g., gene-set enrichments, trait correlations)\n\nProvide model-based evidence for:\n\nPolygenicity\n\nPleiotropy\n\nBiological pathway relevance\n\n\nIn short, variance components are learned descriptors of architecture, not just technical parameters and are a cornerstone of the BLR framework."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#references",
    "href": "bayesian_gene_set_slides.html#references",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "References",
    "text": "References\n\nSørensen P, Rohde PD. A Versatile Data Repository for GWAS Summary Statistics-Based Downstream Genomic Analysis of Human Complex Traits.\nmedRxiv (2025). https://doi.org/10.1101/2025.10.01.25337099\nSørensen IF, Sørensen P. Privacy-Preserving Multivariate Bayesian Regression Models for Overcoming Data Sharing Barriers in Health and Genomics.\nmedRxiv (2025). https://doi.org/10.1101/2025.07.30.25332448\nHjelholt AJ, Gholipourshahraki T, Bai Z, Shrestha M, Kjølby M, Sørensen P, Rohde P. Leveraging Genetic Correlations to Prioritize Drug Groups for Repurposing in Type 2 Diabetes. medRxiv (2025). https://doi.org/10.1101/2025.06.13.25329590\nGholipourshahraki T, Bai Z, Shrestha M, Hjelholt A, Rohde P, Fuglsang MK, Sørensen P. Evaluation of Bayesian Linear Regression Models for Gene Set Prioritization in Complex Diseases. PLOS Genetics 20(11): e1011463 (2025). https://doi.org/10.1371/journal.pgen.1011463\nBai Z, Gholipourshahraki T, Shrestha M, Hjelholt A, Rohde P, Fuglsang MK, Sørensen P. Evaluation of Bayesian Linear Regression Derived Gene Set Test Methods. BMC Genomics 25(1): 1236 (2024). https://doi.org/10.1186/s12864-024-11026-2\nShrestha M, Bai Z, Gholipourshahraki T, Hjelholt A, Rohde P, Fuglsang MK, Sørensen P. Enhanced Genetic Fine Mapping Accuracy with Bayesian Linear Regression Models in Diverse Genetic Architectures. PLOS Genetics 21(7): e1011783 (2025). https://doi.org/10.1371/journal.pgen.1011783\nKunkel D, Sørensen P, Shankar V, Morgante F. Improving Polygenic Prediction from Summary Data by Learning Patterns of Effect Sharing Across Multiple Phenotypes. PLOS Genetics 21(1): e1011519 (2025). https://doi.org/10.1371/journal.pgen.1011519\nRohde P, Sørensen IF, Sørensen P. Expanded Utility of the R Package qgg with Applications within Genomic Medicine. Bioinformatics 39:11 (2023). https://doi.org/10.1093/bioinformatics/btad656\nRohde P, Sørensen IF, Sørensen P. qgg: An R Package for Large-Scale Quantitative Genetic Analyses. Bioinformatics 36(8): 2614–2615 (2020). https://doi.org/10.1093/bioinformatics/btz955"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#gene-set-analyses",
    "href": "bayesian_gene_set_slides.html#gene-set-analyses",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Gene Set Analyses",
    "text": "Gene Set Analyses\nGene and biological pathway prioritization can provide valuable insights into the underlying biology of diseases and potential drug targets.\n\nMAGMA: Multi-marker Analysis of GenoMic Annotation (Leuww et al 2015) generalized gene set analysis of GWAS data\n\nCompute gene-level association statistics (\\(y\\))\nCreate a design matrix based on annotation (\\(X\\))\nFits single or multiple regression models (\\(y=Xb\\))\n\nIdentifies associated features using standard procedures (e.g., t-tests)"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#gene-level-association-statistics",
    "href": "bayesian_gene_set_slides.html#gene-level-association-statistics",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Gene-level Association Statistics",
    "text": "Gene-level Association Statistics\n\n\n\nCompute gene-level (or other feature-level) association statistics:\n\nAccount for correlation among marker statistics (i.e., linkage disequilibrium, LD)\nDifferent LD-adjustment methods (e.g., SVD, clumping and thresholding, BLR)\nThe choice of method depends on the quality of the available GWAS summary statistics and LD reference panel\n\n\nBai et al., 2025"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#bayesian-magma",
    "href": "bayesian_gene_set_slides.html#bayesian-magma",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Bayesian MAGMA",
    "text": "Bayesian MAGMA\n\n\n\n\nFits a Bayesian regression model that allows regularization and variable selection\n\nSupports single- or multi-trait analyses\n\nIdentifies associated features based on posterior inclusion probabilities (PIPs) for the regression effects\n\n\nGholipourshahraki et al., 2024"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#bayesian-magma-kegg-pathway",
    "href": "bayesian_gene_set_slides.html#bayesian-magma-kegg-pathway",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Bayesian MAGMA – KEGG Pathway",
    "text": "Bayesian MAGMA – KEGG Pathway\n\n\n\nGWAS summary statistics from nine studies (T2D, CAD, CKD, HTN, BMI, WHR, Hb1Ac, TG, SBP)\nGene sets are defined by genes linked to KEGG pathways.\n\nPosterior inclusion probabilities (PIPs) quantify the degree of association between gene set and diseases\n\nPathways relevant to diabetes are associated with Type 2 Diabetes (T2D) and correlated traits\n\nEnables identification of cross-disease patterns to better understand comorbidities\n\n\n\n\n\n\n\nGholipourshahraki et al., 2024"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#bayesian-magma-dgidb",
    "href": "bayesian_gene_set_slides.html#bayesian-magma-dgidb",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Bayesian MAGMA – DGIdb",
    "text": "Bayesian MAGMA – DGIdb\n\n\n\nGene sets are defined by genes linked to the Anatomical Therapeutic Chemical (ATC) classification system using the Drug–Gene Interaction Database (DGIdb)\n\nDrug gene sets relevant to diabetes show associations with Type 2 Diabetes (T2D) and related traits\n\nNovel drug–gene set associations may reveal opportunities for drug repurposing\n\n\n\n\n\n\n\nHjelholt et al., 2025"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#bayesian-magma-across-ancestries",
    "href": "bayesian_gene_set_slides.html#bayesian-magma-across-ancestries",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Bayesian MAGMA – Across Ancestries",
    "text": "Bayesian MAGMA – Across Ancestries\n\n\n\nGene sets are defined by genes linked to KEGG pathways.\n\nJoint analysis of T2D across three ancestries (EUR, EAS, SAS).\n\nPathways relevant to diabetes show associations with Type 2 Diabetes (T2D) across two of the ancestries (EUR and EAS).\n\nComparing these associations helps reveal ancestry-specific biological mechanisms."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#why-integrate-diverse-data-sources",
    "href": "bayesian_gene_set_slides.html#why-integrate-diverse-data-sources",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Why Integrate Diverse Data Sources?",
    "text": "Why Integrate Diverse Data Sources?\n\nComplex traits arise from interacting molecular layers — genetic, transcriptomic, proteomic, and metabolic.\n\nSingle data types provide only partial insights complex biological systems (GWAS: DNA → disease phenotype)\n\nIntegration connects variants → genes → pathways → phenotypes may help reveal molecular mechanisms that drive traits and diseases.\nEnables functional interpretation, better prediction, and new discoveries across molecular systems."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#from-data-integration-to-discovery",
    "href": "bayesian_gene_set_slides.html#from-data-integration-to-discovery",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "From Data Integration to Discovery",
    "text": "From Data Integration to Discovery"
  },
  {
    "objectID": "bayesian_gene_set_slides.html#the-gact-r-package",
    "href": "bayesian_gene_set_slides.html#the-gact-r-package",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "The gact R Package",
    "text": "The gact R Package\ngact provides an infrastructure for efficient processing of large-scale genomic association data, with core functions for:\n\nEstablishing and populating a database of genomic associations\n\nDownloading and processing biological databases\n\nHandling and processing GWAS summary statistics\n\nLinking genetic markers to genes, proteins, metabolites, and biological pathways\n\nIntegrates with statistical machine learning tools in the qgg R package\n\ngact is intended to serve as a practical implementation of integrative genomics, bridging statistical modeling and biological interpretation, and supporting reproducible and extensible workflows."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#integrating-data-with-gact",
    "href": "bayesian_gene_set_slides.html#integrating-data-with-gact",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Integrating Data with gact",
    "text": "Integrating Data with gact\nThe gact() function is a single R command that creates and populates the Genomic Association of Complex Traits (GACT) database.\n\nIt automates three main tasks:\n\nInfrastructure creation – sets up a structured folder-based database\n(glist, gstat, gsets, marker, gtex, download, etc.)\nData acquisition – downloads and organizes multiple biological data sources\n(e.g., GWAS Catalog, Ensembl, GTEx, Reactome, STRING, STITCH, DGIdb)\nMarker and feature set generation - integrates data across sources to create curated genomic feature sets that form the basis for the integrative genomic analyses."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#biological-databases-used-by-gact",
    "href": "bayesian_gene_set_slides.html#biological-databases-used-by-gact",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Biological Databases Used by gact",
    "text": "Biological Databases Used by gact\ngact constructs gene and marker sets from a wide range of curated biological databases:\n\n\nEnsembl — genes, transcripts, and proteins\n\nEnsembl Regulation — regulatory genomic features\n\nGO, Reactome, KEGG — ontology and pathway sets\n\nSTRING, STITCH — protein and chemical complexes\n\nDrugBank, ATC — drug–gene and drug-class associations\n\nDISEASE — disease–gene associations\n\nGTEx — eQTL-based gene sets\n\nGWAS Catalog — trait-associated variants and genes\n\nVEP — functional variant annotations\n\nWe plan to add additional biological resources in gact."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#from-database-to-model-inputs",
    "href": "bayesian_gene_set_slides.html#from-database-to-model-inputs",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "From Database to Model Inputs",
    "text": "From Database to Model Inputs\nThe gact R package includes utility functions to extract and structure data from the GACT database into analysis-ready inputs — \\(\\mathbf{Y}\\) (e.g., summary statistic outcomes) and \\(\\mathbf{X}\\) (genomic or biological features).\n\ngetMarkerStat() — retrieve GWAS summary statistics (Y’s)\n\ngetFeatureStat() — extract gene-, protein-, or pathway-level results (Y’s)\n\ngetMarkerSets() — define biological groupings (basis for X’s)\n\ndesignMatrix() — build feature matrices (X) linking variants or genes to biological feature sets\n\nTogether, these functions form a reproducible workflow for generating standardized input data for Bayesian Hierarchical Models and other machine learning approaches."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#the-qgg-r-package",
    "href": "bayesian_gene_set_slides.html#the-qgg-r-package",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "The qgg R Package",
    "text": "The qgg R Package\nqgg provides tools for statistical modeling and analysis of large-scale genomic data, including:\n\nFine-mapping of genomic regions using Bayesian Linear Regression (BLR) models\n\nPolygenic scoring using Bayesian Linear Regression (BLR) models\n\nGene set enrichment analysis using Bayesian Linear Regression (BLR) models\n\nqgg handles large-scale genomic data through efficient algorithms and sparse matrix techniques, combined with multi-core processing using OpenMP, multithreaded matrix operations via BLAS libraries (e.g., OpenBLAS, ATLAS, or MKL), and fast, memory-efficient batch processing of genotype data stored in\nbinary formats such as PLINK .bed files."
  },
  {
    "objectID": "bayesian_gene_set_slides.html#tutorials-using-the-qgg-and-gact-r-packages",
    "href": "bayesian_gene_set_slides.html#tutorials-using-the-qgg-and-gact-r-packages",
    "title": "Gene Set Analyses using Bayesian Linear Regression Models",
    "section": "Tutorials using the qgg and gact R packages",
    "text": "Tutorials using the qgg and gact R packages\n\n\nGene analysis using VEGAS: Gene analysis using the VEGAS (Versatile Gene-based Association Study) approach using the 1000G LD reference data processed above,\nGene set analysis using Bayesian MAGMA: Pathway prioritization using a single and multiple trait Bayesian MAGMA models and gene-level statistics derived from VEGAS (Gholipourshahraki et al.2024).\nGene ranking using PoPS: Polygenic Prioritization Scoring (PoPS) using BLR models and gene-level statistics derived from VEGAS (work in progress).\nFinemapping using BLR models: Finemapping of gene and LD regions using single trait Bayesian Linear Regression models (Shrestha et al.2025).\nPolygenic scoring using BLR models: Polygenic scoring (PGS) using Bayesian Linear Regression models and biological pathway information (work in progress).\nPolygenic scoring using PGS Catalog: Polygenic scoring (PGS) using summary statistics from PGS catalog and biological pathway information.\nLD score regression: LD score regression for estimating genomic heritability and correlations."
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html",
    "href": "bayesian_linear_regression_conjugate.html",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "",
    "text": "This document demonstrates Bayesian Linear Regression using conjugate Gaussian and scaled inverse-chi-squared priors. The model and priors follow the theory outlined in the notes above. We implement a Gibbs sampler for inference, summarize posterior distributions, and assess convergence diagnostics."
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#introduction",
    "href": "bayesian_linear_regression_conjugate.html#introduction",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "",
    "text": "This document demonstrates Bayesian Linear Regression using conjugate Gaussian and scaled inverse-chi-squared priors. The model and priors follow the theory outlined in the notes above. We implement a Gibbs sampler for inference, summarize posterior distributions, and assess convergence diagnostics."
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#model-setup-and-data-simulation",
    "href": "bayesian_linear_regression_conjugate.html#model-setup-and-data-simulation",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Model Setup and Data Simulation",
    "text": "Model Setup and Data Simulation\n\n\nCode\nset.seed(123)\n\n# Simulate data (same as in the classical regression example)\nn &lt;- 100\np &lt;- 3\nX &lt;- cbind(1, matrix(rnorm(n * p), n, p))\nbeta_true &lt;- c(2, 0.5, -1, 1.5)\nsigma_true &lt;- 1\n\ny &lt;- X %*% beta_true + rnorm(n, 0, sigma_true)"
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#gibbs-sampler-implementation",
    "href": "bayesian_linear_regression_conjugate.html#gibbs-sampler-implementation",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Gibbs Sampler Implementation",
    "text": "Gibbs Sampler Implementation\nWe use conjugate priors:\n\n\\(\\beta \\mid \\sigma^2, \\sigma_b^2 \\sim \\mathcal{N}(0, \\sigma_b^2 I_p)\\)\n\\(\\sigma_b^2 \\sim S_b \\chi^{-2}(v_b)\\)\n\\(\\sigma^2 \\sim S \\chi^{-2}(v)\\)\n\n\n\nCode\n# Hyperparameters\nv_b &lt;- 4\nS_b &lt;- 1\nv &lt;- 4\nS &lt;- 1\n\n# Initialization\nbeta &lt;- rep(0, ncol(X))\nsigma2 &lt;- 1\nsigma2_b &lt;- 1\n\nn_iter &lt;- 5000\nburn_in &lt;- 1000\n\n# Store samples\nbeta_samples &lt;- matrix(NA, n_iter, ncol(X))\nsigma2_samples &lt;- numeric(n_iter)\nsigma2_b_samples &lt;- numeric(n_iter)\n\nfor (t in 1:n_iter) {\n  # Sample beta | sigma2, sigma2_b, y\n  Sigma_beta &lt;- solve(t(X) %*% X / sigma2 + diag(1 / sigma2_b, ncol(X)))\n  mu_beta &lt;- Sigma_beta %*% t(X) %*% y / sigma2\n  beta &lt;- as.numeric(mu_beta + t(chol(Sigma_beta)) %*% rnorm(ncol(X)))\n  \n  # Sample sigma_b^2 | beta\n  v_b_tilde &lt;- v_b + ncol(X)\n  S_b_tilde &lt;- (sum(beta^2) + v_b * S_b) / v_b_tilde\n  sigma2_b &lt;- v_b_tilde * S_b_tilde / rchisq(1, df = v_b_tilde)\n  \n  # Sample sigma^2 | beta, y\n  v_tilde &lt;- v + n\n  resid &lt;- y - X %*% beta\n  S_tilde &lt;- (sum(resid^2) + v * S) / v_tilde\n  sigma2 &lt;- v_tilde * S_tilde / rchisq(1, df = v_tilde)\n  \n  # Store samples\n  beta_samples[t, ] &lt;- beta\n  sigma2_samples[t] &lt;- sigma2\n  sigma2_b_samples[t] &lt;- sigma2_b\n}"
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#posterior-summaries",
    "href": "bayesian_linear_regression_conjugate.html#posterior-summaries",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\n\n\nCode\nposterior_summary &lt;- function(samples, probs = c(0.025, 0.5, 0.975)) {\n  c(\n    mean = mean(samples),\n    sd = sd(samples),\n    quantile(samples, probs = probs)\n  )\n}\n\n# Summaries for betas\nbeta_summary &lt;- t(apply(beta_samples[burn_in:n_iter, ], 2, posterior_summary))\nrownames(beta_summary) &lt;- paste0(\"beta\", 0:p)\n\n# Summaries for variance parameters\nsigma2_summary &lt;- posterior_summary(sigma2_samples[burn_in:n_iter])\nsigma2b_summary &lt;- posterior_summary(sigma2_b_samples[burn_in:n_iter])\n\nround(beta_summary, 4)\n\n\n         mean     sd    2.5%     50%   97.5%\nbeta0  1.9673 0.1077  1.7537  1.9669  2.1769\nbeta1  0.4405 0.1186  0.2063  0.4425  0.6727\nbeta2 -0.9460 0.1105 -1.1559 -0.9470 -0.7262\nbeta3  1.4319 0.1124  1.2171  1.4315  1.6524\n\n\nCode\nround(rbind(sigma2 = sigma2_summary, sigma2_b = sigma2b_summary), 4)\n\n\n           mean     sd  2.5%    50%  97.5%\nsigma2   1.1276 0.1638 0.852 1.1154 1.5078\nsigma2_b 1.8685 1.2652 0.625 1.5398 4.9971"
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#trace-plots",
    "href": "bayesian_linear_regression_conjugate.html#trace-plots",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\nCode\npar(mfrow = c(2, 2))\nfor (j in 1:ncol(X)) {\n  plot(beta_samples[, j], type = \"l\", main = paste(\"Trace: beta\", j - 1),\n       xlab = \"Iteration\", ylab = expression(beta))\n  abline(h = beta_true[j], col = \"red\", lwd = 2, lty = 2)\n}"
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#autocorrelation-plots",
    "href": "bayesian_linear_regression_conjugate.html#autocorrelation-plots",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Autocorrelation Plots",
    "text": "Autocorrelation Plots\n\n\nCode\npar(mfrow = c(2, 2))\nfor (j in 1:ncol(X)) {\n  acf(beta_samples[burn_in:n_iter, j], main = paste(\"ACF: beta\", j - 1))\n}"
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#convergence-diagnostics",
    "href": "bayesian_linear_regression_conjugate.html#convergence-diagnostics",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\nWe compute autocorrelation, Monte Carlo standard error, Geweke Z-score, Gelman-Rubin (), and effective sample size.\n\n\nCode\n# Simple diagnostics for one chain\nconvergence_stats &lt;- function(samples) {\n  n &lt;- length(samples)\n  ac1 &lt;- cor(samples[-1], samples[-n])\n  mcse &lt;- sd(samples) * sqrt((1 + ac1) / n)\n  a &lt;- floor(0.1 * n); b &lt;- floor(0.5 * n)\n  z &lt;- (mean(samples[1:a]) - mean(samples[(n - b + 1):n])) /\n       sqrt(var(samples[1:a]) / a + var(samples[(n - b + 1):n]) / b)\n  ess &lt;- n / (1 + 2 * sum(acf(samples, plot = FALSE)$acf[-1]))\n  c(autocorr1 = ac1, mcse = mcse, geweke_z = z, ess = ess)\n}\n\nconv_results &lt;- t(apply(beta_samples[burn_in:n_iter, ], 2, convergence_stats))\nrownames(conv_results) &lt;- paste0(\"beta\", 0:p)\nround(conv_results, 4)\n\n\n      autocorr1   mcse geweke_z      ess\nbeta0   -0.0055 0.0017   1.0590 4562.500\nbeta1    0.0308 0.0019   0.8929 3641.556\nbeta2   -0.0256 0.0017  -2.4631 4014.290\nbeta3    0.0189 0.0018   0.1831 3634.925"
  },
  {
    "objectID": "bayesian_linear_regression_conjugate.html#posterior-mean-vs-true-values",
    "href": "bayesian_linear_regression_conjugate.html#posterior-mean-vs-true-values",
    "title": "Bayesian Linear Regression with Conjugate Priors",
    "section": "Posterior Mean vs True Values",
    "text": "Posterior Mean vs True Values\n\n\nCode\npar(mar = c(5, 5, 4, 2))\nplot(beta_true, beta_summary[, \"mean\"], pch = 19, col = \"blue\",\n     xlab = \"True Coefficients\", ylab = \"Posterior Mean Estimates\",\n     main = \"Posterior Mean vs True Coefficients\")\nabline(0, 1, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topleft\", legend = c(\"Posterior Means\", \"y = x line\"), \n       col = c(\"blue\", \"red\"), pch = c(19, NA), lty = c(NA, 2))"
  },
  {
    "objectID": "bayesian_magma_tutorial.html",
    "href": "bayesian_magma_tutorial.html",
    "title": "Pathway Prioritization Using a Bayesian MAGMA Model",
    "section": "",
    "text": "This tutorial introduces Bayesian MAGMA — a gene set prioritization approach utilizing Bayesian Linear Regression (BLR) models within the MAGMA gene set analysis framework.\nThe figure below shows a schematic overview of the workflow.\nIn the initial step, GWAS summary data for the traits of interest are used to compute gene-level Z-scores using the VEGAS (Versatile Gene-Based Association Study) approach.\nNext, a design matrix linking genes to gene sets is constructed to integrate curated pathway databases (e.g., KEGG).\nThe Bayesian MAGMA model is then fitted using this design matrix as input predictors and the gene-level Z-scores as the response variable.\nThis results in posterior inclusion probabilities (PIPs) for each gene set, representing the probability that the set is included in the model.\nGene sets with higher PIPs are prioritized, highlighting pathways most likely to explain genetic associations.\nThe approach can also be extended to multi-trait analyses, enabling cross-trait exploration of pathway relevance.\nSee full details in Gholipourshahraki T, et al. PLOS Genet. 20, e1011463 (2024).\n\n\n Figure 1. Schematic overview of the Bayesian MAGMA workflow."
  },
  {
    "objectID": "bayesian_magma_tutorial.html#introduction",
    "href": "bayesian_magma_tutorial.html#introduction",
    "title": "Pathway Prioritization Using a Bayesian MAGMA Model",
    "section": "",
    "text": "This tutorial introduces Bayesian MAGMA — a gene set prioritization approach utilizing Bayesian Linear Regression (BLR) models within the MAGMA gene set analysis framework.\nThe figure below shows a schematic overview of the workflow.\nIn the initial step, GWAS summary data for the traits of interest are used to compute gene-level Z-scores using the VEGAS (Versatile Gene-Based Association Study) approach.\nNext, a design matrix linking genes to gene sets is constructed to integrate curated pathway databases (e.g., KEGG).\nThe Bayesian MAGMA model is then fitted using this design matrix as input predictors and the gene-level Z-scores as the response variable.\nThis results in posterior inclusion probabilities (PIPs) for each gene set, representing the probability that the set is included in the model.\nGene sets with higher PIPs are prioritized, highlighting pathways most likely to explain genetic associations.\nThe approach can also be extended to multi-trait analyses, enabling cross-trait exploration of pathway relevance.\nSee full details in Gholipourshahraki T, et al. PLOS Genet. 20, e1011463 (2024).\n\n\n Figure 1. Schematic overview of the Bayesian MAGMA workflow."
  },
  {
    "objectID": "bayesian_magma_tutorial.html#simulation-set-up",
    "href": "bayesian_magma_tutorial.html#simulation-set-up",
    "title": "Pathway Prioritization Using a Bayesian MAGMA Model",
    "section": "Simulation set up",
    "text": "Simulation set up\nThis tutorial demonstrates how to simulate gene-level association statistics and overlapping gene sets (pathways) for enrichment analysis using MAGMA-style approaches, implemented either as OLS joint estimation or via Bayesian inference with BayesC or BayesR priors.\nYou will learn to:\n\nSimulate gene-level Z-scores with causal genes.\n\nCreate overlapping pathways with realistic size distributions.\n\nEnrich specific pathways with causal genes.\n\nVisualize pathway sizes and overlaps.\n\nPerform gene set analysis using magma()-style models (linear and Bayesian)."
  },
  {
    "objectID": "bayesian_magma_tutorial.html#step-0-load-libraries",
    "href": "bayesian_magma_tutorial.html#step-0-load-libraries",
    "title": "Pathway Prioritization Using a Bayesian MAGMA Model",
    "section": "Step 0: Load libraries",
    "text": "Step 0: Load libraries\n\n\nCode\nlibrary(qgg)\nlibrary(ggplot2)"
  },
  {
    "objectID": "bayesian_magma_tutorial.html#step-1-basic-parameters",
    "href": "bayesian_magma_tutorial.html#step-1-basic-parameters",
    "title": "Pathway Prioritization Using a Bayesian MAGMA Model",
    "section": "Step 1: Basic Parameters",
    "text": "Step 1: Basic Parameters\n\n\nCode\nn_genes &lt;- 20000\nn_assoc &lt;-  100\nn_pathways &lt;- 100\neffect_mean &lt;- 2\neffect_sd &lt;- 1"
  },
  {
    "objectID": "bayesian_magma_tutorial.html#step-2-simulate-gene-level-association-statistics",
    "href": "bayesian_magma_tutorial.html#step-2-simulate-gene-level-association-statistics",
    "title": "Pathway Prioritization Using a Bayesian MAGMA Model",
    "section": "Step 2: Simulate Gene-Level Association Statistics",
    "text": "Step 2: Simulate Gene-Level Association Statistics\nIn this step, we simulate gene-level Z-scores for 20,000 genes,\nwhere a subset of 100 are truly associated (causal).\nWe then visualize the results using a QQ plot that distinguishes\ncausal genes (red) from null genes (grey).\n\n\nCode\ngenes &lt;- paste0(\"Gene\", seq_len(n_genes))\n\n# --- Step 1: Null genes centered around 0 ---\nz_scores &lt;- rnorm(n_genes, mean = 0, sd = 1)\nnames(z_scores) &lt;- genes\n\n# --- Step 2: Define causal genes (stronger, possibly positive or negative) ---\nassociated_genes &lt;- sample(genes, n_assoc)\nz_scores[associated_genes] &lt;- rnorm(n_assoc, mean = effect_mean, sd = effect_sd)\n\n# --- Step 3: Compute two-tailed p-values (VEGAS-style) ---\np_values &lt;- 2 * pnorm(-abs(z_scores))\n\n# --- Step 4: QQ plot ---\ndf &lt;- data.frame(\n  Gene = genes,\n  P = p_values,\n  causal = genes %in% associated_genes\n)\ndf &lt;- df[order(df$P), ]\ndf$expected &lt;- -log10(ppoints(n_genes))\ndf$observed &lt;- -log10(df$P)\n\ncol_map &lt;- c(\"FALSE\" = \"grey60\", \"TRUE\" = \"firebrick\")\n\nplot(df$expected, df$observed,\n     pch = 19, cex = 0.6,\n     col = col_map[as.character(df$causal)],\n     xlab = expression(Expected~~-log[10](italic(p))),\n     ylab = expression(Observed~~-log[10](italic(p))),\n     main = \"QQ Plot of Simulated VEGAS-Style Gene Z-scores\")\n\nabline(0, 1, col = \"black\", lwd = 2)\nlegend(\"topleft\",\n       legend = c(\"Null genes\", \"Causal genes\"),\n       col = c(\"grey60\", \"firebrick\"), pch = 19, bty = \"n\")\n\n\n\n\n\nQQ plot of simulated gene-level associations showing causal (red) and null (grey) genes."
  },
  {
    "objectID": "bayesian_magma_tutorial.html#step-3-draw-realistic-gene-set-sizes",
    "href": "bayesian_magma_tutorial.html#step-3-draw-realistic-gene-set-sizes",
    "title": "Pathway Prioritization Using a Bayesian MAGMA Model",
    "section": "Step 3: Draw Realistic Gene Set Sizes",
    "text": "Step 3: Draw Realistic Gene Set Sizes\nWe use a log-normal distribution to simulate pathway sizes, resulting in a right-skewed distribution (many small pathways, few large ones).\n\n\nCode\nsizes_raw &lt;- round(rlnorm(n_pathways, meanlog = log(80), sdlog = 0.6))\nsizes &lt;- pmin(pmax(sizes_raw, 15), 500)  # truncate to 15–500\nsummary(sizes)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   56.75   84.50   97.11  108.75  409.00"
  },
  {
    "objectID": "bayesian_magma_tutorial.html#step-4-create-overlapping-pathways",
    "href": "bayesian_magma_tutorial.html#step-4-create-overlapping-pathways",
    "title": "Pathway Prioritization Using a Bayesian MAGMA Model",
    "section": "Step 4: Create Overlapping Pathways",
    "text": "Step 4: Create Overlapping Pathways\nHere, we simulate overlapping pathways. Each pathway shares some genes with previous ones, mimicking realistic biological redundancy.\n\n\nCode\npathway_list &lt;- vector(\"list\", n_pathways)\nnames(pathway_list) &lt;- paste0(\"Pathway_\", seq_len(n_pathways))\n\nfor (i in seq_len(n_pathways)) {\n  size &lt;- sizes[i]\n  \n  if (i &gt; 1 && runif(1) &lt; 0.9) {  # 90% chance to overlap\n    j &lt;- sample(seq_len(i - 1), 1)  # pick existing pathway\n    target_overlap &lt;- round(size * runif(1, 0.20, 0.50))  # 20–50% overlap\n    overlap_pool &lt;- pathway_list[[j]]\n    k &lt;- min(target_overlap, length(overlap_pool), size)\n    overlap_genes &lt;- if (k &gt; 0) sample(overlap_pool, k) else character(0)\n    remaining_pool &lt;- setdiff(genes, overlap_genes)\n    n_remaining &lt;- size - length(overlap_genes)\n    new_genes &lt;- if (n_remaining &gt; 0) sample(remaining_pool, n_remaining) else character(0)\n    pathway_list[[i]] &lt;- unique(c(overlap_genes, new_genes))\n  } else {\n    pathway_list[[i]] &lt;- sample(genes, size)\n  }\n}"
  },
  {
    "objectID": "bayesian_magma_tutorial.html#step-5-create-causal-pathways",
    "href": "bayesian_magma_tutorial.html#step-5-create-causal-pathways",
    "title": "Pathway Prioritization Using a Bayesian MAGMA Model",
    "section": "Step 5: Create Causal Pathways",
    "text": "Step 5: Create Causal Pathways\nWe enrich 1–2 pathways with truly associated genes to simulate signal.\n\n\nCode\nn_enriched_paths &lt;- 2\nenriched_paths &lt;- sample(names(pathway_list), n_enriched_paths)\n\nfor (p in enriched_paths) {\n  n_enriched_genes &lt;- 30\n  enriched_genes &lt;- sample(associated_genes, n_enriched_genes)\n  pathway_list[[p]] &lt;- unique(c(pathway_list[[p]], enriched_genes))\n}\n\ncat(\"Created\", n_pathways, \"pathways (15–500 genes each)\\n\")\n\n\nCreated 100 pathways (15–500 genes each)\n\n\nCode\ncat(\"Causal pathways:\", paste(enriched_paths, collapse = \", \"), \"\\n\")\n\n\nCausal pathways: Pathway_75, Pathway_5"
  },
  {
    "objectID": "bayesian_magma_tutorial.html#step-6-visualize-pathway-size-distribution",
    "href": "bayesian_magma_tutorial.html#step-6-visualize-pathway-size-distribution",
    "title": "Pathway Prioritization Using a Bayesian MAGMA Model",
    "section": "Step 6: Visualize Pathway Size Distribution",
    "text": "Step 6: Visualize Pathway Size Distribution\n\n\nCode\nsizes_df &lt;- data.frame(Pathway = names(pathway_list), Size = sizes)\n\nggplot(sizes_df, aes(x = Size)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\") +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Distribution of Simulated Pathway Sizes\",\n    x = \"Number of genes per pathway\",\n    y = \"Count\"\n  )"
  },
  {
    "objectID": "bayesian_magma_tutorial.html#step-7-visualize-pathway-overlap",
    "href": "bayesian_magma_tutorial.html#step-7-visualize-pathway-overlap",
    "title": "Pathway Prioritization Using a Bayesian MAGMA Model",
    "section": "Step 7: Visualize Pathway Overlap",
    "text": "Step 7: Visualize Pathway Overlap\nWe calculate the Jaccard index (|A∩B| / |A∪B|) between pathways and visualize it with a heatmap.\n\n\nCode\nn_show &lt;- 30  # display subset\noverlap_matrix &lt;- matrix(NA, n_show, n_show,\n                         dimnames = list(names(pathway_list)[1:n_show],\n                                         names(pathway_list)[1:n_show]))\n\nfor (i in seq_len(n_show)) {\n  for (j in seq_len(n_show)) {\n    g1 &lt;- pathway_list[[i]]\n    g2 &lt;- pathway_list[[j]]\n    overlap_matrix[i, j] &lt;- length(intersect(g1, g2)) / length(union(g1, g2))\n  }\n}\n\nif (requireNamespace(\"pheatmap\", quietly = TRUE)) {\n  library(pheatmap)\n  pheatmap(\n    overlap_matrix,\n    main = \"Pathway Overlap (Jaccard Index)\",\n    color = colorRampPalette(c(\"white\", \"red\"))(50),\n    border_color = NA\n  )\n}"
  },
  {
    "objectID": "bayesian_magma_tutorial.html#step-8-run-and-compare-magma-style-gene-set-analyses",
    "href": "bayesian_magma_tutorial.html#step-8-run-and-compare-magma-style-gene-set-analyses",
    "title": "Pathway Prioritization Using a Bayesian MAGMA Model",
    "section": "Step 8: Run and Compare MAGMA-Style Gene Set Analyses",
    "text": "Step 8: Run and Compare MAGMA-Style Gene Set Analyses\nHere we compare three approaches implemented in qgg::magma():\n\n\n\n\n\n\n\n\n\nMethod\nModel Type\nConcept\nOutput\n\n\n\n\nJoint (fitM)\nLinear regression\nTests all pathways simultaneously\nβ-coefficients, SEs, p-values\n\n\nBayesC (fitC)\nBayesian spike-and-slab\nSelects relevant pathways by posterior inclusion probability (PIP)\nPosterior means, PIPs\n\n\nBayesR (fitR)\nBayesian mixture prior\nAllows pathways to have small, moderate, or large effects\nPosterior means, PIPs\n\n\n\n\nClassical joint model (standard MAGMA-style regression)\n\n\nCode\nfitM &lt;- magma(stat = z_scores, sets = pathway_list, type = \"joint\")\nhead(fitM)\n\n\n           ID   m      b    seb      z            p\n5   Pathway_5  59 0.0632 0.0075 8.4370 1.628093e-17\n75 Pathway_75 114 0.0353 0.0077 4.6053 2.059107e-06\n91 Pathway_91  18 0.0192 0.0073 2.6529 3.989774e-03\n21 Pathway_21 106 0.0203 0.0082 2.4603 6.940166e-03\n74 Pathway_74  87 0.0154 0.0076 2.0342 2.096702e-02\n97 Pathway_97 143 0.0147 0.0074 1.9749 2.413884e-02\n\n\n\n\nBayesian sparse model (BayesC)\n\n\nCode\nfitC &lt;- magma(stat = z_scores, sets = pathway_list, method = \"bayesC\")\nhead(fitC)\n\n\n           ID   m       b    PIP\n5   Pathway_5  59  0.0557 1.0000\n75 Pathway_75 114  0.0307 0.9072\n23 Pathway_23  67 -0.0009 0.0340\n91 Pathway_91  18  0.0001 0.0068\n4   Pathway_4 107 -0.0001 0.0052\n57 Pathway_57 108 -0.0001 0.0038\n\n\n\n\nBayesian mixture model (BayesR)\n\n\nCode\nfitR &lt;- magma(stat = z_scores, sets = pathway_list, method = \"bayesR\")\nhead(fitR)\n\n\n           ID   m       b    PIP\n5   Pathway_5  59  0.0576 1.0000\n75 Pathway_75 114  0.0334 0.9930\n23 Pathway_23  67 -0.0078 0.3201\n91 Pathway_91  18  0.0015 0.0768\n4   Pathway_4 107 -0.0012 0.0648\n57 Pathway_57 108 -0.0006 0.0344\n\n\n\n\nSummarize and Compare Outputs\n\n\nCode\n# Extract comparable metrics\nresults &lt;- list(\n  Joint = fitM[, c(\"ID\", \"b\", \"p\")],\n  BayesC = fitC[, c(\"ID\", \"b\", \"PIP\")],\n  BayesR = fitR[, c(\"ID\", \"b\", \"PIP\")]\n)\n\n# Merge results by Pathway ID\nmerged_results &lt;- Reduce(function(x, y) merge(x, y, by = \"ID\", all = TRUE), results)\ncolnames(merged_results) &lt;- c(\"Pathway\", \"b_joint\", \"p_joint\", \"b_bayesC\", \"PIP_bayesC\", \"b_bayesR\", \"PIP_bayesR\")\nmerged_results$is_enriched &lt;- merged_results$Pathway %in% enriched_paths\n\nhead(merged_results[order(-merged_results$PIP_bayesC), ])\n\n\n      Pathway b_joint      p_joint b_bayesC PIP_bayesC b_bayesR PIP_bayesR\n46  Pathway_5  0.0632 1.628093e-17   0.0557     1.0000   0.0576     1.0000\n74 Pathway_75  0.0353 2.059107e-06   0.0307     0.9072   0.0334     0.9930\n17 Pathway_23 -0.0227 9.924997e-01  -0.0009     0.0340  -0.0078     0.3201\n92 Pathway_91  0.0192 3.989774e-03   0.0001     0.0068   0.0015     0.0768\n35  Pathway_4 -0.0203 9.780063e-01  -0.0001     0.0052  -0.0012     0.0648\n54 Pathway_57 -0.0146 9.545129e-01  -0.0001     0.0038  -0.0006     0.0344\n   is_enriched\n46        TRUE\n74        TRUE\n17       FALSE\n92       FALSE\n35       FALSE\n54       FALSE\n\n\n\n\nVisualize Comparison of Evidence\n\n\nCode\nlibrary(ggplot2)\n\nggplot(merged_results, aes(x = -log10(p_joint), y = PIP_bayesC, color = is_enriched)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_text(data = subset(merged_results, is_enriched), aes(label = Pathway),\n            vjust = -1, size = 3.2) +\n  scale_color_manual(values = c(\"FALSE\" = \"grey60\", \"TRUE\" = \"firebrick\")) +\n  labs(\n    x = expression(-log[10](italic(p))~~\"(Joint model)\"),\n    y = \"Posterior Inclusion Probability (BayesC)\",\n    title = \"Comparison of Classical and Bayesian MAGMA Results\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThe joint linear model detects a broad set of pathways with small effects but cannot quantify inclusion evidence. BayesC and BayesR provide posterior probabilities reflecting uncertainty and sparsity. In this simulation, truly enriched pathways (in red) show both low p-values and high PIPs (&gt;0.9), illustrating consistency between classical and Bayesian inference while offering a probabilistic measure of pathway relevance."
  },
  {
    "objectID": "classical_linear_regression_simulation.html",
    "href": "classical_linear_regression_simulation.html",
    "title": "Classical Linear Regression Simulation in R",
    "section": "",
    "text": "This document demonstrates how to simulate data from a classical linear regression model and estimate parameters using Ordinary Least Squares (OLS), including inference and prediction intervals.\nWe also visualize the estimated vs. true regression coefficients."
  },
  {
    "objectID": "classical_linear_regression_simulation.html#introduction",
    "href": "classical_linear_regression_simulation.html#introduction",
    "title": "Classical Linear Regression Simulation in R",
    "section": "",
    "text": "This document demonstrates how to simulate data from a classical linear regression model and estimate parameters using Ordinary Least Squares (OLS), including inference and prediction intervals.\nWe also visualize the estimated vs. true regression coefficients."
  },
  {
    "objectID": "classical_linear_regression_simulation.html#r-code",
    "href": "classical_linear_regression_simulation.html#r-code",
    "title": "Classical Linear Regression Simulation in R",
    "section": "R Code",
    "text": "R Code\n\n\nCode\n# ============================================================\n# Classical Linear Regression: Simulation + OLS Estimation\n# ============================================================\n\nset.seed(123)\n\n# --- 1. Simulate data ---------------------------------------\nn &lt;- 100          # number of observations\np &lt;- 3            # number of predictors (excluding intercept)\n\nX &lt;- cbind(1, matrix(rnorm(n * p), n, p))  # include intercept\nbeta_true &lt;- c(2, 0.5, -1, 1.5)            # true coefficients\nsigma_true &lt;- 1                            # true residual SD\n\ny &lt;- X %*% beta_true + rnorm(n, 0, sigma_true)\n\n# --- 2. Estimate parameters via OLS --------------------------\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nresiduals &lt;- y - X %*% beta_hat\nsigma2_hat &lt;- as.numeric(t(residuals) %*% residuals / (n - (p + 1)))\n\n# --- 3. Inference: SEs, t-stats, p-values, CI ---------------\nvar_beta_hat &lt;- sigma2_hat * solve(t(X) %*% X)\nse_beta &lt;- sqrt(diag(var_beta_hat))\n\nt_stats &lt;- beta_hat / se_beta\np_values &lt;- 2 * (1 - pt(abs(t_stats), df = n - (p + 1)))\n\nalpha &lt;- 0.05\nt_crit &lt;- qt(1 - alpha/2, df = n - (p + 1))\nci_lower &lt;- beta_hat - t_crit * se_beta\nci_upper &lt;- beta_hat + t_crit * se_beta\n\n# Combine results into a table\nresults &lt;- data.frame(\n  Estimate = as.numeric(beta_hat),\n  SE = se_beta,\n  t_value = as.numeric(t_stats),\n  p_value = as.numeric(p_values),\n  CI_lower = as.numeric(ci_lower),\n  CI_upper = as.numeric(ci_upper)\n)\nrownames(results) &lt;-  paste0(\"beta\", 0:p)\nprint(round(results, 4))\n\n\n      Estimate     SE t_value p_value CI_lower CI_upper\nbeta0   1.9807 0.1073 18.4517   0e+00   1.7676   2.1937\nbeta1   0.4445 0.1169  3.8036   3e-04   0.2126   0.6765\nbeta2  -0.9538 0.1095 -8.7138   0e+00  -1.1711  -0.7365\nbeta3   1.4426 0.1122 12.8540   0e+00   1.2198   1.6654\n\n\nCode\n# --- 4. Prediction for a new observation ---------------------\nx_new &lt;- c(1, 0.5, -1, 1)  # include intercept term\ny_pred &lt;- as.numeric(x_new %*% beta_hat)\nvar_pred &lt;- sigma2_hat * (1 + t(x_new) %*% solve(t(X) %*% X) %*% x_new)\nse_pred &lt;- sqrt(var_pred)\n\n# 95% prediction interval\npred_interval &lt;- c(\n  lower = y_pred - t_crit * se_pred,\n  upper = y_pred + t_crit * se_pred\n)\n\ncat(\"\\nPredicted y_new =\", round(y_pred, 3), \"\\n\")\n\n\n\nPredicted y_new = 4.599 \n\n\nCode\ncat(\"95% Prediction interval: [\", round(pred_interval[1], 3), \",\",\n    round(pred_interval[2], 3), \"]\\n\")\n\n\n95% Prediction interval: [ 2.48 , 6.718 ]\n\n\nCode\n# --- 5. Visualization: True vs. Estimated Coefficients -------\npar(mar = c(5, 5, 4, 2))\nplot(beta_true, beta_hat, pch = 19, col = \"blue\", cex = 1.3,\n     xlab = \"True Coefficients\", ylab = \"Estimated Coefficients\",\n     main = \"OLS Estimates vs True Values\")\nabline(0, 1, col = \"red\", lwd = 2, lty = 2)\narrows(beta_true, ci_lower, beta_true, ci_upper, angle = 90, code = 3, length = 0.05, col = \"darkgray\")\nlegend(\"topleft\", legend = c(\"OLS Estimates\", \"y = x line\"), \n       col = c(\"blue\", \"red\"), pch = c(19, NA), lty = c(NA, 2))"
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#overview",
    "href": "narrated_bayesian_linear_regression_slides.html#overview",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Overview",
    "text": "Overview\n\nClassical Linear Regression\n\nModel, inference, and limitations\n\nBayesian Linear Regression\n\nMotivation, priors, and posteriors\n\nConditional posteriors and inference\n\nComputation and Applications\n\nMCMC and Gibbs sampling\n\nDiagnostics and R implementation\n\n\n\nWe begin with classical linear regression, focusing on the model formulation, inference methods, and the assumptions that often limit its use in complex data settings. Next, we introduce Bayesian linear regression, explaining how prior information and probabilistic reasoning extend classical inference. We will discuss how to define and interpret priors and posteriors and how conditional posteriors allow efficient inference. Finally, we will cover computational methods such as Markov chain Monte Carlo and Gibbs sampling, which make Bayesian estimation practical, and end with examples of diagnostics and R implementations. The goal is to provide both conceptual understanding and hands-on perspective for applying these models in real research."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#introduction",
    "href": "narrated_bayesian_linear_regression_slides.html#introduction",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Linear Regression (BLR) extends classical regression by incorporating prior information and producing posterior distributions over model parameters.\n\nAdvantages:\n\nHandles high-dimensional and small-sample problems.\n\nProvides full uncertainty quantification.\n\nEnables regularization and integration of prior biological knowledge.\n\n\n\nUnlike the classical approach, which produces single-point estimates of parameters, the Bayesian framework treats these parameters as random variables with their own probability distributions. This allows us to explicitly incorporate prior beliefs or biological information into the model. The resulting posterior distribution summarizes both the evidence from the data and the influence of the priors. One key advantage is that Bayesian models naturally handle situations where the number of predictors is large or the sample size is small. They also provide complete uncertainty quantification, enabling us to interpret results probabilistically rather than deterministically. Finally, the framework supports biologically informed priors, which can be useful when modeling genomic data."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#applications-in-genomics",
    "href": "narrated_bayesian_linear_regression_slides.html#applications-in-genomics",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Applications in Genomics",
    "text": "Applications in Genomics\n\nBayesian Linear Regression (BLR) is widely applied in quantitative genetics and genomics.\n\nCommon use cases:\n\nGenome-Wide Association Studies (GWAS) and fine-mapping of causal variants.\n\nGenetic prediction and heritability estimation.\n\nPathway and gene-set enrichment analyses.\n\nIntegrative multi-omics modeling (genome, transcriptome, epigenome).\n\n\n\nIn quantitative genetics, Bayesian methods are particularly useful for modeling large numbers of genetic markers simultaneously, while accounting for uncertainty in their effects. They are often used in genome-wide association studies to fine-map causal variants and quantify their contributions to complex traits. Another major application is genetic prediction, where Bayesian approaches can shrink noisy estimates and improve predictive accuracy. These models are also central to heritability estimation, helping partition genetic and environmental components of variance. Beyond single-variant analysis, Bayesian frameworks are used for pathway or gene-set enrichment studies, integrating biological structure into statistical modeling. Overall, Bayesian regression provides a flexible and coherent approach to studying molecular and genetic variation."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#classical-linear-regression",
    "href": "narrated_bayesian_linear_regression_slides.html#classical-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Classical Linear Regression",
    "text": "Classical Linear Regression\nModel\n\\[\ny = X\\beta + e, \\quad e \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n\\] - \\(y\\): outcomes\n- \\(X\\): design matrix\n- \\(\\beta\\): coefficients\n- \\(e\\): are the residuals\n- \\(\\sigma^2\\): residual variance\n\nWe now transition from the broader applications of Bayesian methods to a review of classical linear regression, which serves as our starting point for building Bayesian intuition.\nIn this model, the response variable ( y ) is explained as a linear combination of predictors in the design matrix ( X ), weighted by regression coefficients ( ), plus a residual error term ( e ).\nThe residuals are assumed to follow a normal distribution with mean zero and variance ( ^2 ), implying that deviations from the regression line are random and homoscedastic.\nEach term has a specific role: ( y ) represents the observed outcomes, ( X ) contains the predictors or explanatory variables, ( ) captures the strength and direction of their effects, and ( e ) accounts for unexplained variability.\nThis framework forms the foundation for both frequentist estimation — through least squares — and for Bayesian extensions, where we will later introduce priors and infer posterior distributions for ( ) and ( ^2 )."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#estimation",
    "href": "narrated_bayesian_linear_regression_slides.html#estimation",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Estimation",
    "text": "Estimation\nRegression effects: \\[\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n\\]\nResidual variance: \\[\n\\hat{\\sigma}^2 = \\frac{1}{n-p}\\sum_i (y_i - x_i^\\top \\hat{\\beta})^2\n\\]\nInference via standard errors and \\(t\\)-tests, confidence intervals, and prediction intervals.\n\nIn classical linear regression, estimation of the model parameters is achieved using the method of least squares.\nThe estimated regression coefficients, denoted by beta hat, are obtained by minimizing the sum of squared residuals.\nMathematically, this solution is given by ( = (XX){-1} X^y ).\nIntuitively, this means we’re finding the set of coefficients that produces the best linear fit to the data, in the sense of minimizing the vertical distances between observed and predicted values.\nOnce the coefficients are estimated, we can compute the residual variance, denoted ( ^2 ), which measures the average squared deviation between observed outcomes and model predictions.\nThis variance provides a key measure of unexplained variability, and it underlies inference through standard errors, t-tests, confidence intervals, and prediction intervals.\nAltogether, these quantities form the backbone of classical regression inference before introducing Bayesian perspectives."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#limitations",
    "href": "narrated_bayesian_linear_regression_slides.html#limitations",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Limitations",
    "text": "Limitations\n\nNo explicit control over effect size distribution\nSensitive when collinearity is high\nNot identifiable when \\(p&gt;n\\)\nUncertainty largely asymptotic unless normality assumptions hold\n\n\nWhile classical linear regression provides a simple and powerful framework, it also has important limitations that motivate Bayesian alternatives.\nFirst, the model does not allow us to explicitly control the distribution of effect sizes — all regression coefficients are treated symmetrically, regardless of prior knowledge about their likely magnitudes.\nSecond, the estimates become unstable in the presence of collinearity, when predictors are highly correlated, leading to inflated variances and unreliable inference.\nThird, the model is not identifiable when the number of predictors exceeds the number of observations, that is, when ( p &gt; n ); in such cases, there are infinitely many solutions that fit the data equally well.\nFinally, classical inference relies heavily on asymptotic approximations and normality assumptions, which can underestimate uncertainty in small or complex data sets.\nThese limitations highlight why Bayesian frameworks, which introduce prior structure and probabilistic uncertainty, are often preferred in modern genomics and high-dimensional settings."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#why-bayesian-linear-regression",
    "href": "narrated_bayesian_linear_regression_slides.html#why-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Why Bayesian Linear Regression?",
    "text": "Why Bayesian Linear Regression?\n\nCombines likelihood and prior to form the posterior.\n\nPriors express beliefs about effect sizes:\n\nNormal → many small effects\n\nSpike-and-slab → sparse effects\n\n\nActs as a regularizer:\n\nShrinks small/noisy effects toward \\(0\\)\n\nPreserves large, important effects\n\n\nStable when \\(p &gt; n\\) due to prior information.\n\nProvides full posterior distributions for \\(\\beta\\) and \\(\\sigma^2\\).\n\n\nBayesian linear regression extends the classical framework by introducing prior information and combining it with the likelihood to obtain a posterior distribution.\nThe likelihood represents what the data tell us about the parameters, while the prior expresses our beliefs about plausible effect sizes before observing the data.\nDifferent choices of priors reflect different assumptions: a normal prior assumes that most effects are small and centered around zero, whereas a spike-and-slab prior encourages sparsity, allowing a few large effects while setting many others exactly to zero.\nThis incorporation of prior structure acts as a form of regularization — it naturally shrinks small or noisy estimates toward zero, while preserving larger, well-supported effects.\nAn additional advantage is that the model remains identifiable and stable even when the number of predictors exceeds the number of observations, that is, when ( p &gt; n ).\nFinally, Bayesian inference provides full posterior distributions for both the regression coefficients and the residual variance, giving us direct probabilistic measures of uncertainty instead of relying on asymptotic approximations."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#overview-bayesian-linear-regression",
    "href": "narrated_bayesian_linear_regression_slides.html#overview-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Overview: Bayesian Linear Regression",
    "text": "Overview: Bayesian Linear Regression\n\nCombines data and prior knowledge using Bayes’ rule.\n\nUses conjugate priors to yield closed-form full conditionals.\n\nEmploys Gibbs sampling to approximate the posterior distribution.\n\nEstimates parameters, uncertainty, and predictions from posterior draws.\n\n\nThis slide provides an overview of the Bayesian linear regression framework and how it differs operationally from the classical approach.\nAt its core, Bayesian inference combines prior knowledge and observed data through Bayes’ rule to obtain the posterior distribution of the model parameters.\nIn practice, we often choose conjugate priors because they lead to closed-form full conditional distributions, which greatly simplify posterior computation.\nTo explore these posterior distributions, we employ Gibbs sampling — a form of Markov chain Monte Carlo that iteratively samples each parameter from its conditional distribution, given the others.\nFrom the resulting posterior draws, we can estimate not only the parameters themselves, but also their uncertainty and predictive distributions for new observations.\nThis process provides a coherent, probabilistic framework for inference, prediction, and model interpretation."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#bayesian-linear-regression-with-gaussian-priors",
    "href": "narrated_bayesian_linear_regression_slides.html#bayesian-linear-regression-with-gaussian-priors",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Bayesian Linear Regression with Gaussian Priors",
    "text": "Bayesian Linear Regression with Gaussian Priors\nBayesian linear regression starts with the same model structure as classical linear regression.\n\\[\ny = X\\beta + e, \\quad e \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n\\]\n\n\\(y\\): \\(n \\times 1\\) vector of observed outcomes\n\n\\(X\\): \\(n \\times p\\) design matrix of predictors\n\n\\(\\beta\\): \\(p \\times 1\\) vector of unknown coefficients\n\n\\(e\\): Gaussian noise with mean \\(0\\) and variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#likelihood-in-bayesian-linear-regression",
    "href": "narrated_bayesian_linear_regression_slides.html#likelihood-in-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Likelihood in Bayesian Linear Regression",
    "text": "Likelihood in Bayesian Linear Regression\nBecause the residuals are Gaussian, it follows that the marginal distribution of \\(y\\) is:\n\\[\ne \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n\\] The marginal distribution of \\(y\\) is:\n\\[\ny \\sim \\mathcal{N}(X\\beta, \\sigma^2 I_n)\n\\] This defines the likelihood the probability of the observed data given parameters \\(\\beta\\) and \\(\\sigma^2\\):\n\\[\np(y \\mid X, \\beta, \\sigma^2) = \\mathcal{N}(X\\beta, \\sigma^2 I_n)\n\\]"
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#introducing-priors",
    "href": "narrated_bayesian_linear_regression_slides.html#introducing-priors",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Introducing Priors",
    "text": "Introducing Priors\nIn Bayesian linear regression, we specify prior distributions that express our beliefs about parameters before seeing the data.\nA common conjugate prior for the regression coefficients is:\n\\[\n\\beta \\mid \\sigma_b^2 \\sim \\mathcal{N}(0, \\sigma_b^2 I_p)\n\\]\nThis reflects the belief that most effect sizes are small and centered near zero — consistent with the polygenic assumption in genetics."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#role-of-the-prior-variance-sigma_b2",
    "href": "narrated_bayesian_linear_regression_slides.html#role-of-the-prior-variance-sigma_b2",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Role of the Prior Variance \\(\\sigma_b^2\\)",
    "text": "Role of the Prior Variance \\(\\sigma_b^2\\)\nThe parameter \\(\\sigma_b^2\\) acts as a shrinkage (regularization) parameter:\n\nSmall \\(\\sigma_b^2\\) → stronger shrinkage toward zero.\n\nLarge \\(\\sigma_b^2\\) → weaker shrinkage, allowing larger effects.\n\nIt controls the strength of regularization and is often treated as an unknown hyperparameter estimated from the data."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#priors-on-variance-components",
    "href": "narrated_bayesian_linear_regression_slides.html#priors-on-variance-components",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Priors on Variance Components",
    "text": "Priors on Variance Components\nWe also place priors on the variance components to complete the hierarchical model.\n\\[\n\\sigma_b^2 \\mid S_b, v_b \\sim S_b \\, \\chi^{-2}(v_b), \\quad\n\\sigma^2 \\mid S, v \\sim S \\, \\chi^{-2}(v)\n\\]\nHere:\n\n\\(S_b\\) and \\(v_b\\) are user-defined hyperparameters that control the prior distribution on the variance of regression coefficients.\n\n\\(S\\) and \\(v\\) are hyperparameters for the residual variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#conjugate-priors-and-regularization",
    "href": "narrated_bayesian_linear_regression_slides.html#conjugate-priors-and-regularization",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Conjugate Priors and Regularization",
    "text": "Conjugate Priors and Regularization\nConjugate priors keep posteriors in the same family\n(e.g., scaled inverse-chi-squared), allowing closed-form Gibbs updates.\nThey also serve as regularizers:\n\nThe prior on \\(\\beta\\) shrinks small or noisy effects toward zero.\n\nPriors on variance components prevent overfitting, especially when \\(p &gt; n\\).\n\nThus, conjugate priors make Bayesian linear regression efficient and stable."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#posterior-distribution",
    "href": "narrated_bayesian_linear_regression_slides.html#posterior-distribution",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\nIn Bayesian analysis, we combine the likelihood and priors using Bayes’ rule to obtain the joint posterior:\n\\[\np(\\beta, \\sigma_b^2, \\sigma^2 \\mid y) \\propto\np(y \\mid \\beta, \\sigma^2)\\;\np(\\beta \\mid \\sigma_b^2)\\;\np(\\sigma_b^2)\\;\np(\\sigma^2)\n\\]\nThis posterior captures all updated knowledge about the unknown parameters after observing the data.\nIt forms the basis for computing posterior means, credible intervals, and predictions."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#conjugacy-and-gibbs-sampling",
    "href": "narrated_bayesian_linear_regression_slides.html#conjugacy-and-gibbs-sampling",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Conjugacy and Gibbs Sampling",
    "text": "Conjugacy and Gibbs Sampling\nWith conjugate priors, each parameter’s full conditional distribution has a closed-form solution.\nThis makes Gibbs sampling a natural and efficient inference method.\n\nParameters are updated one at a time, each from its conditional posterior.\n\nThe resulting Markov chain explores the joint posterior of\n\\((\\beta, \\sigma_b^2, \\sigma^2)\\).\n\nGibbs sampling thus provides an easy way to approximate the full posterior in Bayesian linear regression."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#full-conditional-for-beta",
    "href": "narrated_bayesian_linear_regression_slides.html#full-conditional-for-beta",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Full Conditional for \\(\\beta\\)",
    "text": "Full Conditional for \\(\\beta\\)\nGiven \\(\\sigma^2\\), \\(\\sigma_b^2\\), and the data \\(y\\), the regression coefficients have a multivariate normal conditional posterior:\n\\[\n\\beta \\mid \\sigma^2, \\sigma_b^2, y \\sim\n\\mathcal{N}(\\mu_\\beta, \\Sigma_\\beta)\n\\]\nwhere\n\\[\n\\Sigma_\\beta = \\left( \\frac{X^\\top X}{\\sigma^2} + \\frac{I}{\\sigma_b^2} \\right)^{-1},\n\\quad\n\\mu_\\beta = \\Sigma_\\beta \\frac{X^\\top y}{\\sigma^2}\n\\]\nThis distribution represents our updated belief about \\(\\beta\\)\nafter observing the data, while holding \\(\\sigma_b^2\\) and \\(\\sigma^2\\) fixed."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#comparison-to-classical-ols",
    "href": "narrated_bayesian_linear_regression_slides.html#comparison-to-classical-ols",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Comparison to Classical OLS",
    "text": "Comparison to Classical OLS\nIn classical regression, the OLS estimator is\n\\[\n\\hat\\beta_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top y,\n\\quad y \\sim \\mathcal{N}(X\\beta, \\sigma^2 I)\n\\]\nThe estimate of \\(\\beta\\) is independent of \\(\\sigma^2\\),\nsince \\(\\sigma^2\\) only scales the likelihood, not its maximum.\nIn Bayesian regression, \\(\\sigma^2\\) appears explicitly in the posterior:\n\\[\n\\Sigma_\\beta = \\left( \\frac{X^\\top X}{\\sigma^2} + \\frac{I}{\\sigma_b^2} \\right)^{-1},\n\\quad\n\\mu_\\beta = \\Sigma_\\beta \\frac{X^\\top y}{\\sigma^2}\n\\]\nThe term \\(\\frac{I}{\\sigma_b^2}\\) introduces shrinkage, regularizing estimates and stabilizing inference especially when \\(p &gt; n\\) or predictors are highly correlated.\nThus, the Bayesian posterior mean is a regularized, uncertainty-aware generalization of OLS."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#full-conditional-for-beta_j",
    "href": "narrated_bayesian_linear_regression_slides.html#full-conditional-for-beta_j",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Full Conditional for \\(\\beta_j\\)",
    "text": "Full Conditional for \\(\\beta_j\\)\nInstead of sampling \\(\\beta\\) jointly, we can update each coefficient \\(\\beta_j\\) one at a time, holding all others fixed efficient for large \\(p\\) or spike-and-slab models.\nLet \\(X_j\\) be the \\(j\\)th column of \\(X\\) and define the partial residual:\n\\[\nr_j = y - X_{-j} \\beta_{-j}\n\\]\nThen the conditional posterior for \\(\\beta_j\\) is univariate normal:\n\\[\n\\beta_j \\mid D \\sim \\mathcal{N} \\!\\left(\n\\frac{X_j^\\top r_j}{X_j^\\top X_j + \\sigma^2 / \\sigma_b^2},\\;\n\\frac{\\sigma^2}{X_j^\\top X_j + \\sigma^2 / \\sigma_b^2}\n\\right)\n\\]\nThis corresponds to a regularized least-squares update. Residual updates avoid matrix inversion, scale to high dimensions, and extend naturally to sparse (spike-and-slab) models."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#full-conditional-for-sigma_b2",
    "href": "narrated_bayesian_linear_regression_slides.html#full-conditional-for-sigma_b2",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Full Conditional for \\(\\sigma_b^2\\)",
    "text": "Full Conditional for \\(\\sigma_b^2\\)\nThe conditional distribution of the prior variance \\(\\sigma_b^2\\), given \\(\\beta\\) and the hyperparameters, is a scaled inverse-chi-squared:\n\\[\n\\sigma_b^2 \\mid \\beta \\sim \\tilde{S}_b \\, \\chi^{-2}(\\tilde{v}_b)\n\\]\nwhere\n\\[\n\\tilde{v}_b = v_b + p, \\quad\n\\tilde{S}_b = \\frac{\\beta^\\top \\beta + v_b S_b}{\\tilde{v}_b}\n\\]\nAt each Gibbs iteration, \\(\\sigma_b^2\\) is sampled directly given \\(\\beta\\). This update reflects our revised belief about the variability of effect sizes after observing the current posterior draw of \\(\\beta\\)."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#full-conditional-for-sigma2",
    "href": "narrated_bayesian_linear_regression_slides.html#full-conditional-for-sigma2",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Full Conditional for \\(\\sigma^2\\)",
    "text": "Full Conditional for \\(\\sigma^2\\)\nThe conditional distribution of the residual variance \\(\\sigma^2\\),\ngiven \\(\\beta\\) and the data, is also scaled inverse-chi-squared:\n\\[\n\\sigma^2 \\mid \\beta, y \\sim \\tilde{S} \\, \\chi^{-2}(\\tilde{v})\n\\]\nwhere\n\\[\n\\tilde{v} = v + n, \\quad\n\\tilde{S} = \\frac{(y - X\\beta)^\\top (y - X\\beta) + v S}{\\tilde{v}}\n\\]\nAt each Gibbs iteration, \\(\\sigma^2\\) is sampled directly given \\(\\beta\\).\nThis captures our updated belief about the residual variability\nafter accounting for the current linear predictor \\(X\\beta\\)."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#gibbs-sampling-motivation",
    "href": "narrated_bayesian_linear_regression_slides.html#gibbs-sampling-motivation",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Gibbs Sampling: Motivation",
    "text": "Gibbs Sampling: Motivation\nBayesian inference often involves complex posteriors that lack closed-form solutions. To approximate these, we use Markov Chain Monte Carlo (MCMC) methods.\nMCMC builds a Markov chain whose stationary distribution is the target posterior. Once the chain has converged, its samples can be used to estimate:\n\nPosterior means, variances, and credible intervals\n\nPredictive distributions\n\nOther functions of interest\n\nAmong MCMC algorithms, the Gibbs sampler is especially useful when all full conditional distributions are available in closed form."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#gibbs-sampling-the-algorithm",
    "href": "narrated_bayesian_linear_regression_slides.html#gibbs-sampling-the-algorithm",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Gibbs Sampling: The Algorithm",
    "text": "Gibbs Sampling: The Algorithm\nFor Bayesian linear regression with conjugate priors, the joint posterior is:\n\\[\np(\\beta, \\sigma_b^2 , \\sigma^2 \\mid y) \\propto\np(y \\mid \\beta, \\sigma^2)\\; p(\\beta \\mid \\sigma_b^2)\\; p(\\sigma_b^2)\\; p(\\sigma^2)\n\\]\nWe iteratively draw from the following full conditionals:\n\nSample \\(\\beta \\mid \\sigma_b^2, \\sigma^2, y\\)\n\nSample \\(\\sigma_b^2 \\mid \\beta\\)\n\nSample \\(\\sigma^2 \\mid \\beta, y\\)\n\nEach step updates one parameter given the latest values of the others. Repeating this sequence yields samples from the joint posterior \\(p(\\beta, \\sigma_b^2, \\sigma^2 \\mid y)\\).\nBecause each conditional is standard (Normal or scaled inverse-\\(\\chi^2\\)), Gibbs sampling is both efficient and easy to implement."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#posterior-summaries",
    "href": "narrated_bayesian_linear_regression_slides.html#posterior-summaries",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Summaries",
    "text": "Posterior Summaries\nAfter running the Gibbs sampler, we obtain posterior draws \\(\\{\\theta^{(t)}\\}_{t=1}^T\\) for parameters such as \\(\\beta_j\\), \\(\\sigma^2\\), or \\(\\sigma_b^2\\).\nWe summarize the posterior distribution via:\n\nPosterior mean\n\\[\n\\mathbb{E}[\\theta \\mid y] \\approx \\frac{1}{T} \\sum_{t=1}^{T} \\theta^{(t)}\n\\]\nPosterior median: the median of \\(\\theta^{(t)}\\)\nCredible interval (95%)\n\\[\n[\\theta]_{0.025}, [\\theta]_{0.975}\n\\]\n\nThese summaries describe the most probable values of \\(\\theta\\)\nand their uncertainty after combining data and prior beliefs."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#estimating-uncertainty",
    "href": "narrated_bayesian_linear_regression_slides.html#estimating-uncertainty",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Estimating Uncertainty",
    "text": "Estimating Uncertainty\nBayesian inference provides full posterior distributions, not just point estimates. Uncertainty is quantified directly from the posterior samples:\n\nPosterior standard deviation\n\\[\n\\mathrm{SD}(\\theta \\mid y) \\approx\n\\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T} (\\theta^{(t)} - \\bar{\\theta})^2}\n\\]\n\nThe width of the credible interval reflects this uncertainty. Parameters with broader posteriors are estimated with less precision, and the degree of uncertainty depends on both the data and the prior."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#posterior-prediction",
    "href": "narrated_bayesian_linear_regression_slides.html#posterior-prediction",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\nGiven a new observation \\(x_{\\text{new}}\\), we can predict using posterior draws:\n\nCompute predicted means for each sample: \\[\n\\hat{y}_{\\text{new}}^{(t)} = x_{\\text{new}}^\\top \\beta^{(t)}\n\\]\nAdd residual uncertainty: \\[\ny_{\\text{new}}^{(t)} \\sim\n\\mathcal{N}\\!\\left(x_{\\text{new}}^\\top \\beta^{(t)},\\; \\sigma^{2(t)}\\right)\n\\]\n\nThe resulting samples \\(\\{y_{\\text{new}}^{(t)}\\}\\) form a posterior predictive distribution, from which we can derive predictive intervals and evaluate predictive accuracy."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#model-checking-and-hypothesis-testing",
    "href": "narrated_bayesian_linear_regression_slides.html#model-checking-and-hypothesis-testing",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Model Checking and Hypothesis Testing",
    "text": "Model Checking and Hypothesis Testing\nPosterior samples enable rich model diagnostics and hypothesis testing:\n\nPosterior probability of an event\n\\[\n\\Pr(\\beta_j \\ne 0 \\mid y)\n\\approx \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{1}\\!\\left(\\beta_j^{(t)} \\ne 0\\right)\n\\]\nPosterior predictive checks\nSimulate new datasets using posterior draws and compare them to the observed data to assess model fit.\nModel comparison\nBayes factors and marginal likelihoods can be approximated to formally test or compare competing models.\n\nThese tools extend Bayesian inference beyond estimation to model validation, uncertainty quantification, and decision-making."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#convergence-diagnostics",
    "href": "narrated_bayesian_linear_regression_slides.html#convergence-diagnostics",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\nBefore interpreting MCMC results, we must check that the Gibbs sampler has converged to the target posterior distribution.\nConvergence diagnostics assess whether the Markov chain has reached its stationary distribution and is producing valid samples.\nTwo basic strategies are:\n\nBurn-in – Discard early iterations (e.g., first 1000) to remove dependence on starting values.\n\nThinning – Keep every \\(k\\)-th sample to reduce autocorrelation.\n\nThese steps improve sample quality and ensure reliable posterior summaries."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#trace-plots",
    "href": "narrated_bayesian_linear_regression_slides.html#trace-plots",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Trace Plots",
    "text": "Trace Plots\nA simple yet powerful diagnostic is the trace plot,\nshowing sampled parameter values \\(\\theta^{(t)}\\) over iterations \\(t\\).\n\nA converged chain fluctuates around a stable mean — no trend or drift.\n\nMultiple chains from different starting points should overlap and mix well.\n\nTrace plots help detect: - Lack of stationarity (upward/downward trends) - Poor mixing or multimodality - Burn-in issues\nVisual inspection is often the first step in assessing convergence."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#autocorrelation",
    "href": "narrated_bayesian_linear_regression_slides.html#autocorrelation",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nSamples from a Gibbs sampler are correlated, especially for tightly coupled parameters.\nThe autocorrelation function (ACF) quantifies dependence across lags \\(k\\):\n\\[\n\\hat{\\rho}_k =\n\\frac{\\sum_{t=1}^{T-k} (\\theta^{(t)} - \\bar{\\theta})(\\theta^{(t+k)} - \\bar{\\theta})}\n     {\\sum_{t=1}^{T} (\\theta^{(t)} - \\bar{\\theta})^2}\n\\]\n\nHigh \\(\\hat{\\rho}_k\\) → slow mixing and fewer effective samples\n\nLow \\(\\hat{\\rho}_k\\) → better mixing and faster convergence\n\nReducing autocorrelation may require more iterations,\nreparameterization, or thinning the chain."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#effective-sample-size-ess",
    "href": "narrated_bayesian_linear_regression_slides.html#effective-sample-size-ess",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Effective Sample Size (ESS)",
    "text": "Effective Sample Size (ESS)\nAutocorrelation reduces the number of independent samples obtained.\nThe effective sample size (ESS) adjusts for this:\n\\[\n\\text{ESS}(\\theta) =\n\\frac{T}{1 + 2 \\sum_{k=1}^{K} \\hat{\\rho}_k}\n\\]\n\nSmall ESS → chain is highly correlated, less informative\n\nRule of thumb: \\(\\text{ESS} &gt; 100\\) per parameter for stable inference\n\nESS provides a quantitative measure of sampling efficiency\nand helps determine whether more iterations are needed."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#gelmanrubin-diagnostic-hatr",
    "href": "narrated_bayesian_linear_regression_slides.html#gelmanrubin-diagnostic-hatr",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Gelman–Rubin Diagnostic (\\(\\hat{R}\\))",
    "text": "Gelman–Rubin Diagnostic (\\(\\hat{R}\\))\nWhen running multiple chains, the Gelman–Rubin statistic compares between-chain and within-chain variability.\nFor \\(m\\) chains with \\(T\\) iterations each:\n\\[\nW = \\frac{1}{m} \\sum_{i=1}^{m} s_i^2, \\quad\nB = \\frac{T}{m-1} \\sum_{i=1}^{m} (\\bar{\\theta}_i - \\bar{\\theta})^2\n\\]\nThe potential scale reduction factor:\n\\[\n\\hat{R} = \\sqrt{ \\frac{\\hat{V}}{W} }, \\quad\n\\hat{V} = \\frac{T-1}{T} W + \\frac{1}{T} B\n\\]\n\n\\(\\hat{R} \\approx 1\\) → convergence achieved\n\n\\(\\hat{R} &gt; 1.1\\) → chains have not converged"
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#geweke-diagnostic",
    "href": "narrated_bayesian_linear_regression_slides.html#geweke-diagnostic",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Geweke Diagnostic",
    "text": "Geweke Diagnostic\nThe Geweke test checks whether early and late portions of a single chain have the same mean, indicating stationarity.\n\\[\nZ =\n\\frac{\\bar{\\theta}_A - \\bar{\\theta}_B}\n     {\\sqrt{\\text{Var}(\\bar{\\theta}_A) + \\text{Var}(\\bar{\\theta}_B)}}\n\\]\nTypically:\n\nSegment A = first 10% of the chain\n\nSegment B = last 50% of the chain\n\nUnder convergence, \\(Z \\sim \\mathcal{N}(0,1)\\).\n\n\\(|Z| \\le 2\\) → chain likely stationary\n\n\\(|Z| &gt; 2\\) → potential non-convergence\n\nThese diagnostics ensure that posterior summaries reflect the true target distribution."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#spike-and-slab-bayesian-linear-regression",
    "href": "narrated_bayesian_linear_regression_slides.html#spike-and-slab-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Spike-and-Slab Bayesian Linear Regression",
    "text": "Spike-and-Slab Bayesian Linear Regression\nAs in classical BLR, the outcome is modeled as:\n\\[\ny = Xb + e, \\quad e \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n\\]\nwhere \\(y\\) is the \\(n \\times 1\\) response, \\(X\\) the design matrix, \\(b\\) the regression coefficients, and \\(\\sigma^2\\) the residual variance.\nThis defines the likelihood:\n\\[\ny \\mid b, \\sigma^2 \\sim \\mathcal{N}(Xb, \\sigma^2 I_n)\n\\]\nThe goal is to estimate \\(b\\) and identify which predictors truly contribute to \\(y\\)."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#motivation-for-the-spike-and-slab-prior",
    "href": "narrated_bayesian_linear_regression_slides.html#motivation-for-the-spike-and-slab-prior",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Motivation for the Spike-and-Slab Prior",
    "text": "Motivation for the Spike-and-Slab Prior\nIn standard Bayesian linear regression:\n\\[\n\\beta_j \\sim \\mathcal{N}(0, \\sigma_b^2)\n\\]\nThis Gaussian (shrinkage) prior assumes all predictors have small effects, but it does not allow exact zeros — limiting variable selection.\nThe spike-and-slab prior addresses this by mixing two components:\n\nA spike at zero → excluded predictors\n\nA slab (wide normal) → active predictors\n\nThis yields sparse, interpretable models that select relevant variables."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#the-spike-and-slab-mixture-prior",
    "href": "narrated_bayesian_linear_regression_slides.html#the-spike-and-slab-mixture-prior",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "The Spike-and-Slab Mixture Prior",
    "text": "The Spike-and-Slab Mixture Prior\nEach regression effect is drawn from a two-component mixture:\n\\[\np(b_i \\mid \\sigma_b^2, \\pi)\n= \\pi\\, \\mathcal{N}(0, \\sigma_b^2) + (1-\\pi)\\, \\delta_0\n\\]\nwhere:\n\n\\(\\pi\\) = prior probability that \\(b_i\\) is non-zero\n\n\\(\\delta_0\\) = point mass at zero\n\nThus, with probability \\(\\pi\\) a predictor is active (slab), and with probability \\(1-\\pi\\) it is excluded (spike)."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#advantages-of-spike-and-slab-priors",
    "href": "narrated_bayesian_linear_regression_slides.html#advantages-of-spike-and-slab-priors",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Advantages of Spike-and-Slab Priors",
    "text": "Advantages of Spike-and-Slab Priors\nThis hierarchical mixture prior provides several benefits:\n\nSparsity — allows exact zeros for irrelevant predictors\n\nInterpretability — binary indicators give posterior inclusion probabilities (PIPs)\n\nAdaptivity — the inclusion probability \\(\\pi\\) is learned from the data\n\nBalance — captures both strong signals (detection) and small effects (prediction)\n\nHence, spike-and-slab models combine variable selection with Bayesian uncertainty quantification."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#hierarchical-representation",
    "href": "narrated_bayesian_linear_regression_slides.html#hierarchical-representation",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Hierarchical Representation",
    "text": "Hierarchical Representation\nWe express each effect as:\n\\[\nb_i = \\alpha_i \\, \\delta_i\n\\]\nwhere:\n\\[\n\\alpha_i \\mid \\sigma_b^2 \\sim \\mathcal{N}(0, \\sigma_b^2), \\quad\n\\delta_i \\mid \\pi \\sim \\text{Bernoulli}(\\pi)\n\\]\n\n\\(\\alpha_i\\): effect size when predictor is included\n\n\\(\\delta_i\\): binary inclusion indicator (0 or 1)\n\nMarginalizing over \\(\\delta_i\\) yields the spike-and-slab mixture prior above."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#prior-for-the-inclusion-probability-pi",
    "href": "narrated_bayesian_linear_regression_slides.html#prior-for-the-inclusion-probability-pi",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Prior for the Inclusion Probability \\(\\pi\\)",
    "text": "Prior for the Inclusion Probability \\(\\pi\\)\nThe overall sparsity level is controlled by \\(\\pi\\), assigned a Beta prior:\n\\[\n\\pi \\sim \\text{Beta}(\\alpha, \\beta)\n\\]\n\nSmall \\(\\alpha\\), large \\(\\beta\\) → favor sparser models\n\n\\(\\alpha = \\beta = 1\\) → uniform prior\n\nLarger \\(\\alpha\\) → denser models\n\nThis prior lets the data determine the degree of sparsity."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#priors-for-variance-components",
    "href": "narrated_bayesian_linear_regression_slides.html#priors-for-variance-components",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Priors for Variance Components",
    "text": "Priors for Variance Components\nVariance parameters use scaled inverse-chi-squared priors:\n\\[\n\\sigma_b^2 \\sim S_b \\chi^{-2}(v_b), \\quad\n\\sigma^2 \\sim S \\chi^{-2}(v)\n\\]\nThese are conjugate, providing closed-form conditional updates. Hyperparameters \\((S_b, v_b)\\) and \\((S, v)\\) encode prior beliefs about effect size variability and residual noise."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#joint-posterior-structure",
    "href": "narrated_bayesian_linear_regression_slides.html#joint-posterior-structure",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Joint Posterior Structure",
    "text": "Joint Posterior Structure\nCombining the likelihood and priors, the joint posterior is:\n\\[\np(\\mu, \\alpha, \\delta, \\pi, \\sigma_b^2, \\sigma^2 \\mid y)\n\\propto\np(y \\mid \\mu, \\alpha, \\delta, \\sigma^2)\\,\np(\\alpha \\mid \\sigma_b^2)\\,\np(\\delta \\mid \\pi)\\,\np(\\pi)\\,\np(\\sigma_b^2)\\,\np(\\sigma^2)\n\\]\nThis captures our updated beliefs about effects, inclusion indicators, and variance components."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#gibbs-sampling-for-spike-and-slab-blr",
    "href": "narrated_bayesian_linear_regression_slides.html#gibbs-sampling-for-spike-and-slab-blr",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Gibbs Sampling for Spike-and-Slab BLR",
    "text": "Gibbs Sampling for Spike-and-Slab BLR\nInference proceeds via Gibbs sampling, cycling through these conditional updates:\n\n\\(\\alpha \\mid D\\)\n\n\\(\\delta \\mid D\\)\n\n\\(\\pi \\mid D\\)\n\n\\(\\sigma_b^2 \\mid D\\)\n\n\\(\\sigma^2 \\mid D\\)\n\nHere, \\(D\\) denotes the data and all other current parameter values.\nEach conditional follows a standard distribution (Normal, Bernoulli, Beta, scaled-\\(\\chi^{-2}\\)).\nIterating these updates generates samples from the joint posterior."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#posterior-inclusion-probabilities",
    "href": "narrated_bayesian_linear_regression_slides.html#posterior-inclusion-probabilities",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Posterior Inclusion Probabilities",
    "text": "Posterior Inclusion Probabilities\nThe posterior inclusion probability (PIP) measures how likely each predictor is truly associated with \\(y\\):\n\\[\n\\widehat{\\Pr}(\\delta_i = 1 \\mid y)\n= \\frac{1}{T} \\sum_{t=1}^{T} \\delta_i^{(t)}\n\\]\n\nHigh PIP → predictor is likely important\n\nLow PIP → predictor likely irrelevant\n\nPIPs summarize variable relevance and drive Bayesian feature selection."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#summary-of-bayesian-linear-regression",
    "href": "narrated_bayesian_linear_regression_slides.html#summary-of-bayesian-linear-regression",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Summary of Bayesian Linear Regression",
    "text": "Summary of Bayesian Linear Regression\nBayesian Linear Regression combines likelihood and prior to form the posterior, enabling principled modeling, regularization, and uncertainty quantification.\n\nInference via MCMC (often Gibbs sampling) with posterior draws for means, credible intervals, and predictions.\n\nSpike-and-slab priors enable sparsity and variable selection, assigning exact zeros to irrelevant predictors and identifying key variables via posterior inclusion probabilities (PIPs).\n\nConjugate and mixture priors yield efficient and robust inference, even when \\(p &gt; n\\).\n\nWith proper convergence checks, Bayesian models provide stable and reliable inference across a wide range of data settings."
  },
  {
    "objectID": "narrated_bayesian_linear_regression_slides.html#applications-in-genomics-1",
    "href": "narrated_bayesian_linear_regression_slides.html#applications-in-genomics-1",
    "title": "Introduction to Bayesian Linear Regression, Posterior Inference, and MCMC",
    "section": "Applications in Genomics",
    "text": "Applications in Genomics\nWe have now seen the basic framework of Bayesian Linear Regression (BLR)\nand will illustrate how it provides a unified approach for analyzing genetic and genomic data.\n\nGenome-Wide Association Studies (GWAS) and fine-mapping of causal variants.\n\nGenetic prediction and heritability estimation.\n\nPathway and gene-set enrichment analyses.\n\nThese examples show how BLR connects statistical modeling with biological interpretation in quantitative genetics."
  }
]